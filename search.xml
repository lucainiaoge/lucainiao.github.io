<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Neural Network Theory Lecture Note 1 - Approximation Theorem]]></title>
    <url>%2F2021%2F09%2F22%2FNeural_network_theory_course_1%2F</url>
    <content type="text"><![CDATA[Pre-requisites: It will be helpful to know some basic concept about functional analysis, e.g. linear functional, normed space, Hahn-Banach Theorem; anyway they will be explained in my note Basic form of single-layer neural network. After the happy summer vacation, I have to attend the master program at ETH Zurich. This is my lecture note for my first course, Neural Network Theory, at ETH. Lecturer: Prof. Helmut Elbrächter This first note is about the capacity of a single-layer neural network in approximating an arbitrary function whose domain is in ${\rm \pmb{R}}^d$. To view this note, please download:NNT_note1_tongyulu.pdf]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REPET Algorithm - Background Music Separation Using Auto-correlation]]></title>
    <url>%2F2021%2F03%2F31%2FREPET_Algorithm_study%2F</url>
    <content type="text"><![CDATA[Pre-requisites: Knowing the discrete Fourier transform (DFT); Knowing the auto-correlation calculation for discrete series. This is a note for Zafar Rafii’s REpeating Pattern Extraction Technique (REPET) algorithm in its basic form. REPET is a super-simple algorithm which could separate human voice from accompanied music, although it is effective only in typical cases when background music is highly repetitive. To view this article, please download:REPET_Algorithm_study.pdf]]></content>
      <tags>
        <tag>mir</tag>
        <tag>MSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beta Regression vs. Logistic Regression]]></title>
    <url>%2F2021%2F03%2F21%2FBeta_Regression_vs_Logistic_Regression%2F</url>
    <content type="text"><![CDATA[Pre-requisites: Knowing the Maximum-likelihood criterion; It is recommended that readers have known the concepts and basic applications of Beta distribution. This article tells you a method to modeling a regression problem whose output is bounded and supposed to be countinuous. To view this article, please download:Beta_Regression_vs_Logistic_Regression.pdfThere is a .ipynb notebook for a toy experiment, which could be downloaded from:BoundedRegression.ipynb]]></content>
      <tags>
        <tag>math</tag>
        <tag>statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch - A Seemingly Indifferent 1-D Dimension Leading to Model Failure]]></title>
    <url>%2F2021%2F03%2F21%2FPyTorch_lesson_1D_dimension_leading_to_bad_loss%2F</url>
    <content type="text"><![CDATA[This is a reflection on PyTorch: be careful with the shape of tensor, even for differences between shape[B,1] and shape[B]. Have you encountered such a problem when training a neural network: you are sure that your model is perfectly correct, and your training procedure is also correct. But you cannot get the expect result. Your model outputs seem blurred, and the loss curve does not show obvious descent. What is wrong?Have you encountered python warning like this? UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size. There is a lesson behind this. To view this article, please download:PyTorch_lesson_1D_dimension_leading_to_bad_loss.pdf]]></content>
      <tags>
        <tag>machine_learning</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[From Beta Distribution to Conjugate Distribution]]></title>
    <url>%2F2021%2F03%2F14%2FFrom_Beta_Distribution_to_Conjugate_Distributions%2F</url>
    <content type="text"><![CDATA[Pre-requisites: Knowing the Bayes theorem; It is recommended that readers have gone through elementary training about statistics. This article tells you the basic ideas behind the Beta distribution, and its basic applications. In detail, we could use Beta distribution to model random varibales which represent proportions or probabilities. In this article, you could understand why Beta distribution is called “distribution of distributions”, and get an intuition about the concept “conjugate distribution”. To view this article, please download:From_Beta_Distribution_to_Conjugate_Distributions.pdf]]></content>
      <tags>
        <tag>math</tag>
        <tag>statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(Chinese) - 记录一个儿时的游戏-"攒" (A Biography for A Game in my Childhood-"Zan")]]></title>
    <url>%2F2020%2F12%2F28%2Fgame_chiledhood_zan%2F</url>
    <content type="text"><![CDATA[This article contains two Chinese articles about my childhood: a Chinese introduction for game "Zan" (see also http://www.wikibin.org/articles/bor-bor-zan-2.html), and a Chinese article regarding my view of points on this game. Game_instruction_zan_Chinese.pdf Memory_zan_Chinese.pdf If I have spare-time in the future, I will complete the English version.]]></content>
      <tags>
        <tag>misc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch - Use create_graph to Compute Second-order Derivative]]></title>
    <url>%2F2020%2F12%2F22%2FPyTorch_create_graph_is_true%2F</url>
    <content type="text"><![CDATA[by lucainiaoge This article assumes that readers have been familiar with the concept of computation graph. Can we use computation graph to calculate arbitrary-order derivative? The answer is yes, we can. But how to leverage it using PyTorch, and how to understand the whole thing? To view this article, please download:PyTorch: Use create_graph to Compute Second-order Derivative And the illustrations in that passage could be downloaded from create_graph_true_PyTorch_tutorial.ipynb]]></content>
      <tags>
        <tag>machine_learning</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[An Introduction to Deep Clustering]]></title>
    <url>%2F2020%2F12%2F17%2Fdeep_clustering_intro%2F</url>
    <content type="text"><![CDATA[by lucainiaoge Toy ExampleSuppose we have a series of vectors $V=[v_n],n=1,2,…,N$, where $v_n\in {\rm R^K}$. We want to cluster together similar vectors, what will we do? To view the whole passage, please download:An Introduction to Deep Clustering]]></content>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear-upper-coverage Smoothing for Peak Finding]]></title>
    <url>%2F2020%2F10%2F10%2Flinear-upper-coverage-algorithm-for-peak-finding%2F</url>
    <content type="text"><![CDATA[by lucainiaoge IntroI came up with this idea on October 9th, 2020, when I was considering how to find the overtone peaks in a spectrogram. I happened to be playing LIMBO the previous day, and that inspired me to come up with my idea regarding Linear-upper-coverage (LUC), which converts the peak finding problem into linear optimization subtasks. It is recommended that readers have grasped basic knowledge about calculus, convex optimization and digital signal processing (DSP). Still, if those pre-requisites are not satisfied, it is also OK to understand my ideas and algorithms. Most of those knowledges are helpful when readers want to understand my proofs for the Lemmas or Theorems proposed in this article. If [Math Processing Error] occurs or figures are not showing, please refresh this page or switch to another browser (Google chrome is recommended). One more tip: this article is a little bit long. However, if you skip my rationales/proofs and go directly to my motivations and experiment results, you will finish reading this article quicker. Anyway, welcome to delve into my math works and logics in order to understand LUC deeply, and welcome to correct me if I made mistakes. Problem Definition and MotivationThe ProblemHow to find the peaks in the following curve? If we manually mark the peaks, the result may normally look like the following figure? However, how to design an algorithm to robustly find the desired peaks (local maxima)? To answer this question, we have to ask ourselves: what is a desired peak? If we just want every local maxima, we may get this following result: And according to our current definition of “desired peak”, this ugly result is correct. Therefore, you may be willing to design an algorithm which is adaptable according to our variable definitions for a “desired peak”. What Inspired MeWell, in addition to the inspiring peak-finding-problem itself, another incentive of my idea is a rolling wheel on a coarse road: As you can see: the centroid of the wheels form up a much smoother curve (although still a little bumping) than the original curve. However, several insignificant local maxima are still troublesome. And it is also a little bit hart do model a wheel rolling on a coarse road with a few lines of codes. So how to improve this idea? I was lucky to be playing LIMBO the previous day, and one scene in the vedio game LIMBO just struck me: Notice the the red arrows: there are tiles covering the roof/hill (whatever it looks like as it is dark). So, why not cover our curve with tiles? I drew the following picture to show my idea of tile-covering: First, divide the x-axis into intervals; second, cover the curves in each intervals with tiles; third, find the centroid of each tile; finally, use the centroid scatters to find the local maxima. Notice that we can cover over curve with tiles of different lengths, which is equivalent our variable definitions on “desired peak”: if we use short tiles, we will be more sensitive to small local maxima, while long tiles will ignore the local maxima which are insignificant in terms of a smaller measure on $x$. Now comes the question: how to implement this idea? From Peak Finding to Linear OptimizationDeriving the Basic FormulationAs the title of this section suggests, I utilized linear optimization to solve the problem. Among the four steps in the previous section, you may notice that the phrase “over the curves in each intervals with tiles” is not clearly defined. How can we cover a curve with a tile? Here is where linear optimization comes into being: Suppose we got an interval $x \in [x_i,x_{i+1}]$, where a 1-D curve $f(x)$ is defined. The tile is depicted as $l_i(x)=a_i x+b_i$. Obviously, we must have $f(x) \leq l_i(x), \forall x \in [x_i,x_{i+1}]$. With this constraint, we want to find the parameters $(a_i,b_i)$ such that the tile seems to cover the curve. Newton’s laws suggest: if you put an object in a gravitatio, it is stable when its gravitational potential energy reaches its minimum. If we assume that the density of our tiles is uniformly distributed, we can conclude that the tile’s gravitational potential energy can be calculated by a particle locating at its centroid, which is $(\frac{x_i+x_{i+1}}{2},l_i(\frac{x_i+x_{i+1}}{2}))$. Therefore, we get our objective function: $l_i(\frac{x_i+x_{i+1}}{2})=a_i\frac{x_i+x_{i+1}}{2}+b_i$. To sum up, we get the linear optimization problems: $\forall i=1,2,…$, solve the linear programming (LP)problem within interval $[x_i,x_{i+1}]$:$$min_{a_i,b_i} \quad a_i\frac{x_i+x_{i+1}}{2}+b_i $$ $$s.t. f(x) \leq a_i x+b_i, \forall x \in [x_i,x_{i+1}]$$ And I may call it linear-upper-coverage (LUC) algorithm. Notice that there are infinite constraints, which are intractable. One intuitive way is to sample the interval $x \in [x_i,x_{i+1}]$ to get $x_{i}^{k} \in [x_i,x_{i+1}], \forall k=1,2,…,K$. And consequently, the linear constraints become: $$f(x_{i}^{k}) \leq a_i x_{i}^{k}+b_i, \forall k=1,2,…,K$$ Until now, we are able to solve that linear optimization problem with inequality constraints! The classical linear programming algorithms (e.g. the Simplex Method) are well-equipped to sove them! Formal LUC Definition Generalizing Interval PartitionsYou may noticed that although I divided the x-axis equally, the lengths of tiles are not the same. Is it necessary to set the tile lengths all the same? This is actually about the choosing the segmentation strategy, i.e. the procedure to get $[x_i,x_{i+1}], i=1,2,…$ Therefore, if restricted to linear coverage, the most configurable aspect is the segmentation strategy. Not only can we segment in equal length, we can also segment in a log-scale, or even sample randomly. Different segmentation strategy lead to different definitions of “desired peak”. Also, it is not necessary to assume that the intervals do not overlap. Instead, we can arbitrarily sample the curve and construct intervals for each of those sample points, even if those intervals can overlap. Such more general definition becomes: Definition 1 (normal LUC): Given $f:D\rightarrow \pmb{R}$, sample data points $x_i\in D,i=1,2,…$. Define intervals $I_i = [x_i^l,x_i^r]$ where $x_i \in I_i$, and define linear segments $l_i(x)=a_i x+b_i, \forall x\in I_i$.Define the linear programming problems $LUC_i$ within intervals$$LUC_i:\quad min_{a_i,b_i} \quad l_i(\frac{1}{2}(x_i^l+x_i^r))$$ $$s.t. f(x) \leq l_i(x), \forall x \in I_i$$ The LUC solution is the set of all optimum linear segments, i.e. $l_i^{\star}(x)$ From now on, $x_i$ no longer means the left endpoint of interval $I_i$, but the centroid point of interval $I_i$. The left endpoint of interval $I_i$ is written as $x_i^l$ instead. (If I sometime breaks this law, please understand that because those cases are obvious.) Moreover, we can cover the curve with actually whatever function we like: circles, ovels etc. I chose lines because they look more simple and intuitive. What about other elements? It might be worth trying, but I am not going to talk about them at this moment… Proof that LUC Has Smooth Effects: Convex CaseIf we want to prove that an algorithm can smoothify a curve, we have to prove that the algorithm dampens the high-frequency components in certain ways. The main challenge to prove this for LUC is that there are linear-programming calculations in each interval, which is non-trivial. I find it difficult to prove this even convex constraint is considered. Two Lemmas with ProofI start from the (lower) convex segments defined in $[x^l,x^r]$:$$f(\lambda x^l+(1-\lambda) x^r)\leq \lambda f(x^l)+(1-\lambda) f(x^r), \forall 0\leq \lambda \leq 1$$ Intuitively, we can reach the following conclusion: Lemma 1 (LUC for convex function): if $f:[x^l,x^r]\rightarrow \pmb{R}$ is convex in $[x^l,x^r]$, then the optimum LUC function becomes $l^{\star}(x)=\frac{f(x^l)-f(x^r)}{x^l-x^r} (x-\frac{x^l+x^r}{2}) + \frac{f(x^l)+f(x^r)}{2}$ Proof:①First prove that the given $l^{\star}$ obeys the constraints of LUC linear programming.That constraint is: $f(x) \leq l(x), \forall x \in [x^l,x^r]$. If we apply $l^{\star}$ to that constraint, we just need to prove: $f(x) \leq l^{\star}(x), \forall x \in [x^l,x^r]$.Plug in the convex assumption on $f(x)$ and we get: $f(\lambda x^l+(1-\lambda) x^r)\leq \lambda f(x^l)+(1-\lambda) f(x^r), \forall 0\leq \lambda \leq 1$.As $f(x^l) \leq l^{\star}(x^l)$ and $f(x^r) \leq l^{\star}(x^r)$, we get $\lambda f(x^l)+(1-\lambda) f(x^r)\leq \lambda l^{\star}(x^l)+(1-\lambda) l^{\star}(x^r) = l^{\star}(\lambda x^l+(1-\lambda) x^r)$.Therefore, we proved that $f(x) \leq l^{\star}(x), \forall x \in [x^l,x^r]$ is true.②Then prove that $l^{\star}$ is the optimum.Recall that the objective function is $l(\frac{x^l+x^r}{2}) = a_i\frac{x^l+x^r}{2}+b_i$.If we want to decrease the objective value $l(\frac{x^l+x^r}{2})=\frac{l(x^l)+l(x^r)}{2}$, either $l(x^l)$ or $l(x^r)$ should be decreased.However, according to the constraints, $f(x^l) \leq l(x^l)$ and $f(x^r) \leq l(x^r)$: once we have $f(x^l) = l^{\star}(x^l)$ and $f(x^r) = l^{\star}(x^r)$, we can assert that $l^{\star}$ reaches the optimum, for neither $l^{\star}(x^l)$ or $l^{\star}(x^r)$ can be decreased. □ On the other hand, if we assume that $f(x)$ is bounded in interval $[x^l,x^r]$, it is also intuitive to reach the following conclusion: Lemma 2 (existance of LUC solutions and tightness): if $f:[x^l,x^r]\rightarrow \pmb{R}$ is bounded, then: There are at least two zero-slackness LUC constraints at $x^{-}$ and $x^{+}$ ($x^{-}\leq x^{+}$); the term zero-slackness indicates that $f(x^{-})=l^{\star}(x^{-})$ and $f(x^{+})=l^{\star}(x^{+})$. Equality of $x^{-}\leq x^{+}$ is reached only when $x^{-}=x^{+}=x^l$ or $x^{-}=x^{+}=x^r$ Proof:① First prove that there is at least one optimum solution for the LUC problem.As $f(x)$ is bounded in interval $[x^l,x^r]$, assume that $m&lt;f(x)&lt;M_1&lt;M_2$, there exists $l(x)=a_i x+b_i$ such that $M_2&gt;l(x^l)&gt;M_1&gt;f(\forall x)$ and $M_2&gt;l(x^r)&gt;M_1&gt;f(\forall x)$. Therefore, feasible region is not empty.On the other hand, there exists $l(x)=a_i x+b_i$ such that $m&lt;f(x^l)\leq l(x^l)$ and $m&lt;f(x^r)\leq l(x^r)$. This is to say, the objective $\frac{l(x^l)+l(x^r)}{2}&gt;m$. Therefore, the objective cannot be arbitrarily small.In sum, there is at least one optimum solution for the LUC problem.② Then prove that the optimum solution lead to zero slackness. Recall the complementary slackness theorem:Given a standard LP problem $$min\quad cx, \quad s.t. \quad Ax \geq b$$ and its dual problem $$max\quad b^T y, \quad s.t. \quad A^T y \leq c^T$$ Definition of slackness of the $k^{th}$ constraint is $s_k = A_k x - b_k$.The complementary slackness theorem says that if a feasible solution $x_0$ is optimum (which exists because of ①), and its corresponding dual solution is $y_0$ is also optimum, then $x_0^T (c-A^T y_0)=0$ and $y_0^T (A x_0 - b)=0$.Therefore, for those $y_0^k \neq 0$, we must have $A_k x_0 - b_k = 0, k \in \lbrace k:y_0^k \neq 0 \rbrace$. As a result, $s_k = A_k x_0 - b_k = 0,k \in \lbrace k:y_0^k \neq 0 \rbrace$.Translate this into LUC context: $x_0=(a_{i}^{\star},b_{i}^{\star})$, $A=[x^{[1:K]},ones(1,K)^T]$, $c=[\frac{x^l+x^r}{2},1]$ and $b=[f(x^1),…,f(x^K)]^T$. The “$y_0^k$” in LUC cannot be all zero, or otherwize $c=0$, which is not true. Therefore, there always exists $k$ such that “$A_k x_0 - b_k = 0$”, which is actually $f(x^k)=l^{\star}(x^k)$ (the $k^{th}$ constraint), leading to zero slackness.③ Then prove that there are at least two different tight constraints.According to the convexity of linear feasible region (which is a polyhedron), the optimum solution touches the vertex of that region. As we know, the vertex of a region is an intersection of two hyperplanes (in LUC, they are actually two 2-D lines, because there are only two variables), corresponding to two different constraints. Therefore, if there are at least two constraints which have zero slackness, they are different.④ Finally prove the property when $x^{-}=x^{+}$.We know that each constraint can be linked with an $x$ in $[x^l,x^r]$. Moreover, the statement $x\in [x^l,x^r]$ indicates two more underlying constraints $x\geq x^l$ and $x\leq x^r$. Recall ③: the two constraints at $x=x^{-}$ and $x=x^{+}$ are different. If a constraint at $x=x^{-}$ is tight and $x^{-}=x^{+}$, then this case only happens at $x=x^l$ or $x=x^r$, because: any $x\in (x^l,x^r)$ can be linked with only one constraint $f(x)\leq l(x)$; but at $x=x^l$, there exists one more constraint $x\geq x^l$, which reaches tightness (and the case when $x=x^r$ is similar). In conclusion, the statement $x^{-}=x^{+}$ indicates that two tight constraints sharing the same point, and such case can only happen at $x=x^l$ or $x=x^r$; consequently, $x^{-}=x^{+}=x^l$ or $x^{-}=x^{+}=x^r$.□ Explore the Effects of LUC on DTFT SpectrumNow, we have all the ingradients! What is left is to describe our problem in a proper way. Consider a uniformally-sampled sequence of $f(x)$, which is written as $[f_i] = [f_1,f_2,…] = [f(x_1),f(x_2),…]$.Without downsampling, we do LUC for each interval centered with the $i^{th}$ point, i.e. the interval $[x_i-\Delta x,x_i+\Delta x]$. It is also OK to write this interval as $[x_{i-\Delta i},x_{i+\Delta i}]$ because of equal-partition. Therefore, we have the LUC problem formally: $$min_{a_i,b_i} \quad a_i x_i+b_i $$ $$s.t. f(x) \leq a_i x+b_i, \forall x \in [x_{i-\Delta i},x_{i+\Delta i}]$$ According to Lemma 2, there are at least two points $x_i^{-}$ and $x_i^{+}$ such that $f(x_i^{-})=l_i^{\star}(x_i^{-})$ and $f(x_i^{+})=l_i^{\star}(x_i^{+})$. We can subsequently determine $l_i^{\star}$ with those two points: $$a_i^{\star} = \frac{f(x_i^{-})-f(x_i^{+})}{x_i^{-}-x_i^{+}}$$ $$l_i^{\star}(x) = a_i^{\star}(x-\frac{x_i^{-}+x_i^{+}}{2})+\frac{f(x_i^{-})+f(x_i^{+})}{2} \quad (*)$$ Consequently, I write the LUC version of uniformally-sampled sequence as $[f_i^{\star}] = [f_1^{\star},f_2^{\star},…] = [l_1^{\star}(x_1),l_2^{\star}(x_2),…]$ Assume that $f(x)$ is convex in each interval $[x_{i-\Delta i},x_{i+\Delta i}]$ (no need to be convex in the overall definition interval). Then calculate the DTFT for both $[f_i]$ and $[f_i^{\star}]$:$$F(\omega) = {\rm DTFT}[f_i] = \sum_i f_i e^{\sqrt{-1}\omega i} \quad ①$$ $$F^{\star}(\omega) = {\rm DTFT}[f_i^{\star}] = \sum_i f_i^{\star} e^{\sqrt{-1}\omega i} = \sum_i [a_i^{\star}(x_i-\frac{x_i^{-}+x_i^{+}}{2})+\frac{f(x_i^{-})+f(x_i^{+})}{2}] e^{\sqrt{-1}\omega i} \quad ②$$ According to the convexity assumption, we can apply Lemma 1, which says that $l_i^{\star}(x)=\frac{f(x_{i-\Delta i})-f(x_{i+\Delta i})}{x_{i-\Delta i}-x_{i+\Delta i}} (x-\frac{x_{i-\Delta i}+x_{i+\Delta i}}{2}) + \frac{f(x_{i-\Delta i})+f(x_{i+\Delta i})}{2}$ Compare this with $(*)$ and we will get $x_i^{-}=x_{i-\Delta i}$ and $x_i^{+}=x_{i+\Delta i}$. Plug this conclusion into $②$ and also apply the equality that $x_i = \frac{f(x_{i-\Delta i})+f(x_{i+\Delta i})}{2}$, we will get:$F^{\star}(\omega) = {\rm DTFT}[f_i^{\star}] = \sum_i \frac{f(x_{i-\Delta i})+f(x_{i+\Delta i})}{2} e^{\sqrt{-1}\omega i}$$= \sum_i \frac{f_{i-\Delta i}+f_{i+\Delta i}}{2} e^{\sqrt{-1}\omega i}$$= \frac{1}{2}({\rm DTFT}[f_{i-\Delta i}] + {\rm DTFT}[f_{i+\Delta i}])\quad ③$ We know that ${\rm DTFT}[f_{i-\Delta i}]=\sum_i f_{i-\Delta i}e^{\sqrt{-1}\omega i}=\sum_i f_{i}e^{\sqrt{-1}\omega (i+\Delta i)}=e^{\sqrt{-1}\omega \Delta i}\sum_i f_{i}e^{\sqrt{-1}\omega i}=e^{\sqrt{-1}\omega \Delta i}{\rm DTFT}[f_{i}]$ Therefore, $③$ becomes $$F^{\star}(\omega) = {\rm DTFT}[f_i^{\star}] = \frac{1}{2}[e^{\sqrt{-1}\omega \Delta i}+e^{-\sqrt{-1}\omega \Delta i}]F(\omega) = F(\omega){\rm cos}(\omega \Delta i) \quad ④$$ Finally, we can reach from $④$ that $\vert F^{\star}(\omega) \vert = \vert F(\omega)\vert \vert {\rm cos}(\omega \Delta i)\vert \leq \vert F(\omega)\vert$ Summing up the discussions above: Theorem 1 (smooth effect of convex LUC): given $\Delta i &gt; 0$ and bounded $f(x):D\rightarrow R$, sample a sequence $x_i, i=1,2,…$ from $D$, whose corresponding intervals are $[x_{i-\Delta i},x_{i+\Delta i}], i=1,2,…$; the LUC centroids form a sequence $f_i^{\star}=l_i^{\star}(x_i), i=1,2,…$ whose DTFT is $F^{\star}(\omega)$, while the sampled curve becomes $f_i=f(x_i), i=1,2,…$ whose DTFT is $F(\omega)$. Then we have:if $f(x)$ is convex in intervals $[x_{i-\Delta i},x_{i+\Delta i}], i=1,2,…$ separately, then $F^{\star}(\omega)=F(\omega){\rm cos}(\omega \Delta i)$ Due to Taylor’s approximation of ${\rm cos}(\omega \Delta i)$: when $\omega$ is near zero, $F^{\star}(\omega)$ is near $F(\omega)$; but when $\omega$ is a little bit far from zero, $F^{\star}(\omega)$ will be dampened. Of course, the choice of $\Delta i$ is also a factor controlling such influence. To understand this effect more intuitively, here is an example, in which $\Delta i = 1/2$, and $f(x)$ is convex in every separate interval. The red line (downsampled results) is more rugged than the green line (LUC centroids), which confers to my previous analysis. A Candidate-set LUC Algorithm for General CasesIn the previous section, we discussed the LUC for a convex segment. More importantly, we proved Lemma 2, which guarentees the existance of solution, and even gives the exact solution: $$a_i^{\star} = \frac{f(x_i^{-})-f(x_i^{+})}{x_i^{-}-x_i^{+}}$$ $$l_i^{\star}(x) = a_i^{\star}(x-\frac{x_i^{-}+x_i^{+}}{2})+\frac{f(x_i^{-})+f(x_i^{+})}{2} \quad (*)$$ But it did not touch the essencial case of the tile covering a rugged curve whose convexity is ill defined. How to get a more clear mind about the rugged case? My plan is to find candidates in that rugged curve, from which we select two of them as $x_i^{-}$ and $x_i^{+}$ to determine our final result. Before delving into this, we look at several examples… Intuitive ExamplesThere are different curves and we will cover each of those with a line. Case (c) is a convex, whose solution is that its $x_i^{-}$ and $x_i^{+}$ are actually the endpoints, which has already been proven.Case (a) and (b) are concave cases, which are intuitively different from the convex case.Case (d),(e) and (f) are rugged cases.Case (g),(h) and (i) are combinitions of convex and concave segments. (h) and (i) has local maxinima, while (g) is monotonic. Think for a while about the results…And then, I give my intuitions: My finding is that: the candidate points (marked in orange) are such typical points: The local maximima The endpoints $X^l=(x^l,f(x^l))$ and $X^r=(x^r,f(x^r))$ The curve point $P$ such that the slope of $PX^l$ reaches maximum; the curve point $Q$ such that the slope of $QX^r$ reaches minimum Designing Candidate-Set AlgorithmMy intuition leads to an algorithm to find the candidate points: First, deal with the problem of finding local maximima: Algorithm 1 (finding local maxima): $m=locmax(y)$(1-based indexing is used) Given sample squence $y = [y_1,y_2,…,y_k,…,y_K]_{1\times K}$ initialize a buffer $b = [1,0,…,0,…,0,0]_{1\times (K+1)}$ initialize the result $m = [0,0,…,0,…,0]_{1\times K}$ for $k=1,2,…,K-1$: $\quad$ $b[k+1] = bool(y[k] \geq y[k+1])$ for $k=1,2,…,K$: $\quad$ $m[k] = b[k]·\overline{b[k+1]}$ return $m$ Then, deal with the problem of finding $P$ and $Q$: Algorithm 2 (finding $P$ and $Q$): $k_P,k_Q=findPQ(x,y)$(1-based indexing is used) Given sample squence $y_{1:K}$ and its corresponding x-axis $x_{1:K}$ $p = (y[2:K]-y[1])/(x[2:K]-x[1])$ $q = (y[1:K-1]-y[K])/(x[1:K-1]-x[K])$ $k_P = {\rm argmax}_k p[k]$ $k_Q = 1+{\rm argmin}_k q[k]$ return $k_P,k_Q$ Next, we reach the algorithm for getting the candidate set: Algorithm 3 (finding candidate set): $I_c=findCandidate(x,y)$(1-based indexing is used) Given sample squence $y_{1:K}$ and its corresponding x-axis $x_{1:K}$ initialize $I_c = \lbrace 1,K \rbrace$ $m=locmax(y)$ $I_m = find(m==1)$ $k_P,k_Q=findPQ(x,y)$ $I_c = I_c \cup I_m \cup \lbrace k_P,k_Q \rbrace$ return $I_c$ Now, take a look at the example (d),(e) and (f) in the last section: as those curves are rugged, the candidate points in $\lbrace (x_{i_c},x_{i_c}): i_c \in I_c \rbrace$ still form a rugged curve. Why not plug those points again into Algorithm 3? Consequently, we get $\hat{I_c}=findCandidate(x[I_c],y[I_c])$, as illustrated in the following figure: This leads to the ultimate algorithm: Algorithm 4 (finding the smallest candidate set): $I_c=findCandidateSmallest(x,y,N)$(1-based indexing is used) Given sample squence $y_{1:K}$ and its corresponding x-axis $x_{1:K}$;$N$ denotes the max iteration times; initialize $I_c = \lbrace 1,2,…,K \rbrace, \tilde{I}=I_c, \hat{x} = x[I_c], \hat{y} = y[I_c]$ for $n$ in $range(N)$: $\quad$ $\hat{I_c}=findCandidate(\hat{x},\hat{y})$ $\quad$ if $\vert \hat{I_c} \vert == \vert I_c \vert$: $\quad\quad$ break $\quad$ else: $\quad\quad$ $I_c=\hat{I_c}$ $\quad\quad$ $\tilde{I} = \tilde{I}[I_c], \hat{x} = \hat{x}[I_c], \hat{y} = \hat{y}[I_c]$ return $\tilde{I}$ Finally, we reduce the number of constraints to those indexed by $I_c$. ReflectionsWhy does the candidate-set algorithm work? Obviously, my intuitive explanations are far from compelling. To develop the following sections and to reconcile the effectiveness of candidat-set algorithm, I need to introduce my hypothesis, which is not proved at this moment: Hypothesis 1 (equivalence of candidate-set in terms of LUC):if the candidate set for bounded $f(x):I_i\rightarrow R$ is $I_{ci}$, then the normal LUC problem reduce to: $$LUC_i:\quad min_{a_i,b_i} \quad l_i(x_i)$$ $$s.t. f(x) \leq l_i(x), \forall x \in I_{ci}$$Or in the language of Lemma 2: $x_i^{-}\in I_{ci}$ and $x_i^{+}\in I_{ci}$ In addition, if you are careful enough, you may find that my definition for candidate-set is flawwed: it does not lead to the equivalent result as the normal LUC. This can be proven by the figure in the previous section which illustrates the candidate-set algorithm, which provides a counter-example: the ultimate LUC calculated based on the blue points actually violates the LUC constraints based on the initial black curve!Luckily, although this effect sometimes happens, the LUC results do not seriously change. I am going to demonstrate this in the experiment section. Still, I have my solution which can perfectly solve this flawwed-definition problem. The key is: modify the $3^{rd}$ criterion to get a tentative $4^{th}$ criterion: For type 1 candidate points $X_k = (x_k,f(x_k))$ (which are local maxima), find curve points $P_k = (x_{k}^{+},f(x_{k}^{+})),Q_k = (x_{k}^{-},f(x_{k}^{-}))$ for each $X_k$, such that $x_{k}^{+}&gt;x_k$ and the slope of $X_k P_k$ reaches maximum $x_{k}^{-}&lt;x_k$ and the slope of $Q_k X_k$ reaches minimum Consequently, the candidate-set finder algorithm should be modified according to the $4^{th}$ criterion. Fortunatelly, it is quite easy by changing Algorithm 2 and 3 a little bit. After all, I am going summarize all about candidate-set algorithm in the next section. Formal Definition for Candidate-set Algorithm Definition 2 (full definition for candidate set): Given bounded $f(x):I\rightarrow R$;the endpoints are $x^l=inf(I),x^r=sup(I)$.Then, the candidate set for $I$ is written as $I_{c}=\lbrace x:x\in I_m, x\in I_{e}, x\in I_{e}^a, x\in I_{m}^a \rbrace$, as defined by the following 4 criteria: $I_m = \lbrace x: x\in I \quad AND \quad \exists \delta &gt;0 \quad s.t.\quad f(x)\geq f(x+\epsilon), \forall \vert \epsilon \vert &lt; \delta \rbrace$, named “Maxima Set” $I_{e} = \lbrace x^l,x^r \rbrace$, named “Endpoints” $I_{e}^a = \lbrace \arg\max_{x\in I} \frac{f(x)-f(x^l)}{x-x^l},\arg\min_{x\in I} \frac{f(x)-f(x^r)}{x-x^r} \rbrace$, named “Endpoint Accompany Set” $I_{m}^a = \lbrace \arg\max_{x\in I,x&gt;x_m} \frac{f(x)-f(x_m)}{x-x_m}, \forall x_m\in I_m \rbrace \cup \lbrace \arg\min_{x\in I,x&lt;x_m} \frac{f(x)-f(x_m)}{x-x_m}, \forall x\in I_m \rbrace$, named “Accompany Set” As described, I need to modify the Algorithm 2 and 3 a little here: Algorithm 2.2 (finding the accompany set): $I_m^a=findAccompany(x,y,I_m)$(1-based indexing is used) Given sample squence $y_{1:K}$, its corresponding x-axis $x_{1:K}$ and its maxima index set $I_m$ Initialize $I_m^a = \emptyset$ for $k_m$ in $I_m$: $\quad I^{-} = [1:(k_m-1)], I^{+} = [(k_m+1):K]$ $\quad a^{+} = (y[k_m]-y[I^{+}])/(x[k_m]-x[I^{+}])$ $\quad a^{-} = (y[k_m]-y[I^{-}])/(x[k_m]-x[I^{-}])$ $\quad k^{a+} = I^{+}[\arg\max_{k}a^{+}[k]]$ $\quad k^{a-} = I^{-}[\arg\min_{k}a^{-}[k]]$ $\quad I_m^a = I_m^a \cup \lbrace k^{a+},k^{a-} \rbrace $ return $I_m^a$ Algorithm 3.2 (finding the full candidate set): $I_c=findCandidate(x,y)$(1-based indexing is used) Given sample squence $y_{1:K}$ and its corresponding x-axis $x_{1:K}$ initialize $I_c = \lbrace 1,K \rbrace$ $m=locmax(y)$ $I_m = find(m==1)$ $k_P,k_Q=findPQ(x,y)$ $I_m^a=findAccompany(x,y,I_m)$ $I_c = I_c \cup I_m \cup \lbrace k_P,k_Q \rbrace \cup I_m^a$ return $I_c$ Stepping Further: is it OK to Leave Out Linear Programming?Under Hypothesis 1, there seems no need to bother calling linear programming in order to get $l_i^{\star}$: as the candidate set is small, we can just search infinite-many pairs in order to locate $x_i^{-}$ and $x_i^{+}$.This motivates me to think about accompany pairs. What is an accompany pair? Well, I define it as follows: Definition 3 (accompany pair and accompany pair set): Given bounded $f(x):I\rightarrow R$; the endpoints are $x^l=inf(I),x^r=sup(I)$; the maxima set is $I_m$.Then, an accompany pair is such a tuple $p_m = (x_m,x_m^a)$, where: $x_m \in I_m \cup \lbrace x^l,x^r\rbrace$; $x_m^a = \arg\max_{x\in I,x&gt;x_m} \frac{f(x)-f(x_m)}{x-x_m}$ or $x_m^a = \arg\min_{x\in I,x&lt;x_m} \frac{f(x)-f(x_m)}{x-x_m}$Correspondingly, if $x&gt;x_m^a$, then we can also write this pair as $p_m^{-} = (x_m,x^{a-}_m)$, which is called the left-side accompany pair; if $x&lt;x_m^a$, then we can also write this pair as $p_m^{+} = (x_m,x^{a+}_m)$, which is called the right-side accompany pair.The set of all accompany pairs is written as $P^a(f,I)$, or just $P^a$ Easy to know that $x^l$ has no left-side accompany pair, and $x^r$ has no righy-side accompany pair. If there are $M$ elements in $I_m$, then the number of accompany pair is no greater than $2M+2$. Lastly, as each accompany pair can determine a linear function, there must exist the best pairs in terms of LUC objective function. Let’s define the best accompany pair step-by-step: Definition 4 (linear accompany function): Given bounded $f(x):I\rightarrow R$; the accompany pair set is $P^a$.Then, for each accompany pair $p=(x_m,x_m^a)\in P^a$, define its linear accompany function as $l_{p_m}(x)=\frac{f(x_m^a)-f(x_m)}{x_m^a-x_m}(x-x_m)+f(x_m), x\in I$.Definition 5 (feasible accompany pairs and their set): Given bounded $f(x):I\rightarrow R$; the maxima set is $I_m$; the accompany pair set is $P^a$.Then, an accompany pair $p\in P^a$ is feasible i.i.f $l_p(x)\geq f(x), \forall x\in I_m \cup \lbrace x^l,x^r \rbrace$Correspondingly, all feasible accompay pairs consist of the feasible accompany pair set, written as $P^{a_f}$Definition 6 (best accompany pair): Given bounded $f(x):I\rightarrow R$; the endpoints are $x^l=inf(I),x^r=sup(I)$; the maxima set is $I_m$; the feasible accompany pair set is $P^{a_f}$.Then, the best accompany pair $p^{\star}$ is defined as $p^{\star}=\arg\min_{p\in P^{a_f}}\quad l_p (\frac{1}{2} (x^l+x^r))$ Equipped with concepts of accompany pairs, I introduce another hypothesis, which is a vital hint for my next steps. Hypothesis 2 (at least one accompany pair is a zero-slackness pair):Given $f(x):I_i\rightarrow R$ whose feasible accompany pair set is $P_i^{a_f}$; then, there is at least one accompany pair ${x_i^{-},x_i^{+}}\in P_i^{a_f}, s.t.\quad l_i^{\star}(x_i^{-})=f(x_i^{-}),l_i^{\star}(x_i^{+})=f(x_i^{+})$, where $l_i^{\star}$ is the optimum LUC in interval $I_i$ for $f(x)$. The definitions regarding accompamy pair and Hypothesis 2 are actually paving the way to describe the following algorithm, which I expect to be equivalent to the LP based LUC: Algorithm 5 (LUCA: finding the best accompany pair): $p^{\star}=LUCBestAccompany(x,y)$(1-based indexing is used) Given sample squence $y_{1:K}$ and its corresponding x-axis $x_{1:K}$. Initialize $P^{a} = \emptyset, y^{\star}=+\infty, x^c = \frac{1}{2} (x[1]+x[K])$ $m=locmax(y)$ $I_m = find(m==1)$ $k_P,k_Q=findPQ(x,y)$ $P^{a} = P^{a}\cup \lbrace (1,k_P),(K,k_Q) \rbrace$ for $k_m$ in $I_m$: $\quad I^{-} = [1:(k_m-1)], I^{+} = [(k_m+1):K]$ $\quad a^{+} = (y[k_m]-y[I^{+}])/(x[k_m]-x[I^{+}])$ $\quad a^{-} = (y[k_m]-y[I^{-}])/(x[k_m]-x[I^{-}])$ $\quad k^{a+} = I^{+}[\arg\max_{k}a_k^{+}]$ $\quad k^{a-} = I^{-}[\arg\min_{k}a_k^{-}]$ $\quad P^{a} = P^{a}\cup \lbrace (k_m,k^{a+}),(k_m,k^{a-}) \rbrace$ $y_m = y[I_m], x_m = x[I_m]$ for $(k,k^a)$ in $P^{a}$: $\quad y^c = \frac{y[k^a]-y[k]}{x[k^a]-x[k]}(x^c-x[k])+y[k]$ $\quad \hat{y_m} = \frac{y[k^a]-y[k]}{x[k^a]-x[k]}(x_m-x[k])+y[k]$ $\quad$if $y^c&lt;y^{\star}$ and $\hat{y_m}\geq y_m$: $\quad\quad y^{\star} = y^c$ $\quad\quad p^{\star} = (k,k^a)$ return $p^{\star}$ See? the result $p^{\star}$ is actually the final decision!Even so, it is still unsolved in terms of judging if LUCA is equivalent with the normal LUC, or verifying Hypothesis 2. Unsolved Problems and My OrintationsTo finishing the discussions, I have to recapitulate several unexplained phenomena or unjustified hypothesis: to prove that the general case LUC (not restricted to convexity) has smooth effects. to verify or deny Hypothesis 1 and 2 to explain the phenomenon “no matter how rugged $f(x)$ is in interval $I$, once covered with LUC, the rug-details will vanish” to invent a 2-D version of LUC, or even multi-dimension version In terms of task 1, I have an idea (although not strictly proved): because we can get candidate sets $I_{ci},i=1,2…$ according to Algorithm 3.2, we can construct a new function $f_c(x),x\in \cup_i I_{ci}$. Take $f_c(x)$ as $f(x)$ and consider convexity again! Lemma 3 (candidate reconstruction): if $f:[x^l,x^r]\rightarrow \pmb{R}$ has LUC solution $l^{\star}(x)$, and the equation $l^{\star}(x)=f(x),x\in I$ has solutions $x_n^s,n=1,2,…,N$ then:there exists function $f_c(x)$ such that: $f(x)&lt;f_c(x)\leq l^{\star}(x), \forall x\in I-\lbrace x_n^s,n=1,2,…,N \rbrace$ $f_c(x)$ is convex respectively in $[x^l,x_1^s],[x_1^s,x_2^s],…,[x_N^s,x^{r}]$ Proof:For any interval $[x_i^s,x_{i+1}^s]$:① if $f(x)$ is convex in $[x_i^s,x_{i+1}^s]$: set $f_c(x)=\frac{1}{2}(l^{\star}(x)+f(x))$ in this interval, which confers to the conditions;② if $f(x)$ is not convex in $[x_i^s,x_{i+1}^s]$: set $f_c(x)=l^{\star}(x)$ in this interval, which confers to the conditions. □ After reconstruction using Lemma 3, we get $f_c(x)$, which is a concave version of $f(x)$ and is somehow like the case in Lemma 1, although the intervals are not equally partitioned. This intuition gives us a hint that even though the curve $f(x)$ is rugged, the smooth effects may still be similar to the convex case in Lemma 1 (although not exactly the same). Then, it is easy to see that $f_c(x)$ is smoother than $f(x)$ because of down-sampling. Therefore, as $l^{\star}(x)$ is smoother than $f_c(x)$ and $f_c(x)$ is smoother than $f(x)$, $l^{\star}(x)$ has smooth effects on $f(x)$, and even better than a mere down-sampling. In terms of task 2, I think we have to consider all convexity and corresponding behaviour regarding the candidate pairs… To do task 2, we have to think about the relationship with Lemma 2 and (Definition 2,3,4,5 and Algorithm 2.2,3.2,5).I think a conclusion may be of help: (I am not going to utilize this Lemma to solve task 2. I merely think that this conclusion is beautiful and may be of help, therefore I put it here with proof.) Lemma 4 (LUC for concave function): if $f:[x^l,x^r]\rightarrow \pmb{R}$ is concave in $I=[x^l,x^r]$, then the optimum LUC function $l^{\star}(x),x\in I$ becomes one of the tangents of $f(x)$ at $x^l$ or $x^r$ Proof:① first prove that $l^{\star}(x)$ must be tangent to $f(x)$ in interval $I$.Assume that $l^{\star}(x)$ is not tangent to $f(x)$ in $I$, which indicates that equition $l^{\star}(x)=f(x)$ has two different solutions $x^{-}$ and $x^{+}$ where $x^{-}&lt;x^{+}$. Then, for each $x\in (x^{-},x^{+})$, there exists $0&lt;\lambda&lt;1$ such that $x=(1-\lambda) x^{-} + \lambda x^{+}$, and then $f(x)=f((1-\lambda) x^{-} + \lambda x^{+})&gt;(1-\lambda) f(x^{-}) + \lambda f(x^{+})=(1-\lambda) l^{\star}(x^{-}) + \lambda l^{\star}(x^{+})=l^{\star}((1-\lambda) x^{-} + \lambda x^{+})$, which violates the LUC constraints.② then prove that $l^{\star}(x)$ is tangent to $f(x)$ at $x^l$ or $x^r$.Let $l^{\star}(x)=f’(x_0)(x-x_0)+f(x_0)$, therefore the LUC min objective function $obj(x_0)=l^{\star}(\frac{1}{2}(x^l+x^r))=f’(x_0)(\frac{1}{2}(x^l+x^r)-x_0)+f(x_0)$.Take the first derivative of $obj(x_0)$ and get $obj’(x_0)=f’’(x_0)(\frac{1}{2}(x^l+x^r)-x_0)$. When $obj’(x_0)=0$, then $x_0=(\frac{1}{2}(x^l+x^r)$. Unfortunately, when we take the second derivative of $obj(x_0)$ and get $obj’’(x_0)=f’’’(x_0)(\frac{1}{2}(x^l+x^r)-x_0)-f’’(x_0)$, we find that $obj’’(\frac{1}{2}(x^l+x^r))&lt;0$, which indicates that $obj(x_0)$ reaches the maxima at $\frac{1}{2}(x^l+x^r)$. But we want the minima.Luckily, we find that $obj’(x_0)$ is monotonically increasing in $I^l=[x^l,\frac{1}{2}(x^l+x^r)]$ and monotonically decreasing in $I^r=[\frac{1}{2}(x^l+x^r),x^r]$. Therefore, $obj(x^l)&lt;obj(x), x\in I^l$ and $obj(x^r)&lt;obj(x), x\in I^r$. Therefore, we can just compare $obj(x^l)$ and $obj(x^r)$ in order to determine whether the final $l^{\star}(x)$ locates at $x_l$ or $x_r$.③ finally give the condition for whether $x^l$ or $x^r$ is the tangent point.Solve the inequality $obj(x^r)&gt;obj(x^l)$ and we shall get $\frac{f(x^r)-f(x^l)}{x^r-x^l}&gt;\frac{1}{2}[f’(x^r)+f’(x^l)]$. If this equality holds true, we choose $x^l$ as the tangent point; otherwise, we choose $x^r$. □ In terms of task 3, it is OK to regard LUC a low-pass filter. But even if LUC coverage is relatively smooth, it still has infinite frequency components, for it is not perfectly sinusioid. As a result, the tool of Fourier Analysis may not be enough to explain the phenomenon that “LUC ignores the rugged surface, no matter how rugged the surface is”. I think the potintial explanation should be based on candidate sets. In terms of task 4, my intuition is to segment the definition set into elment cells. Correspondingly, the function is a 2-D manifold in 3-D space. The 2-D LUC task is to cover the manifold with a linear plate, which is definited within each cell.My intuition tells me that Lemma 2 still holds true for the multi-dimension versions. However, it may not be easy to find the centroids of each element cell, therefore making the statements regarding “element cell division” complicated. One example is that there no longer exists “endpoints” in the 2-D case, but margins instead. Also, the convexivity is more complexed. Implementation and ExperimentsFinally, I am going to show you the effectiveness of LUC. I am going to show two examples: one is about LUC applied to a hand-crafted curve, and another is about LUC applied to audio CQT spectrograms. In those examples, time consumptions and comparisons are included. Several challenges subsequently emerge. In this section, I will introduce those experiments and detailed problems. Experiment 1: Handcrafted Noisy WaveIn this example, the function $f(x):D\rightarrow \pmb{R}$ is designed as: $f(x)=10\sin (0.1x)+\cos (4x) + \mathcal{N}(0,2^2), x\in [0,200]$, where $\mathcal{N}(\mu,\sigma^2)$ represents Gaussian Noise parameterized by mean $\mu$ and std $\sigma$. This curve is shown in the following figure:(a) is the whole plot, and (b) zooms in $[0,20]$.For simplication, I use sample rate $1$ to sample interval $[0,200]$, forming $\pmb{x} = x[0],x[2],…,x[199]$. Notice that in this section, Python-styled $0-based$ indexing is used, for easy link with codes. ConfigurationsBefore implementing the algorithms, the whole problem still needs more specific definition: what is the hop length and the interval size?When I am talking about hop length, I assumed that the sampling strategy is equal-stepped. In the this experiment, data points are equally sampled.If we denote hop length as $\Delta_h i$ and interval size as $\Delta_I$, and each index interval as $I_i$, then: $I_i[0]+\Delta_h=I_{i+1}[0]$ and $I_i[-1]-I_i[0]=\Delta_I$. For example, the first index interval is $I_0 =[0,1,…,\Delta_I - 1] = [0:0+\Delta_I]$, the second index interval is $I_1 = [\Delta_h:\Delta_h+\Delta_I]$, and the third index interval is $I_2 = [2\Delta_h:2\Delta_h+\Delta_I]$, and so forth. Because we can use a for-loop to implement LUC for each interval, let’s just focus on implementing LUC in one interval. When it comes to one interval, then its LP, candidate set and candidate pairs come into use. In this experiment, I compared four algorithms, which are: (LP-based) normal LUC algorithm, as introduced by Definition 1 (LP-based) (non-full) candidate-set algorithm (LP-based) full candidate-set algorithm accompany-pair algorithm I used SciPy to implement LP-based methods by calling the scipy linprog method.One of the problems is to translate the LUC problem into the standard LP problem. The proof for Lemma 2 has already shown how to achieve this.Another problem is to choose a proper method for LP solution. I tried different methods and concluded that the revised simplex method is most efficient. For (non-full) candidate-set algorithm, I applied Algorithm 1,2,3,4 to find the candidate set, and then use LP (whose constraints are based on the candidate set) to find the solutions. The full-candidate-set algorithm modifies the candidate-set algorithm by replacing Algorithm 3 with Algorithm 3.2. For the accompany-pair algorithm, I apply Algorithm 5 only, which takes the role of LP. LUC ResultsIn this experiment, I set $\Delta_h = 10$ and $\Delta_I = 10$. First I extract the first two intervals and do LUC for them. Here is the result: The yellow lines indicate candidate-sets, while the red lines indicate the original curve. Those results verify the correctness of Lemma 2 by showing that at least two zero-slackness constraints are reached. They also show that applying full definition of candidate set does not lead to seriously different results compared with the results obtained by the non-full definition. Then, I test the whole curve, which is shown in the following figure:The green-star marks indicate local maxima of the centroid sequence, which are marked mostly by blue marks. From the whole-curve result we can see several phenomena: The accompany-pair algorithm result is the same as the normal-LP algorithm result and the candidate-set-full algorithm result. There are slight differences between results of the candidate-set-full algorithm and non-full candidate-set algorithm. Peaks are correctly detected regardless of serious noise. Phenomenon 1 verifies my Hypothesis 1 and 2, because the candidate-set-full algorithm and the accompany-pair algorithm is grounded on Hypothesis 1 and 2.Phenomenon 2 gives us a hint that we can change candidate-set-full algorithm into non-full candidate-set algorithm without losing much accuracy. In the next section I will compare the time consumption between those two algorithms.Phenomenon 3 verifies the effectiveness of LUC algorithm: after smoothed by LUC, it is OK to directly apply the strict $locmax$ function to the centroid serie and get the desired peaks. Next, I will compare each algorithm more deeply regarding their time cost and correctness. Time ConsumptionFor each algorithm, I adjusted interval size $\Delta_I$ and LP methods (except accompany-pair algorithm). I used Monte-Carlo method to reduce the variation of time cost estimation. In this test, I run each case for 100 times. Results are shown in the following figure:Time cost experiment results. This experiment is done by my old laptop, whose CPU is an Intel CORE i5.From the results, we can observe that: The most efficient algorithm is the accompany-pair algorithm, whose time cost is much lower than the others Candidate-set algorithm did reduce time cost compaired with normal algorithm, when the simplex-family LP methods are used. However, in when interior-point LP method is used, the candidate-set algorithm will be a little bit slower than normal algorithm. The revised simplex LP method is most efficient, while the original simplex LP method is least efficient. The full-candidate-set algorithm does not increase time cost too much compared with the non-full-candidate-set algorithm. Still, non-full-candidate-set algorithm is faster. The candidate-set algorithms show an interesting trend regarding $\Delta_I$: when $\Delta_I$ increases, time cost do not increase, but even decrease. This may be because that candidate-set filered out most irrelevant constraints and therefore there are less constraints left for LP. Influence of SNR and SIR on LUC AccuracyInfluence of Interval Partition on LUC AccuracyHigh-order LUCIt is not a must to apply $locmax$ directly after LUC. Insteac, we can do another LUC after LUC, which can be called as 2-order LUC. An example is shown in the following figure: The first LUC, namely first-order LUC, adopts $\Delta_{h1}=1$ and $\Delta_{I1}=10$. This smoothifies the original curve, as shown in (a). The second-order LUC adopts $\Delta_{h2}=5$ and $\Delta_{I2}=10$, and then we get the correct peaks. Notice that LUC is good at finding peaks with proper x-axis span. For example, in the figure above, the index width of each peak is around 20; when $\Delta_{h2}=5$, it is prone to find those peaks because 5 is around the same scale as 20, no matter how low those peaks are; when $\Delta_{h1}=1$, however, it is easier to find peaks of shorter x-axis span. Experiment 2: LUC on SpectrogramsTo demonstrate the potentials of LUC on audio analysis, I did another experiment, in which I applied LUC to find peaks in spectrograms.A spectrogram consists of time-variant spectrums. Those spectrums may be FFT, CQT or mel-spectra etc. Normally, we use color to demonstrate the intensity at each time-frequency. An example of CQT spectrogram is shown as following: If those CQT stuff do not make sense to you, no need to worry. Just remember that our task is to find the peaks (bright stripes) in that spectrogram, ignoring noisy points. Dealing with low-hight peaksSometimes, we do not want our model to detect low peaks, althought those peaks are wide enough. This leads to a problem: what exactly is our definition on a desired peak? On the one hand, we want a desired peak to be wide enough (otherwise, it is a fake peak); on the other hand, we want to ignore the peaks which are too low compared with the higher peaks. The first condition can be guarenteed by LUC, but LUC does not provide solution to meet the second condition. An example is shown in the following figure: Therefore, we have to come up with a new way to discriminate those low peaks from high peaks. My solution is to weight the peaks with salience. If a spectrogram $\pmb{S}=[s_{ft}]{F\times T}$ is inputted to a 1-order LUC, I hope my model can output a peak-weight map $\pmb{W}=[w{it}]_{L\times T}$. The relationship between $\pmb{S}$ and $\pmb{W}$ is as such: If hop length is $\Delta_h$ and interval size is $\Delta_I$, then $L=floor(\frac{F+\Delta_h-\Delta_I}{\Delta_h})$ $w_{i}$ refers to the weights at the $i^{th}$ interval centroid. Experiment 3: Comparing LUC with Existing Smoothing MethodsThere are all kinds of existing smoothing methods, among which the most common ones include: low-pass filtering, kernel smoothing, moving average smoothing and local regression. In order to fairly compare those methods, I use the same pipeline: signal → smoothing (to be tested) → locmax.]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My Music Thoughts]]></title>
    <url>%2F2020%2F09%2F03%2Fmy-music-thoughts%2F</url>
    <content type="text"><![CDATA[This article summarizes my understandings on music composing and appreciation. If you have something to add, or if you simply disagree with me, feel free to comment below! Starting from Sample SpaceWhat is the music sample space? The largest (whole) music sample space is the set of all possible combinitions of musical elements. Actually, the whole musical set is far too large. Great composers can extract the exact subsets which are good compositions. When composing music, we try to reduce our sample space, or eliminate the uncertainty in our mind. This process is like finding a winding path towards a beautiful hidden corner. For example, theory of harmony tells us what kind of chords are recommended to be used in certain musical contexts, and this reduces the chord sample space to a few so-called consonant chords. Now, we may ask: what is the property of those good subsets, and how to reach them? Well, a musician can write a thousand-paged essay to explain that, maybe from the point of view of melody, harmony, counterpoint and music structures… I am not going to expand those topics here. Instead, if we are inspired by information theory, we can find the commonplace of good musical works: they bring something unexpected. Those unexpected can be good melodies, satisfying chord prograssions or impressing performance skills etc. This may be a little hard to understand. I will explain this “unexpectation theory” in the next section. Music Appreciation: from Information Theory’s PerspectiveAppreciation = Eliminate the UncertaintyAppreciating music is like travelling: we aim to encounter the unknown world. From my point of view (and also an information theoretrical point of view), appreciation is centered in acquiring knowledge, eliminating uncertainty, or adding up regularity. (And no matter what we appreciate, like music, sculptures, maths etc.) Therefore, just like famous attractions have some landmarks which can “surprise” the tourists (e.g. the spectacular landscape of Yello Stone Park which tourists had never experienced on the scene), musical works should contain something out of listeners’ expectations. This is the intuition from information theory, which defines information as the existance to eliminate uncertainty. When appreciating music, we try to cumulate more information by decoding the music signal. Again, in information theory’s point of view, we are eliminating the uncertainty of the world. Our validation music sample space is then enlarged. Why Listening to a Music Segment Again and AgainYou may be curious about the case that we may listen to a piece of music again and again, enjoying an impressing music moment for many times even if we are familiar with that moment. Why are we so keen on the moment which seemingly cannot surprise us? Well… I explain this with our limited capacity of music decoding. Remember one of the music pieces which did not impress you when you first hear, but later when you happened to listen to it again, and then it captivated you. You may have listened to a music segment in that piece over and over again, and each time you listened to that piece, you expected to reach that segment. This was because, when you first listened to that music, you did not successfully decode the music signal into exciting information. Later when your decoder worked better, you captured the moment that important information was decipherd, and you wanted to emphasize that moment over and over again until you could easily decipher the similar information. You appreciation skills are improved during this process! Then comes the question: how to evaluate “appreciation skills”? Our Appreciation Skills Need TrainingDifferent people are sensitive on different things, and this is because of the diversity in their capacities of (music) decoders. For example, many listeners cannot be moved by atonal music because they focus on the jarring sound and cannot acquire artistic information which is impressive to musicians, who may be impressed by the organization of the piece or fantastic chord tensions. This is similar to: illiterate people cannot appreciate the beauty of Euler’s $e^{i\pi}+1=0$ because they do not know the prior knowledge associated with this formula. Again, our appreciation skills are in fact the capacity of our decoders. How to train those skills? First, we need training data. The key is trying to appreciate unfamiliar music and look for the surprising moments, which is like visiting an unknown place on your own. Another choice is to check music reviews (some of which are logs of the beautiful moments of a music piece), and this process is similar to visiting an attraction with a tour guide. Of course, even if we are well-trained to appreciate music, we cannot despise the music we have already familar with. After all, we upgraded out decoder system thanks to them. And more importantly, there may remain something we did not discover in the music even if we think we are well familiar with. How About Music ComposingWe have established the point that listeners are to be surprised when appreciating music. Therefore, composers have to surprise the listeners in order to create impressing works. What does surprising the listeners mean? This is not necessarily mean that they have to use “$\pmb{pppp} \rightarrow \pmb{ffff}$” to startle the listeners. This is to say that they have to convey important information which is out of listeners’ expectations, just as we have already discussed. The Case of Pop MusicFor listeners who did not receive a lot of appreciation training, a fairly normal chord progression could grasp their attentions and make them feel surprised and happy. Hence, most pop music composers achieve this by applying easy-listening chord progressions and music structures to their music, and they focus on writing “surprising” melodies, which are normally easy to appreciate. In this case, the music sample space of the composer may be greatly larger than listeners. Composers should find the music samples at the edge of listeners’ music sample spaces, and this is enough to entertain listeners. The intersection of the music sample spaces of most people form the set of universal pop music. What about the rest (which may be a lot larger)? The Case of All Kinds of MusicFor listeners who are well-trained, they may seek for an unexpected music element combination (remember, it is actually a sample point of the music sample space). This challenges composers to explore their music sample spaces, and this is the fairly interesting: because not everyone share the same music sample space, the hidden corner you visited is not likely to be visited by others. Before surprising the listeners, composers had better first surprise themselves, and this needs composers to have be skillful at appreciation, which means that they have to dig out more and more information. In other words, composers themselves have to own powerful decoders. But where do the surprising things come from? You know that even if you have a good decoder, you may not have musical codes to feed into the decoder. And this is where generative models are crutial! Randomness, Generative Models and SamplingTo be frank, we do generation by implementing randomness. We randomly sample music data points from our music sample spaces. Main differences between composers lie in 1. the structure of music sample spaces, 2. their sampling strategies and 3. their appreciation skills. Appreciation skills are analyzed in the previous section. Therefore, I mainly talk about the other two. First talk about music sample spaces. Remember the first section? I put it here again… When composing music, we try to reduce our sample space, or eliminate the uncertainty in our mind. This process is like finding a winding path towards a beautiful hidden corner. Yeah, good composers know how to reduce their sample spaces. They may use harmony theory to exclude the bad chords, and thus the amount of remaining music data points are smaller. They also have other constraints on their sample spaces, like: melody alignments, music structures, instruments etc. You know, it is fairly easy to sample a data point from a small sample space according to their wills! In order to be skillful at reducing the sample space, composers need to know what compact sample spaces look like. Therefore, they had better learn harmony theory, music structures, counterpoint and different music styles. All aspects lead to certain compact sample spaces, and great composers are skilled in pinpointing the required compact sample space. Now let’s go to another question: even if we have a compact sample space, how to sample a music data point from that space? Randomness and Sampling Stratagies for MusicNotice that the famous generative models (e.g. HMM, VAE, GAN) all depend on randomness, and they achieve generation by sampling their sample space in various ways. With a good sampling stratagy (e.g. conferring to the thinking pattern of first determining the music structure), you can have a good start point to converge your chaotic mind into a relatively small sample space. Music is contextulized. Therefore, we do sampling according to contexts. Most composers may sample their music measure by measure, and the result might be that their music do not have a good structure. This is similar to the strategy of RNNs. Another strategy is to first determine some global constraints and then do sampling over time. This is the case of most composers because this safe guards a good music structure. This is similar to the strategy of transformers. What about music with unexpected beautiful organizations? Chances are that composers see the music structures as non-deterministic, and they have a unique subset of sample space for music organizations. They first sample a music organization, and then follow the traditional way as described before. Of course there are many many other ways I have not mentioned. For example, some avant-guard compsers may record natural sounds and reorganize them as a music work, which is totally unconventional. This is where composers have to explore various of ways of sampling, and this is where to expand the border of art! Go PracticeA last topic, how to practice composing music? We have to know that, when practicing to compose music or perform music, we try to revisit the winding paths towards beautiful hidden corners. This is because we are humans rather than GODs (who know the global maxima), and we have to struggle to do optimization, just like training the machine learning models. On the one hand, if you find an unvisited “winding path” (e.g. a unique melody), you can revisit it over and over again until you can introduce it to others, and then you become a composer! You must train yourself to be familiar with the process of encoding the “winding path” into audios or music language. Remember not all people have visited those winding paths. Therefore, practicing to efficiently encode your music minds into music signals can let a composer quickly impress the audience. On the other hand, your decoder should also be sensitive enough to discover the winding paths in your chaotic mind or the colorful world! In this case, you should try to listen to more music you did not tried, and push yourself into digging out surprising information from them, or even imitate them. Just as I had explained previously. Another thing you can really do is to optimize your sampling stratagy, or the composition process. You have to try a lot in order to find a desirable process. For me, I am kind of conventional that I use draft scores to compose music instead of use DAW. What about you? Try try try! Finally, your chaotic mind, the generation source. Well, your mind can be influenced by external conditions like changing moods or spirits. Therefore, try to find different envirments of composing. Go practice, think more, and appreciate more! Algorithm ComposingIn fact, all of my discussions above can be instructions for designing algorithm composing models. Algorithm composing models face the same challenges: to constrain the sample space, to have a sample stratagy, to have a good decoder and a good discriminator, to be optimized in order to be familiar with the “winding paths”… Interestingly, you can find corresponding topics in machine learning in terms of each of the aspect I mentioned above. Let’s go and see! A Problem of My Theory Above[Updated on 30-April-2021]I might have taken it for granted that we tend to appreciate music which brings us surprise. How to explain the phenomenon that the music which sounds common is popular? For example, experiments have shown that car-radio listeners tend to prefer music which sounds typical (the so-called “sticky music”), rather than classical music which have abundant artistic information. And how can we explain that we listen to a piece of music again and again, even if we know that we are quite familiar with that? For example, after finishing composing a piece of music, I tend to listen to it again and again even if I am the composer and I know that I know a lot about it; and sometimes I listen to a pop song again and again. How to expain them? In my previous opinion, I defined happiness as knowing something unexpected. However, a friend of mine inspired me that happiness might also come to us when some of our expectation are met. (We were discussing why people get unhappy, and we found that a great reason is that their expectations, from either subconscious or conscious, are not reached.) What is expectation? Can we explain it with a definition, like “surprise is elimination of uncertainty, which is measured by entropy”? Well… For me, it is a big problem to be solved. But I have an idea that habit is a kind of expectation. Most of our habits work in our subconscious; therefore, when we find ourself appreciating a familiar old song, we may say that we find our subconscious is feeding on that old song by meeting its expectation. Here, the two theories seem to fight with each other: when we define happiness as eliminating uncertainty, we may say that things are happening out of our expectation (if not, we may not be surprised), but we cannot explain the case when we are appreciating a piece of familiar music and enjoying one particular moment again and again; when we define happiness as meeting our expectation, we cannot explain the euphoria of listening to a great symphony. There must exist a theory which is able to blend those two into one (and I think the great theory of happiness exists because of Hegel’s dialectics). I think the main difference between those two theories lie on the quality of happiness: the “surprise theory” dipicts a “tired but happy” case, while the “expectation theory” dipicts a “safe and sound” case. Our habits tend to listen to music which is familiar to us, and they do not consume much of our computation resource in our mind; when we find ourself being able to appreciate a symphony, we have consumed a lot in our mind, and we find that we did not consume it in vain because we found something unexpected. How to explain when doing experiments, we get an everything ruined (e.g. unexpect error in coding), and get mad? How to explain when listening to a piece of music, we get startled by a wrong note played by the music performer and get angry? In those cases, our expectations are not met at all, but we consumed a lot of mental energy. It seems that, when knowing something unexpected, we get either happy or unhappy, and we really have to tell the difference between those two cases. To sum up, we have four cases: we did not consume much mental energy, but get something expected; we consumed a lot of mental energy, and get something expected; we did not consume much mental energy, but get something unexpected; we consumed a lot of mental energy, and get something unexpected. We did not discuss case 2 and case 3; but we demonstrated that case 1 brings happiness and case 4 could bring either happiness or unhappiness. When thinking about our experiences, we could find that case 3 could bring either happiness or unhappiness, while case 2 is complicated… It seems that happiness is a rather complicated thing to be explained. I believe that it could be measured and decomposed into more elementary concepts. However, my assumption that “elimination of uncertainty brings happiness” might be incorrect; or I should endow this statement a more suitable definition.]]></content>
      <tags>
        <tag>music</tag>
        <tag>philosophy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intuition for Statistical Inference]]></title>
    <url>%2F2020%2F08%2F03%2Fintuition-for-statistical-inference%2F</url>
    <content type="text"><![CDATA[by lucainiaoge IntroIn a word, statistical inference deal with the problem of “trying to reach the God”. In other words, we observe data, and we try to fit to the distribution of such data. A little bit hard to understand? Hope that this article can help you understand that… An example: pictures Suppose you are studying digital pictures $x \in \pmb{R^{M\times M}}$. Accordingly, its distribution is $p_G(x): \pmb{X_{G}} \rightarrow \pmb{R^+}\cup\pmb{O}$. One of our goals is to find exactly the subspace $\pmb{X_{G}}\subseteq\pmb{R^{M\times M}}$ where the pictures $x \in \pmb{X_{G}}$ look like hand-written numbers (correspondingly, $\pmb{X_{G}}$ is the space where the ${M\times M}$ hand-written number pictures are defined). Again, our mission is to find the $p_G(x): \pmb{X_{G}} \rightarrow \pmb{R^+}\cup\pmb{O}$ given a hand-written number dataset $\pmb{X_{s}}$. We take $\pmb{X_{G}}$ as the “God/Ground-truth dataset” and $\pmb{X_{s}} \subseteq \pmb{X_{G}}$ as the “observation dataset” (e.g. MNIST). Now, question is: how to fit to that God/Ground-truth distribution? Well… For each question, we have to represent it before we solve it. In this article, I try to represent it clearly. This article does not try to solve those questions explicitly. The aim of this article is to remind you what you are really doing when you get lost in some annoying math works. By the way, why it is important to estimate $p_G(\pmb{x})$? Because once we know $p_G(\pmb{x})$, we can do such things: 1. to judge whether a given data point $x_0$ belongs to our distribution $p_G(\pmb{x})$, and then we can do classification; 2. to sample data points according to $p_G(\pmb{x})$, and then we can do generation! The Variational ProblemWe have already been familiar with this form: given a vector space $X$ (e.g. $X=\lbrace 0,1,2 \rbrace$) and a function $f$ (e.g. $f(x)=x^2$), we want to solve$$\mathrm{argmax}_{x∈X} f(x)$$What about the argmax problem which finds the optimum function? i.e. given a function space $\mathcal{F}$ (e.g. $\mathcal{F}=\lbrace f: f(x)=x^2+c,c\in \pmb{R} \rbrace$) and a functional $F: \mathcal{F}→\pmb{R}$ (e.g. $F[f]=\int_0^1 f^2(x)dx$), we want to calculate: $$\mathrm{argmin}_{f∈\mathcal{F}} F[f]$$ This problem is called a variational problem, which aims to find a function $f∈\mathcal{F}$ which minimizes/maximizes a functional $F$. We have already encountered this kind of problem in information theory!For example, we have already found the “discrete maximum entropy” $$\mathrm{argmax}_{p∈\mathcal{P}} H[p]$$ If $\mathcal{P}= \lbrace AllDiscreteDistributionsWith\quad M \quad Values \rbrace$, and $H[p(x)]=\sum_{x}p(x) \mathrm{log}⁡ \frac{1}{p(x)}$, then $$\mathrm{argmax}_{p∈\mathcal{P}} H[p]=U(x)$$ where, $U(x)$ is the uniform distribution on $M$ possible discrete values. The Statistical Inference Variational ProblemProblem definitionsWhat we want to do is: given data $\pmb{x}$, we want to approximate its distribution $p_G(\pmb{x})$. i.e. we want to solve this variational problem: $$p^*(\pmb{x})=\mathrm{argmax}_{p∈\mathcal{F}} D[p(\pmb{x}),p_G (\pmb{x})]$$ The term $p_G (\pmb{x})$ means the God’s distribution of data $\pmb{x}$, or the “Ground truth distribution”. Well, I have to say that we assumed that there is a ground truth (or GOD!), which is our philosophical point of view.Now is the problem: Problem 1: How to define the functional (distance between distributions) $D[p(\pmb{x}),p_G (\pmb{x})]$? Problem 2: How to define the function space $\mathcal{F}$? Problem 1: where the “Maximum Likelihood” comes fromProblem 1: how to find the functional (distance between distributions) $D[p(\pmb{x}),p_G (\pmb{x})]$? Note that we do not know the GOD distribution i.e. $p_G(\pmb{x})$. Thus, it is impossible to calculate this functional by directly calculating the distance between $p(\pmb{x})$ and GOD distribution. However, there is an easy way to define $D[p(\pmb{x}),p_G (\pmb{x})]$ as:$$D[p(\pmb{x}),p_G (\pmb{x})]≜\mathrm{log}⁡p(\pmb{x})$$(Other ways include estimating the momentums or other sufficient statistics. Those are out of the current scope.)For i.i.d. data $\pmb{x}$, we can even write $\mathrm{log}⁡p(\pmb{x})$ as $\mathrm{log}⁡p(\pmb{x})=\sum_{i=1}^N \mathrm{log}⁡p(x_i)$The term $\mathrm{log}⁡p(\pmb{x})$ is called as “log-likelihood”, and the variational problem $p^*(\pmb{x})=\mathrm{argmax}_{p∈\mathcal{F}} D[p(\pmb{x}),p_G (\pmb{x})]$ is called “maximum likelihood”, which we have already been familiar with.(Note that in the scheme of “maximum likelihood”, we did not consider $p_G (\pmb{x})$ explicitly. We assumed that if $\pmb{x}$ is from the ground-truth, $p_G (\pmb{x})$ should always be larger or equal than other $p(\pmb{x})$. We tend to think that “what we see is GOD”. Or in other words, “data (what we observed) are all we know, and we cannot do better unless we know more information (more data, or more observations)”. The “momentum estimation method” directly applies this philosophical assumption, i.e. calculating the distances between moments in order to make $p(\pmb{x})$ as near as $p_G (\pmb{x})$.) Problem 2: to parameterize or not toWell… For problem 2, we have two ways: to parameterize or not.What is parameterization? I use an example to illustrate that:Suppose the data $\pmb{x}$ are 1D. And we assume the function space is Gaussian, i.e. $\mathcal{F}=\lbrace AllOneDimGaussianPDFs \rbrace$. We can easily write $\mathcal{F}$ in the form of the space of its sufficient statistics (i.e. $\mu$ and $\sigma$). In other words, the space $\Theta=\pmb{R}\times\pmb{R^+}$ is actually the function space $\mathcal{F}$. And there exists a unique Gaussian Function $G:\Theta→\mathcal{F}$ such that for any $(\mu,\sigma)∈\Theta$, $G(\mu,\sigma)=f(x\vert \mu,\sigma)∈\mathcal{F}$In this case, we parameterized $\mathcal{F}$ with $\Theta$, with the distribution assumption $P:\Theta→\mathcal{F}$. In other words, $\Theta$ are the parameters which control the function space. Thus, the variational problem$$p^*(\pmb{x})=\mathrm{argmax}_{p∈\mathcal{F}} D[p(\pmb{x}),p_G (\pmb{x})]$$ can be written as an argmax problem for parameters: $$\theta^*=\mathrm{argmax}_{\theta∈\Theta} D[f(\pmb{x}\vert\theta),p_G (\pmb{x})]$$ $$p^* (\pmb{x})=P(\theta^* )$$ Of course, we can search the $\mathcal{F}$ directly without introducing $\Theta$ and $P:\Theta→\mathcal{F}$. How to search $\mathcal{F}$ given $\pmb{x}$? There are also various ways (e.g. $k-means$). However, we are not talking about those methods right now. A statement for a challenge: how to find the God - bias vs. varianceHow to assume a nice function space $\mathcal{F}$ such that the $p^* (\pmb{x})∈\mathcal{F}$ is easy to find and that $D[p(\pmb{x}),p_G (\pmb{x})]$ is relatively small? In other words, the function space $\mathcal{F}$ (or parameter space $\Theta$ in the case of parameterization) should satisfy such properties in order to be a good one: there exists $p^* (\pmb{x}))∈\mathcal{F}$ such that $D[p(\pmb{x}),p_G (\pmb{x})]$ is small, i.e. $\mathrm{E}_{\pmb{x}\in\pmb{X_G}} [D[p(\pmb{x}),p_G (\pmb{x})]]$ is small; $D[p(\pmb{x}),p_G (\pmb{x})]$ cannot be too unstable, i.e. $\mathrm{Var}_{\pmb{x}\in\pmb{X_G}} [D[p(\pmb{x}),p_G (\pmb{x})]]$ is small. If $\mathrm{E}_{\pmb{x}\in\pmb{X_G}} [D[p(\pmb{x}),p_G (\pmb{x})]]$ is small, we say that the function space F is of small bias compared with the GOD’s function $p_G (\pmb{x})$; If $\mathrm{Var}_{\pmb{x}\in\pmb{X_G}} [D[p(\pmb{x}),p_G (\pmb{x})]]$ is small, we say that the function space $\mathcal{F}$ is of small variance compared with the GOD’s function $p_G (\pmb{x})$, or that our model $p^* (\pmb{x})$ has a good ability to generalize. The term “generalize” means that no matter what data $\pmb{x}$ is given, the difference between $p^* (\pmb{x})$ and $p_G (\pmb{x})$ will not change too much. Intuition from Hung-yi Lee: see the pictures inhttp://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Bias%20and%20Variance%20(v2).pdf Each “sample point $p_s^* (\pmb{x})$” is actually an instance of $p_s^*(\pmb{x})=\mathrm{argmax}_{p∈\mathcal{F}} D[p(\pmb{x}),p_G (\pmb{x})]$, where $\pmb{x}\in\pmb{X_s}$, and $\pmb{X_s}$ (e.g. MNIST) is a subset of the God data set $X_G$ (e.g. all possible pictures of handwritten numbers). Afterwards…You have already got an intuition of our challenges. Now, time to solve the challenges! Of course, for several specific problems, simple models are enough! For example, if we want to fit a Gaussian distribution, the most convenient way is to calculate the mean and variance of the dataset (observation). However, most problems are complex in reality. We have to design models which can deal with such complexity. A genious way is to introduce Latent Variables to make our models more powerful. We assume that the Latent Variables can reveal the underlying modes of our observations. How to calculate the Latent Variables and find the argmax? We will discuss later when introducing HMM (Hidden Markov Model), GMM (Gaussian Mixure Model), EM (Expectation Maximization) algorithm and variational inference. The Latent Variable trick is in fact a statistical assumption. Another genious way is to use Neural Networks in order to fit arbitrary functions. The Neural Networks trick is in fact a structural (connectivism) assumption. In order to reduce the variance, more tricks can be considered (e.g. early stopping, dropout, normalization, residual connection, parameter sharing, simplified models…). Details are overwhelming. Not going to introduce them here!]]></content>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Summarization for Labs and People in Music Technology (Useful Links Attached!)]]></title>
    <url>%2F2020%2F08%2F02%2Fmusic-computation-labs-and-people-and-useful-links%2F</url>
    <content type="text"><![CDATA[(2020) I am trying to summarize labs and researchers around the world who dedicate in music computation. I am writing this list because I really want to do researches about MIR and music generation. Of course, this list is far from complete. If you have recommendations, feel free to comment below! If You Did not Know about MIR Here is a superfacial introduction for starters and for people who are interested in music computation. MIR means Music Information Retrieval. Its relevant topics are broad! If I am talking about MIR, I am refering to the topics which trie to analyze everything about music in a technological fashion. In many cases, MIR is a term which has the same meaning with “music computation”. By the way, another term “music technology” mainly means topics about synthesizers, digital music formats (e.g. MIDI, mp3) and instruments, which emphasizes differently from the term “MIR”. You may be curious about how to identify a song by humming, which is an MIR topic called Query by Humming (wiki). You may also be curious about how to translate a musical performance into scores, which is an MIR topic called automatic music transcription (AMT) (paper). Other interesting topics include music recommendation (blog), computational musicology (blog), sound synthesizing (blog) and algorithm composing (wiki) etc., which are all relevant topics.Search “MIR” in wiki: https://en.wikipedia.org/wiki/Music_information_retrieval. Those listed above are all tasks/goals. But what about the methodology to achieve them? Remember: music is created by people. Therefore, tasks about understanding musical rules and phenomenons are actually tasks about understanding human thoughts. How to understand them?Well… On the one hand, the nature of music is audio and the nature of musical understandings is human cognition; thus, we have to deal with audio properties and human perceptions. Relevant techonologies include Digital Signal Processing (wiki), Synthesizer (wiki) and Cognitive Musicology (paper).On the other hand, music is a kind of rule-based art, and we are interested in such rules. But how to understand and apply such rules? Relevant methods include Music as Language (paper), Natural Language Processing (NLP)(wiki). We know that the “rules” in rule-based-arts are far from simple. Therefore, we have to use systems which have great complexity to fit to those rules. Such method is machine learning (paper).With such tools, we can deal with problems which are directly relevant to music. For example, we can design our “generative music language”, and then apply it for algorithm composing. Or we can apply the DSP methods to identify chords from audio. Of course, every topic has its unique side. Therefore, never stop learning! The following links in the rest of this page are all relevant with what I introduced before. Labs, Groups and Individual ResearchersLinks of Summarizations Research Centers Summarized by SMC:http://www.smcnetwork.org/centers.htmlThis is a comprehensive list, which includes groups mainly in Europe and America. Archives, Journals and Societies about “Science &amp; Music” by University of Cambridge, Center for Music and Science:https://cms.mus.cam.ac.uk/linksThis page is a summarization of research groups and research society mainly in Europe. You may find links of research groups as well as academic sources in this page. Research Centers Summarized by Beici Liang:https://mp.weixin.qq.com/s/2nDWikda9fh2x5o093F6HAThe groups mentioned in this list are mostly in universities. By the way, the wechat official account (in Chinese 中文) in this link is a tutorial for beginners who are interested in music tech. I give those comprehensive links first. After all, I have to reinvent the wheel because I cannot remember all of them unless I write them down and read them all. My summarizationHere is a summarization of research group websites I have visited. My list is far from complete. I am just writing them down in order to have a review on the MIR society I have explored, straightening my mind. After all, I felt dizzy when I first began to search about MIR groups around the world…If you are a starter, hope that the following links can help you. If you are already a researcher, well… hope that my naive list does not bother you and that you can give some suggestions!Note that the information here is limited. I may have left out or misunderstood a lot of information. Therefore, the information in my list is inevitably biased! Again, this list is created mainly for myself and for starters to get familiar with the MIR society. If I wrote something improper, please contact me for correction! …To be expanded Europe Center for Digital Music (C4DM), Queen Mary University of London (QMUL)http://c4dm.eecs.qmul.ac.uk/index.htmlA very big lab with many groups. Digital and Cognitive Musicology Lab (DCML), École polytechnique fédérale de Lausanne (EPFL)https://www.epfl.ch/labs/dcml/Led by Prof. Martin Alois Rohrmeier. Music Technology Group (MTG), Universitat Pompeu Fabra (UPF), Barcelonahttps://www.upf.edu/web/mtg/ Center for Music and Science (CMS), Faculty of Music, University of Cambridge.Faculty of Music: https://www.mus.cam.ac.uk/; CMS: https://cms.mus.cam.ac.uk/ Prof. Remco Veltkamp, Utrecht Universityhttp://www.cs.uu.nl/centers/give/multimedia/music/index.html Asia Laboratory of Audio and Music Technology (FD-LAMT), Fudan Universityhttp://homepage.fudan.edu.cn/weili/fd-lamt/Led by Prof. Wei Li. Musix X Lab, New York University Shanghai (NYU Shanghai)http://www.musicxlab.com/#/indexLed by Prof. Gus Xia. Sound &amp; Music Computing Lab, National University of Singapore (NUS)https://smcnus.comp.nus.edu.sg/Currently Led by Prof. Ye Wang. Affective Computing and AI Team (AMAAI), Singapore University of Technology and Design (SUTD)https://dorienherremans.com/teamCurrently Led by Prof. Dorien Herremans. America Center for Music Technology, Georgic Tech (GaTech).https://gtcmt.gatech.eduCurrently Led by Prof. Alexander Lerch. Center for Computer Research in Music and Acoustics (CCRMA), Stanford Universityhttps://ccrma.stanford.edu/This is a big lab. There are many groups in CCRMA. See here. Prof. Julian McAuley, Computer Science Department, University of California San Diago (UCSD)https://cseweb.ucsd.edu/~jmcauley/ Music and Audio Research Laboratory (MARL), Dept. of Music and Performing Arts Professions, New York University (NYU)https://research.steinhardt.nyu.edu/marl/This is a big group. Its music informatics group is currently led by Prof. Juan Pablo Bello, Prof. Roger B. Dannenberg, CMUhttp://www.cs.cmu.edu/~rbd/A great computer music research, a camposer and a trumpet player. However, he is not accepting students. Centre For Interdisciplinary Research in Music Media And Technology (CIRMMT), McGill Universityhttps://www.cirmmt.org/A big lab. Research Groups in Industry…Pending An outline: Google Magenta, Spotify, Open AI, NAVIDA, Jukedeck - TikTok London, Tencent Music (TME), Netease Music… Useful LinksPages of Conferences ISMIR (International Society for Music Information Retrieval)https://ismir.net/“The ISMIR conference is held annually and is the world’s leading research forum on processing, searching, organising and accessing music-related data.” MIREX (Music Information Retrieval Evaluation eXchange)https://www.music-ir.org/mirex/wiki/MIREX_HOME“The Music Information Retrieval Evaluation eXchange (MIREX) is an annual evaluation campaign for MIR algorithms, coupled to the ISMIR conference.” SMC (Sound and Music Computing Network)http://www.smcnetwork.org/“The SMC Conference is a double-blind peer-reviewed international scientific conference around the core interdisciplinary topics of Sound and Music Computing.” NLP4MusAhttps://sites.google.com/view/nlp4musa“In this context, we propose the First Workshop on NLP for Music and Audio, a forum for bringing together academic and industrial scientists and stakeholders interested in exploring synergies between NLP and music and audio.” “Accepted papers will be published in the ACL anthology.” CSMT (Conference on Sound and Music Techonology)http://www.csmcw-csmt.cn/CSMT是中国音乐科技相关的研究者交流的很好平台，可以见到很多工业界和学术界的同行。CSMT推动者中国的音乐科技领域的发展合作，也在吸引世界范围内学术界的关注。 Of course, information here is limited due to my limited knowledge… More links pending… Dataset sources Datasets summarized by Prof. Alexander Lerch, Gatechhttps://www.audiocontentanalysis.org/data-sets/A grrrrrreat list of useful music datasets. Brief tags are attached there (e.g. MIDI or not? Labelled or not? Rhythm or melody? Monophonic or Polyphonic…) Datasets summarized by UPF Compmusichttps://compmusic.upf.edu/datasetsMainly about Indian art music, Turkish Makam music and Beijing Opera Magenta Datasetshttps://magenta.tensorflow.org/datasetsMainly about Bach Doodle Dataset, Groove MIDI Dataset, MAESTRO and NSynth Blogs Computer Music Conferences Deadline by Yixiao Zhang’s Bloghttps://yixiao-music.github.io/?sub=SYM,AI,OTHER,AUOSee also: his zhihu Chinese Blog of rogerkeanehttp://blog.sina.com.cn/rogerkeaneA little bit old Sina blog. He studied in GaTech. 很有生活趣味的一个博客! Intelligent sound engineering by Prof Joshua D ReissIntelligent sound engineering Chris Donahuehttps://chrisdonahue.com/ …To be expanded And… Hey! And my blog here! Other Useful Links dblp: computer science bibliographyhttps://dblp.uni-trier.de/“The dblp computer science bibliography provides open bibliographic information on major computer science journals and proceedings.” Papers With Code: The latest in machine learninghttps://paperswithcode.com/“Papers With Code highlights trending ML research and the code to implement it.” imslp: download sheet musichttps://imslp.org/wiki/Main_Page“The International Music Score Library Project (IMSLP), also known as the Petrucci Music Library after publisher Ottaviano Petrucci, is a subscription-based project for the creation of a virtual library of public-domain music scores.” - wiki word2tex and tex2wordhttps://www.chikrii.com/ Google scholar, Media, Wiki, Zhihu, CSDN…]]></content>
      <tags>
        <tag>introduction</tag>
        <tag>guide</tag>
        <tag>music</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My Music Explorations (from 2017 to 2020)]]></title>
    <url>%2F2020%2F07%2F18%2Fmy-music-explorations%2F</url>
    <content type="text"><![CDATA[Welcome! I am Tongyu Lu (路通宇)! This is an introduction of my explorations on composing! This is an introduction of my explorations and my musical pieces! I am far from professional. Instead, I hope that my explorations on composition can inspire more music lovers to try bravely! If my music fails to be played here, it may be because of copyright barriers. You can click the links and listen to them in outside webpages. You can also find my music in my SoundCloud Page or Netease Music Personal Page Early explorations My aunt taught me piano when I was a little boy! In around 2012, I began practicing music myself: transcripting the pieces I like into piano scores and practicing piano skills. I composed my first collection of 12 Scottish and Irish folk song styled pieces in 2018 and published it here: Lucainiao’s Musical Adventures in Ireland (路菜鸟梦游爱尔兰) Here are several pieces in this collection: Reel In the Summer (夏日的舞会)Wonderland (秘境) Trials on orchistral works I tried composing orchistral works in 2018 just for fun: Collection of 6 pieces: The Walking of Insects (惊蛰)Here are two of this collection: Short orchistral piece: The Walking of Insects (惊蛰) Short orchistral piece: Spring Birds (春天的鸟) The Walking of Insects (惊蛰) was performed in Shenzhen College Music Festival of 2019. Further, I tried to compose a march and an overture in Chinese style: Lucainiao’s Trial on Orchestral Pieces In 2020, I realized that I lacked professional skills :) Then I began studying harmony myself… (I will modify many of my pieces in the future because I am not satisfied with my composing skills in the past… After relistening my works, I found a lot of flaws. Anyway, I am kind of surprised that I composed them even if I did not learn harmony and other basics about composing!) By the way, I am currently fancinated with Brahms Symphony No.4! Trials on chamber music and piano pieces Meanwhile, I tried to compose solo piano pieces and chamber music. In early 2019, I composed a few pieces trying to depict my feelings about winter: Two Quartets and a piano Etude: Charming winterHere are the two Quartets: Quartet: Snow and Flying Leaves Quartet: Charming Winter I collected my early piano solo works. Although I play the piano, it is still challenging to write friendly piano pieces (especially when I want to express beauty of skills). I collected many of my piano solo works (old ones and new ones) here: Piano solo collection of 7 pieces: Lucainiao’s Trial on Piano PiecesHere are the two of them: Etude in c minor - phone ringtone Etude in g minor - autumn rain After reflections on my early chamber music, I found that cooperation between different voices is important. Then I tried more: Chamber music collection of 4 pieces: Lucainiao’s Trial on Chamber MusicHere are the two of them: Duet for violin and piano: Crazy bug Fantasia Trio Fantasia in c# minor - for Flute, Cello and Piano In late 2020, I plan to explore orchestration topics, and I tried to orchestrate Beethoven’s Piano Sonata No.30, 2nd movement. Orchestration - Beethoven Piano Sonata No.30 - 2 Prestissimo; Links: SoundCloud | Netease Explorations on jazz In 2018, inspired by a friend who plays saxphone, I tried to write several works in Jazz style. After trying this, I found myself arrested! I began to add something jazzy in my works in late 2019 and early 2020. I created my first collection for my jazzy works in 2018 and early 2019. Collection of 8 pieces: JAZZ rehearsal roomHere are the three of them: Rehearsal room for experts (高手的排练室) Are you feeling bored (是否感觉无聊) A flock of entertainers (喜剧仪仗队) I then began to read the Jazz Piano Book in early 2020, and learned several tricks! Then I composed a few jazz piano solos (and even played and recorded some of them): Fantasia for Christmas in C major My blind mind In late 2020, I began trying to compose pieces for brass ensambles. Here are several of such pieces: Jazz Sextet: 6+4; Links: SoundCloud | Netease Jazz Sextet: Walking Freely; Links: SoundCloud | Netease Brass Quintet in E flat major - A Happy Festival; Links: SoundCloud | Netease]]></content>
      <tags>
        <tag>introduction</tag>
        <tag>music</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome! I am Tongyu Lu! 我是路通宇!]]></title>
    <url>%2F2020%2F07%2F18%2Fpersonal-page%2F</url>
    <content type="text"><![CDATA[Thank you for coming to know about me!If “Tongyu Lu” is hard to pronounce… Try T’ungyü Lu… Or… Tony Lu! hahhh… About Me Currently studying at Harbin Institute of Technology (Shenzhen) as an undergraduate student in Electronic Information Engineering major. I am interested in music informatics, including automatic music transcription, music generation and computational musicology. I am an amateur composer. My ideas and my works are introduced here My photo in LA Universal Studio ![me.jpg](https://i.loli.net/2020/07/18/Y9hEZvTqo8Fe4bD.jpg) My Musical MindsInstead of using DAW, I keep composing music with staff sheets. Although I cannot create impressing sound effects in this manner, I think it is more important to express the musical information in a higher level. Some of my understandings on music composing and appreciation are introduced in the page of my music thoughts. Hope that you can find something inspiring in that article. And here are a few of my pieces! If my music fails to be played here, you can click the links and listen to them in outside webpages. You can also find my music in my SoundCloud Personal Page or Netease Music (网易云音乐) Personal Page Journey to a Memory (翻出一个回忆); Links: SoundCloud | NeteaseLu Tongyu · Journey to a memory 翻出一个回忆 - For woodwinds and strings Jazz Sextet: Walking Freely; Links: SoundCloud | NeteaseLu Tongyu · Jazz Sextet in F major - Walking Freely Jazz Quartet: Rehearsal Room for Experts (高手的排练室); Links: Netease | SoundCloudLu Tongyu · Rehearsal Room for Experts 高手的排练室 Jazz Piano Work: Fantasia for Christmas in C Major; Links: SoundCloud | NeteaseLu Tongyu · Fantasia for Christmas in C major Brass Quintet in E flat major - A Happy Festival; Links: SoundCloud | NeteaseLu Tongyu · Brass Quintet in E flat major - A Happy Festival Etude in F Major - Echoing; Links: SoundCloud | Netease Jazz Sextet: 6+4; Links: SoundCloud | Netease Duet for Violin and Piano: Crazy Bug; Links: SoundCloud | Netease Short Orchistral Piece: Spring Birds (春天的鸟); Links: SoundCloud | Netease Early Work: Wonderland (秘境); Links: SoundCloud | Netease]]></content>
      <tags>
        <tag>blog_start</tag>
        <tag>introduction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN Note - Back Propagation Alogorithm]]></title>
    <url>%2F2020%2F04%2F03%2FCNNnote2-BP_algorithm%2F</url>
    <content type="text"><![CDATA[by lucainiaoge IntroThis is a detailed derivation and summerization on back propagation (BP) algorithm in fully-connected neural networks and CNN: References and helpful links Reference 1: CNN tutorial note of Jake Bouvrie Reference 2: A helpful CSDN blog in Chinese, author - xqp_dream Prerequisites for this noteneural network concepts, CNN concepts, vector calculus First Part: Review – BP (back propagation) in fully connected layersReview of Feedforward and BP formulaJust a review, not a tutorial! How to derive the formula? Please go to learn neural network basics.Give a link of tutorial from Andrew Ng Detailed derivation of BP formula Second Part: math in CNN – representation and BPMath formalization of CNN feeding forward BP for convolutional layers The start of error terms and dealing with BP of the $L^{th}$ layer (which is a pooling layer) Dealing with BP of the $(L-1)^{th}$ layer (which is a convolutional layer) BP for CNN – a summarization]]></content>
      <tags>
        <tag>math</tag>
        <tag>machine_learning</tag>
        <tag>CNN</tag>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Course Note - Propositional Logic - BN and inference UCLA Adnan Darwiche]]></title>
    <url>%2F2020%2F03%2F18%2Fcourse_note1-propositional_logic-BN_and_inference_UCLA_Adnan%2F</url>
    <content type="text"><![CDATA[by lucainiaoge IntroThis is a fundamental course for those who wanna have a thorough understanding of Baysian Networks: bilibili course link: UCLA Bayesian Networks and Inference - Adnan Darwiche youtube course link: UCLA Bayesian Networks and Inference - Adnan Darwiche When studying the course, I was somehow confused partly because the lecture goes too fast. I have to frequently halt and comprehend the slides. Luckily, the slides are very well arranged.Through the first several courses, we are lead to discover the mathematical essence of Baysian Networks (BN). Prerequisites for this noteKnowing what sets are, knowing basic boolean logic, a mathematical mind Purpose of this noteSet up the foundation of probability calculus. Link logical calculation with set calculation. Constructing Propositional LogicMotivation: There will be a great deal of definitions. My understanding is that they are like fundations of the magnificent building of probibilistic inference for what we interpret as “events” are explained with the Propositional Logic. Definition (propositional variable and sentence/event): a propositional variable $p$ is defined on a discrete set P;a sentence or an event $\alpha$ is an assignment of propositional variables. $\alpha$ can hold true or false In other words,$(p=p_0)=\alpha \in \lbrace \pmb{true},\pmb{false} \rbrace$In the following discussions, we take True as T and False as F. e.g. propositional variable: B=”Burglery”, E=”Earthquake”, A=”Alarm”; a sentence: $\alpha$=(B=T,A=F) When we have a lot of propositional variables, the combinitions of possible circumstances are exploding exponentially. To efficiently deal with that is one of our tasks.First we have to define such possible circumstances with term “world”: Definition (world): a world $w$ is an assignment for all propositional variables $p_i$. We can treat worlds as smallest elements of the whole set $\Omega$. e.g. $p_B$=”Burglery”, $p_E$=”Earthquake”, $p_A$=”Alarm”$w_1=[(p_B,p_E,p_A)=(T,T,T)]$$w_2=[(p_B,p_E,p_A)=(T,T,F)]$$w_3=[(p_B,p_E,p_A)=(T,F,T)]$$w_4=[(p_B,p_E,p_A)=(T,F,F)]$$w_5=[(p_B,p_E,p_A)=(F,T,T)]$$w_6=[(p_B,p_E,p_A)=(F,T,F)]$$w_7=[(p_B,p_E,p_A)=(F,F,T)]$$w_8=[(p_B,p_E,p_A)=(F,F,F)]$$\Omega=\lbrace w_1,w_2,…,w_8 \rbrace$ Definition ($w \models \alpha$): a world $w$ holds true if a sentence $\alpha$ is true. e.g. given sentences: $B=[p_B=T]$(means Burglery happens), $E=[p_E=T]$, $A=[p_A=T]$, $\neg B=[p_B=F]$ and so forthThen, $w_1 \models B$, $w_5 \models \neg B$ Definition (model of sentence $\alpha$): $Mods(\alpha)=\lbrace w:w \models \alpha ,w\in \Omega \rbrace$Model of a sentence is a set. It’s obvious that model of a sentence is unique and given a set of worlds there is a unique sentence whose model is the set. e.g. $Mods(B)=\lbrace w_1,w_2,w_3,w_4 \rbrace$ Until now, we know that: We are studying Propasitional Variables which are discrete A sentence/event is a function for Propasitional Variable vectors (i.e. a value assignment). The function value domain is $\lbrace \pmb{true},\pmb{false} \rbrace$ A world is a sentence considering all Propasitional Variables. Model of a sentence is a set whose elements are worlds that hold true when this sentence holds true. Hope you are getting excited: we are linking the logic world of Ture or False with the world of sets. We can really define logical calculations. Propositional Logic CalculationDefinition (Basic Propositional Logic Calculation): given sentence $\alpha$ and $\beta$$\alpha \wedge \beta $ is such a sentence that $Mods(\alpha \wedge \beta)=Mods(\alpha)\cap Mods(\beta)$$\alpha \vee \beta $ is such a sentence that $Mods(\alpha \wedge \beta)=Mods(\alpha)\cup Mods(\beta)$$\neg \alpha $ is such a sentence that $Mods(\neg \alpha)=\Omega \setminus Mods(\alpha)$You see? Your familiar logical calculation “and”,”or” and “not” is actually set calculation “intersection”,”union” and “complement”. Definition (relationships of sentences): given sentence $\alpha$ and $\beta$$\alpha$ is consistent or satisfiable $\Leftrightarrow$ $Mods(\alpha)\neq \emptyset$$\alpha$ is valid $\Leftrightarrow$ $Mods(\alpha)= \Omega$ $\Leftrightarrow$ $\models \alpha$ $\Leftrightarrow$ $\alpha = True$$\alpha$ and $\beta$ are equivalent $\Leftrightarrow$ $Mod(\alpha)=Mods(\beta)$$\alpha$ and $\beta$ are mutually exclusive $\Leftrightarrow$ $Mods(\alpha)\cap Mods(\beta)=\emptyset$$\alpha$ and $\beta$ are exhaustive $\Leftrightarrow$ $Mods(\alpha)\cup Mods(\beta)=\Omega$$\alpha$ implies $\beta$ $\Leftrightarrow$ $Mods(\alpha)\subset Mods(\beta)$ $\Leftrightarrow$ $\alpha \models \beta$I think those definitions are to remind people the importance of such relationships. Also helping people to communicate in a more logical manner… A few more definitions of Propositional Logic calculation:Definition (More Propositional Logic Calculation): given sentence $\alpha$ and $\beta$$\alpha \rightarrow \beta $ $\Leftrightarrow$ $\neg \alpha \vee \beta$$\alpha \leftrightarrow \beta $ $\Leftrightarrow$ $(\alpha \rightarrow \beta)\wedge (\beta \rightarrow \alpha)$Worth noticing those definitions.$\alpha \rightarrow \beta $ is in fact telling you that $\alpha$ implies $\beta$, or contraposition. $\alpha \rightarrow \beta = True$ means: whenever $\alpha$ is true, $\beta$ is true. So $Mods(\alpha)\subset Mods(\beta)$. (Think the worlds!)$\alpha \leftrightarrow \beta $ is actually XNOR. $\alpha \leftrightarrow \beta = True$ means that $\alpha$ and $\beta$ are equivalent. Several examples: given sentences: $B=[p_B=T]$, $E=[p_E=T]$, $A=[p_A=T]$, $\neg B=[p_B=F]$ and so forth$\alpha=(E \vee B)\rightarrow A$$Mods(\alpha)=\lbrace w_1,w_3,w_5,w_7,w_8 \rbrace$$\beta=(E \rightarrow B)$$Mods(\beta)=\lbrace w_1,w_2,w_5,w_6,w_7,w_8 \rbrace$then, $Mods(\alpha \wedge \beta)=\lbrace w_1,w_5,w_7,w_8 \rbrace$ Calculating intersection is in fact adding information, and we throw away world elements by doing this. This is compactible with information theory!]]></content>
      <tags>
        <tag>machine_learning</tag>
        <tag>BN</tag>
        <tag>logic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harmonic Constraint in Music Spectrogram Reconstruction]]></title>
    <url>%2F2020%2F01%2F15%2FHarmonic_Constraint_in_Music_Spectrogram_Reconstruction%2F</url>
    <content type="text"><![CDATA[It is assumed that readers have known the concepts about harmonic (overtone) structure of musical notes. And readers are recommended to know the non-negative matrix factorization (NMF) algorithm. But anyway, I tried to briefly elaborate it in this article. This article gives a brief review of harmonic constraints on timbre dictionary. Such harmonic constraints are useful in helping non-negative matrix factorization (NMF) algorithm to converge into a musically meaningful result, which could enable automatic music transcription (AMT) and music source separation (MSS). Noticing the disadvantages of the traditional NMF algorithm and hard-harmonic-constraints, this article proposes a dB-trick for NMF which could loosen the non-negativity constraints, and a soft-harmonic-constraint method based on regularization which gives more freedom to the parameter compared with the existing hard-harmonic-constraints. To view this article, please download:Harmonic_Constraint_in_Music_Spectrogram_Reconstruction]]></content>
      <tags>
        <tag>MIR</tag>
        <tag>AMT</tag>
        <tag>research</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reflections on Linear Algebra - Range and Nullspace]]></title>
    <url>%2F2020%2F01%2F13%2FLinear_Algebra_Reflection_Range_and_Nullspace%2F</url>
    <content type="text"><![CDATA[This article assumes readers have grasped the basic concepts of linear algebra (e.g. inner product space, subspace, rank of matrix, solving linear equations with Gaussian elimination etc.). When I was learning about convex optimization, I encountered a statement about feasibility: given constraint $Ax=b$, if $b\notin R(A)$, then this problem is not feasible. I was wondering what $R(A)$ was. After reading this article, hope that the answer will be trivial to you! To view this article, please download:Reflections on Linear Algebra: Range and Nullspace]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></title>
    <url>%2F2019%2F10%2F02%2Fmath-modling-2019CUMCM-answer-2%2F</url>
    <content type="text"><![CDATA[by lucainiaoge 这篇文章讲解2019年数学建模国赛B题的求解历程。这是第二篇。 基础知识required：运动学和牛顿力学，空间向量的概念，最好有点反馈控制的知识（没有也行） 本文物理量如果无特殊说明一律采用国际单位制 若无法显示公式，或者出现[Math Processing Error]，请尝试shift+F5或者ctrl+shift+F5刷新页面 Restart: Operate in 3D World上回书说道，为了解决前两问，我们搭建了逼真的物理引擎，然后建立了竖直方向施力的反馈机制。上一文章链接：2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 但是，我们怎么让我们的模型适用于三维世界？我们还有哪些没有做？捋了一遍，发现还差好多！具体总结如下： 让我们逐(man)一(man)解决！ Solution: Get Feedback and Adjust在这里依然使用反馈的思路，只不过不能是简单地画一个单调递减函数就能解决的问题了。这回，我们要按照控制论的思想实时进行控制了！ 什么是控制论的思想？就是：首先，我们有一个目标的值，叫做期望值。假如我们想让$\Delta t$时间后，被控制物体的位置$\pmb{r}(t+\Delta t)$变到$\pmb{r_0}$，那么我们就说期望值为$\pmb{r_0}$。 在这里请允许我借用概率论中求期望的符号:$E[\pmb{r}(t+\Delta t)]=\pmb{r_0}$ 请注意：这里的$E[·]$作用的对象是我们要调整的时变量，我们假定$E[·]$对于这些时变量的期望具有线性性质，这么认为的理由在后面会有解释。 那么接下来，我们就要竭尽所能达成这个期望。我们能干什么？当然是：调整力！牛顿力学的体系下，我们可以认为：力是改变物体运动状态的原因！ 接下来，我是用PID（比例积分微分）的方法来进行控制。什么是PID？可以自行百度，这里只是一个名称罢了，代表一种方法，没必要知道工程上如何使用。实现这道题的要求，进行比例控制就够了。 如果仅使用比例控制，则$E[x]=K_p x$，$K_p$为比例系数。也就是下一次的输出量为期望值乘以一个系数。下文中，出现“求期望”字样，指的是获取目标量的期望值，不是统计学中的期望！ PID in problem ①&amp;②①好说，每次将要碰撞（球中心距离距鼓面距离等于球的半径）的时候进行一次判断，如果此时球在水平面的投影不落在鼓的投影内部，那么就判负。 主要讲如何实现②：控制拉力合力的水平分力，使鼓始终跟随球走。 力和位置，一个是二阶量，一个是零阶量，无法直接得出力的控制方程（就是决定下一时刻的输出量的计算式）。有一个办法：使用串级PID，不过这里我不打算采用这样的方法。 还有一个办法：我们连推两阶，就可以获取力的控制方程了。 下一时刻，鼓水平位置的期望值为球水平位置：$E[\pmb{r_d}(i+1)]=\pmb{r_b(i)}$(注意，这里向量都是二维向量)考虑运动方程$\pmb{r_d}(i+1)=\pmb{r_d}(i)+\pmb{v_d}(i)\Delta t$由于实际世界是连续的，我们的迭代步长可以取得足够小，使得：$|\pmb{v_d}(i+1)-\pmb{v_d}(i)|&lt;\epsilon$所以运动方程还可写为：$\pmb{r_d}(i+1)=\pmb{r_d}(i)+\pmb{v_d}(i+1)\Delta t$ 运动方程两边求期望得到：$E[\pmb{v_d}(i+1)]=(E[\pmb{r_d}(i+1)]-\pmb{r_d}(i))/\Delta t=(\pmb{r_b}(i)-\pmb{r_d}(i))/\Delta t$记$\pmb{r_{db}}(i)=(\pmb{r_b}(i)-\pmb{r_d}(i))$则有$E[\pmb{v_d}(i+1)]=\pmb{r_{db}}(i)/\Delta t$仅使用PID比例控制，则可以拆掉期望运算$\pmb{v_d}(i+1)=\pmb{K_{pv}}\pmb{r_{db}}(i)/\Delta t$其中：$\pmb{K_{pv}}=diag(k_{vx},k_{vy})$又根据冲量定理，$\pmb{v_d}(i+1)=\pmb{v_d}(i)+\pmb{F_{xy}}(i)\Delta t/m_d$结合以上式子，经过简单代入和移项，可以推出： $\pmb{F_{xy}}(i)=\pmb{K_{pv}}\pmb{r_{db}}(i)m_d/(\Delta t)^2-\pmb{v_d}(i)m_d/\Delta t $这就是我们想要的控制方程，按照这个式子设置此时的合力即可使得鼓的水平位置趋向于球的水平位置！ PID in problem ③③的目的是：让鼓始终保持水平。为什么要让鼓保持水平？一个原因是让球更容易被接到；还有一个原因就是减少转动防止力的不均匀性使得鼓过不了多久就转翻了。对于力矩的推导相似，但是更复杂。具体呢：看下面吧！ 我们期望鼓的法向量$\pmb{n}(i+1)$为$z单位向量\pmb{e_z}$，即$E[\pmb{n}(i+1)]=\pmb{e_z}$根据转动定理和连续性，$\pmb{n}(i+1)=\pmb{n}(i)+\pmb{\omega}(i+1)\times\pmb{n}(i)\Delta t$两边求期望得到$E[\pmb{\omega}(i+1)]\times\pmb{n}(i)\Delta t=E[\pmb{n}(i+1)]-\pmb{n}(i)$代入$E[\pmb{n}(i+1)]=\pmb{e_z}$得到$E[\pmb{\omega}(i+1)]\times\pmb{n}(i)\Delta t=\pmb{e_z}-\pmb{n}(i)$我们仅使用比例控制PID，那么可以继续拆掉期望运算，得到：$\pmb{\omega}(i+1)\times\pmb{n}(i)\Delta t=\pmb{K_{pr}}(\pmb{e_z}-\pmb{n}(i))$其中，$\pmb{K_{pr}}=diag(k_{rx},k_{ry},k_{rz})$列出动力学公式$J\pmb{\omega}(i+1)=J\pmb{\omega}(i)+\pmb{M}(i)\Delta t$将动力学公式两边同时叉乘$\pmb{n}(i)$，结合已得到的式子，推出$\pmb{M}(i)\times\pmb{n}(i)\Delta t=J\pmb{K_{pr}}(\pmb{e_z}-\pmb{n}(i))/\Delta t-J\pmb{\omega}(i)\times\pmb{n}(i)$我们最终要得到$\pmb{M}(i)$&lt;所以把这个向量积方程解出来就好了。这个方程怎么解？我们先把和设定值$\pmb{M}(i)$无关的已知量（即时刻$i$的运动参量）挪到一边去，令$\pmb{S}(i)=J[\pmb{K_{pr}}(\pmb{e_z}-\pmb{n}(i))/\Delta t-\pmb{\omega}(i)\times\pmb{n}(i)]/\Delta t$那么这个方程就写成了$\pmb{M}(i)\times\pmb{n}(i)=\pmb{S}(i)$求解过程如下图最终得到： $\pmb{M}(i)=diag(-S_y(i)/n_z(i),S_x(i)/n_z(i),0)$这就是我们想要的力矩的控制方程，按照此式子迭代可以使得鼓始终保持水平 Problem ④&amp;⑤问题一个个解决，一半多的理论问题已经被搞定了！那么，三维碰撞怎么描绘？如何刻画使球竖直弹回的条件？ Describe 3D Collision先解决碰撞的迭代公式上图是碰撞瞬间部分物理量的示意图。我们需要研究N点两个物体的速度。这涉及到了：鼓在N点的线速度(质心速度和转动线速度之和)、鼓的转动角速度、碰撞点的位置、球的质心速度(之前假设不考虑球的自转)。让我们一一列方程！设鼓的厚度为$h$，球的半径为$R$，用单一字母代表原点到该字母表示点的向量，那么有$\pmb{PQ}=h\pmb{n}/2$,$\pmb{N}=\pmb{O}-h\pmb{n}/2$,$\pmb{Q}=\pmb{P}+\pmb{PQ}$我们知道$平面\sigma:n_x(x-x_Q)+n_y(y-y_Q)+n_z(z-z_Q)=0$其中$n_x,n_y,n_z$为鼓面单位法向量的分量那么球到鼓面的距离为$d_{ON}=|n_x(x_O-x_Q)+n_y(y_O-y_Q)+n_z(z_O-z_Q)|$每次迭代判断一次$d_{ON}\le R?$，如果成立则对碰撞进行迭代：设此时鼓的质心速度为$\pmb{v_d}$，鼓的角速度为$\pmb{\omega}$，$N$点处鼓的旋转线速度为$\pmb{v_{\tau}}$，N点鼓的质点速度为$\pmb{v_{N}}$，球的质心速度为$\pmb{v_d}$，那么我们有：$\pmb{v_{\tau}}=\pmb{\omega}\times\pmb{PN}$$\pmb{v_{N}}=\pmb{v_d}+\pmb{v_{\tau}}$下面，列写动力学方程更新下一时刻鼓和球的运动参量：对物体列冲量定理和碰撞系数定义式$\pmb{P}(i)=m_{b} \pmb{v_b}(i)+m_{d} \pmb{v_d}(i) $（前一时刻动量）$\pmb{P}(i+1)=m_{b} \pmb{v_b}(i+1)+m_{d} \pmb{v_d}(i+1) $（后一时刻动量）$I(i)=\sum \pmb{F}(i) \Delta t-\left(m_{b}+m_{d}\right) g \Delta t \pmb{e_z} $（外力冲量）$\pmb{P}(i)+\pmb{I}(i)=\pmb{P}(i+1) $（冲量定理）$e\left[\pmb{v_b}(i+1)-\pmb{v_d}(i+1)\right]=\pmb{v_d}(i)-\pmb{v_b}(i) $（碰撞系数定义） 上面$\pmb{\sum F}$为队员拉力的合力。可以求得下一时刻二者的质心速度 对系统列角动量守恒得到$J\pmb{\omega}(i)+\pmb{PO}\times m_b\pmb{v_b}(i)=J\pmb{\omega_d}(i+1)+\pmb{PO}\times m_b\pmb{v_b}(i+1)$整理得到下一时刻鼓的角速度 $\pmb{\omega}(i+1)=\pmb{\omega}(i)+\pmb{PO}\times m_b(\pmb{v_b}(i)-\pmb{v_b}(i+1))$ Describe the Condition in ⑤下面，我们需要探究碰撞瞬间满足什么条件，就可以将球竖直弹回。如果下一时刻球竖直弹回，那么一定有：$\pmb{v_b}(i+1)\times\pmb{e_z}=0$根据上一部分推出的鼓质心速度$\pmb{v_d}(i+1)$的计算公式，忽略重力影响，得到式子两边同时叉乘$\pmb{e_z}$并移项化简，得到：写成分量形式并运算向量积，得到： 这就是让球竖直弹回应该满足的条件。可以发现，如果不考虑碰撞过程中摩擦等情况，仅调用碰撞系数，球在下一时刻的弹回方向仅由鼓和球的水平质心速度决定，而且二者速度反向共线。 Problem ⑥将上一部分推导的条件写成二维向量的向量表达式：下面推导省去脚标$xy$，向量都是二维向量，$水平合力\pmb{F_{xy}}=\pmb{\sum T}$。将此式子中的$ \pmb{v_d}(i)$作为下一时刻鼓速度$\pmb{v_d}(i+1)$的参考值。移项，可以获得：$E[\pmb{v_d}(i+1)]=-(\frac{\pmb{\sum T(i)}-(m_b+m_d)g\pmb{e_z}}{(1+e)m_d}\Delta t + \frac{m_b-em_d}{(1+e)m_d}\pmb{v_b}(i))$仅采用比例控制PID，则可以拆掉期望运算：$\pmb{v_d}(i+1)=-\pmb{K_{pf}}(\frac{\pmb{\sum T(i)}-(m_b+m_d)g\pmb{e_z}}{(1+e)m_d}\Delta t + \frac{m_b-em_d}{(1+e)m_d}\pmb{v_b}(i))$其中$\pmb{K_{pf}}=diag(k_{fx},k_{fy})$结合冲量定理：$m_d\pmb{v_d}(i+1)-m_d\pmb{v_d}(i)=\pmb{\sum T(i)}\Delta t$令$\lambda=(em_d-m_b)/\Delta t$，再参考连续性并化简上式可以得到： $\pmb{F_{xy}}(i)=\pmb{\sum T(i)}=diag(\frac{\lambda k_{fx}-1}{1+k_{fx}},\frac{\lambda k_{fy}-1}{1+k_{fy}})\pmb{v_b}(i)$这就是迎接球使其竖直弹回需要设定的下一时刻水平合力迭代公式细心的小伙伴会发现：这个式子和之前问题②的迭代式子打架了，不过没关系，我们使用分段PID，在判断球快要碰撞时才采用此表达式。 Results: 3D movement先上效果图：如何实现的？之前我们的准备工作已经十分充足了！话不多说，现在只需要把迭代公式列一遍： Section 0: get several values$d_{ON}=|n_x(x_O-x_Q)+n_y(y_O-y_Q)+n_z(z_O-z_Q)|$ (球到鼓面的距离)set $\pmb{K_{pv}},\pmb{K_{pr}},\pmb{K_{pf}},\epsilon_d,\epsilon_v,\epsilon_{collide}$ Section 1: set $\pmb{M}(i),\pmb{F}(i), status \quad X$ 角动量设置$\pmb{S}(i)=J[\pmb{K_{pr}}(\pmb{e_z}-\pmb{n}(i))/\Delta t-\pmb{\omega}(i)\times\pmb{n}(i)]/\Delta t$$\pmb{M}(i)=diag(-S_y(i)/n_z(i),S_x(i)/n_z(i),0)$ 水平合力设置$if \quad d_{ON}&gt;\epsilon_d$ ……(距离较远，鼓追球)$\quad\pmb{F_{xy}}(i)=\pmb{K_{pv}}\pmb{r_{db}}(i)m_d/(\Delta t)^2-\pmb{v_d}(i)m_d/\Delta t $ ……(二维向量)$else$ ……(距离较近，鼓迎球)$\quad\lambda=(em_d-m_b)/\Delta t$$\quad\pmb{F_{xy}}(i)=diag(\frac{\lambda k_{fx}-1}{1+k_{fx}},\frac{\lambda k_{fy}-1}{1+k_{fy}})\pmb{v_b}(i)$…… (二维向量) 竖直合力设置$F_z(i)=min(F_X(i),T_{lim}(z_d(i)))$,$F_X(i)\in \lbrace F_I,F_{II},F_{III},F_{IV} \rbrace$ 根据第一问的状态机更新状态X(此处略去) Section 2: physics system rotation$\pmb{\omega}(i+1)=\pmb{\omega}(i)+\pmb{M}(i)\Delta t/J$……(转动定理)$\pmb{\Delta n}(i)=\pmb{n(i)}\times\pmb{\omega}(i)\Delta t$……(角速度造成的法向量瞬时变化量)$\pmb{n}(i+1)=\pmb{n}(i)+\pmb{\Delta n}(i)$……(更新法向量)$\pmb{n}(i+1)=\pmb{n}(i+1)/ | \pmb{n}(i+1) |$……(归一化消除长度误差)$\pmb{R_k^o}(i+1)=\pmb{R}(i+1)\pmb{R_k^o}(1)$……(更新鼓的径向,R为旋转矩阵) translationdrum：$\pmb{f_{d_{air}}}(i) = 0.5 C_d \rho S_d \pmb{v_d}(i) |\pmb{v_d}(i)|$$m_d \pmb{v_d}(i+1) = (-m_d g\pmb{e_z} + \rho V_d g\pmb{e_z} - \pmb{f_{d_{air}}}(i) + \pmb{F}(i))\Delta t + m_d \pmb{v_d(i)}$$\pmb{r_d}(i+1) = \pmb{r_d}(i) + \pmb{v_d}(i)\Delta t$ball：$\pmb{f_{b_{air}}}(i) = 0.5 C_b \rho S_b \pmb{v_b}(i) |\pmb{v_b}(i)|$$m_b \pmb{v_b}(i+1) = (-m_b g\pmb{e_z} + \rho V_b g\pmb{e_z} - \pmb{f_{b_{air}}}(i) + m_b \pmb{v_b(i)}$.$\pmb{r_b}(i+1) = \pmb{r_b}(i) + \pmb{v_b}(i)\Delta t$ collision$if \quad d_{ON}&lt;\epsilon_{collide}$$\quad\pmb{P}(i)=m_{b} \pmb{v_b}(i)+m_{d} \pmb{v_d}(i) $$\quad I(i)=\pmb{F}(i) \Delta t-\left(m_{b}+m_{d}\right) g \Delta t \pmb{e_z} $$\quad$$\quad\pmb{\omega}(i+1)=\pmb{\omega}(i)+\pmb{r_{db}}(i)\times m_b(\pmb{v_b}(i)-\pmb{v_b}(i+1))$ Section 3: judge system$if \quad d_{ON}&lt;\epsilon_{collide}$……(接不到球)$\quad if \quad (x_d-x_b)^2+(y_d-y_b)^2&gt;R_d^2$……(简单化处理)$\quad\quad fail=2$$if \quad v_b(i)&lt;\epsilon_{v}$……(球达到最高)$\quad if \quad d_{ON}&lt;0.4$……(球太低的判负)$\quad\quad fail=1$ Implementation: solve Q4 partially Q4: 当鼓面发生倾斜时，球跳动方向不再竖直，于是需要队员调整拉绳策略。假设人数为10，绳长为2m，球的反弹高度为60cm，相对于竖直方向产生1度的倾斜角度，且倾斜方向在水平面的投影指向某两位队员之间，与这两位队员的夹角之比为1:2。为了将球调整为竖直状态弹跳，请给出在可精确控制条件下所有队员的发力时机及力度，并分析在现实情形中这种调整策略的实施效果。 初始条件，按照我们的解读刻画如下：$\pmb{r_b}(1) =\left(\cos \left(\left(\frac{2 \pi}{N}\right)(k-1)+\frac{2 \pi}{3 N}\right) x, \sin \left(\left(\frac{2 \pi}{N}\right)(k-1)+\frac{2 \pi}{3 N}\right) x, \Delta H\right)$$\pmb{v_b}(1) =\left(\cos \left(\left(\frac{2 \pi}{N}\right)(k-1)+\frac{2 \pi}{3 N}\right) v_{x O y}, \sin \left(\left(\frac{2 \pi}{N}\right)(k-1)+\frac{2 \pi}{3 N}\right) v_{x O y}, 0\right)$ 代入初始条件，依然按照目标规划搜索三组比例系数，z方向直接调用第一问的最优结果(我们跑了很多次，由于搜索时间实在太长，每次都没跑完，所以在这里只取一个有代表性的来演示，论文中用的另一个结果)，得到第四问的决策结果和游戏过程： Complete the systemSolve problem ⑦这个问题是本队翔哥解决的，在这里直接贴出来原汁原味的总结：按照这个规则分解以后，代入问题4的初始条件，出来结果是这样子： System Summary and Robustness Analysis可以看到，大部分我们已经做完，就差最后一个验证了。计算接近度，我们使用的是相关系数。将上面搞出来的分力合成以后再次代入系统，发现：原来设定25000步，运行了11584步就输掉了。大概是因为重新合成的力没有引入反馈吧，误差会累计。不过结果已经相当不错了。计算一下接近度（标注part的是只算了11584步的接近度）发现即使力非常接近（都是1），运行轨迹也不是很接近。反馈的重要性可见一斑了。 一个遗憾：时间和能力有限，我们没有做分力代入系统的实时反馈以及考虑了力矩的合力分解。 啊写了好多！鲁棒性分析就贴图好了。结果是：系统至少可以支持碰撞系数0.68&lt;e&lt;0.99的变化，在此区间内可以无限颠球。 At last先写到这里吧，期间有许多细节也略过了，毕竟要总结的东西特别多，要抓住重点。 总觉得这次数学建模以后，我就可以去游戏公司开发3D游戏了！其实自己写出仿真程序还是很爽的，体验了一把决定论哲学下的上帝视角！今后用到控制、用到三维世界描述的时候，也会更加得心应手。 数学建模还是很占精力的，尤其是当你不是为了划水得奖而是认认真真研究问题的时候。不过闹了半天，我也没有用上什么特别先进特别现代的算法，也没用上什么近代的数学知识。不过也不要好高骛远，慢慢成长慢慢来，时间可以填补知识空白。 祝大家学业有成，参加数模的小伙伴遇到自己心仪的问题，有志于科研的小伙伴遇到自己心仪的课题（祝我自己）！ 2019数学建模系列传送门 2019数学建模国赛总结 Part 1 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 2 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 3 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 4 (2019 CUMCM Summary) 2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></content>
      <tags>
        <tag>math</tag>
        <tag>math_moding</tag>
        <tag>MATLAB</tag>
        <tag>physics</tag>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三维旋转矩阵和刚体旋转(Object Rotation in 3D Space)]]></title>
    <url>%2F2019%2F10%2F02%2Frotation-in-3D-space%2F</url>
    <content type="text"><![CDATA[by lucainiaoge 如何描绘一个向量在三维世界中的旋转？如何描绘一个物体在三维世界中的旋转？ 基础知识required：空间向量的概念 若无法显示公式，或者出现[Math Processing Error]，请尝试shift+F5或者ctrl+shift+F5刷新页面 Rotation of Vector 我们知道,在三维空间中，描绘一个三维列向量$\pmb{v}$逆时针绕着$z$轴旋转$\theta$角度，可以用这个矩阵来表示：那么$Rv$就是旋转之后的向量了！（如果不能理解，可以试试推导一下二维空间中向量绕原点旋转的矩阵）同理，绕着x轴,y轴旋转的矩阵如下： 接下来：我们试着求一下三维空间中任何一个向量$\pmb{n_2}$绕着原点旋转到$z$轴方向的矩阵：肯定要经历两次二维旋转！ 如上图：先绕$x轴$转到$xOz$平面，再绕$y轴$转到$z$方向。构造出来旋转矩阵：(上图中$\alpha \beta$ 的直角坐标计算公式就不给出了！)那么$\pmb{n_2}=\pmb{R_1} \pmb{R_2} \pmb{e_z}$ 我们令$\pmb{R_z}=\pmb{R_1R_2}$，即为将z单位向量$\pmb{e_z}$旋转到任意向量$\pmb{n}$的旋转矩阵（简记为z旋转矩阵）。$\pmb{R^{-1}}$即为将任意向量$\pmb{n}$旋转到$\pmb{e_z}$方向的旋转矩阵。 如何将任意向量$\pmb{n_1}$旋转到另一个任意向量$\pmb{n_2}$的方向？设$\pmb{n_1}$的z旋转矩阵为$\pmb{R_{z1}}$，$\pmb{n_2}$的z旋转矩阵为$\pmb{R_{z2}}$，那么先将$\pmb{n_1}$转到z轴，再将z轴转到$\pmb{n_2}$，得到$\pmb{n_2}=\pmb{R_{z2}}\pmb{R_{z1}^{-1}}\pmb{n_1}$ Rotation of Object我们已知某个物体初始时刻的任意质点的位矢$\pmb{r}(x,y,z)$。现在，我们想描述这个物体绕着原点随便转动，转动的角度按球坐标系刻画，为：$xOy平面极角\beta$，$z轴极角\alpha$。对应的z旋转矩阵为$\pmb{R_{z}}$。 如果知道这个物体某个质点$\pmb{P}$位于$\pmb{e_{z}}$方向时其他质点相对于这个质点的位矢为$\pmb{\Delta r}=(\Delta x,\Delta y,\Delta z)$，那么$\pmb{R_{z}P}+\pmb{\Delta r}$即为刚体任意其他质点的方向了~ 描述一个物体，只需要知道其一个位矢$\pmb{r}$（一般为质心），要旋转物体，只需要对$\pmb{r}$进行操作：$\pmb{R_{z}} \pmb{r}$，其余质点按照相对位置分配即可。]]></content>
      <tags>
        <tag>math</tag>
        <tag>physics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A brief introduction of lucainiaoge's website]]></title>
    <url>%2F2019%2F09%2F29%2Fblog-intro%2F</url>
    <content type="text"><![CDATA[by lucainiaoge 最近(2020.9.29)心血来潮想要开始写博客。 我想写这些东西（当然是我觉得值得记录并感兴趣的啦）： 个人介绍和研究 技术教程和链接搬运（造福人类嗷） 学习笔记 平时的思考结果 值得记录的经历和故事 可能还有很多不知道怎么分类的。无论如何，所有文章都可以在博客的边栏中“All articles”项目中找到，并且可以按照标签筛选哦！ 稍微YY一下博客将会侧重涉及的门类： 数学,物理学,信息科学,电子信息技术,音乐… 构想很宏大，那就从现在开始拧螺丝了~ 欢迎大家来一起交流。 A Guide through my website! (added on July 21, 2020) This website is for blogging myself and sharing my ideas! I will archive my researches/notes/thoughts/stories/works in this website! All articles are tagged and you can find all of them by clicking the all articles button in the menu on the left/top side. If you are reading a certain webpage, you can find a menu of contents on the bottom-right side of the page. (Feel free to comment below! You can use Markdown!) You can visit my personal page in this website to know about me! Hurry up, lucainiao! Stay ambitious!]]></content>
      <tags>
        <tag>blog_start</tag>
        <tag>introduction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1)]]></title>
    <url>%2F2019%2F09%2F29%2Fmath-modling-2019CUMCM-answer-1%2F</url>
    <content type="text"><![CDATA[by lucainiaoge 这篇文章讲解2019年数学建模国赛B题的求解历程。 基础知识required：运动学和牛顿力学，空间向量的概念 本文物理量如果无特殊说明一律采用国际单位制 若无法显示公式，或者出现[Math Processing Error]，请尝试shift+F5或者ctrl+shift+F5刷新页面 First Sight: Discovering and Creating 问题链接： 全国大学生数学建模大赛官网http://www.mcm.edu.cn/，进去点击“赛题与评奖”，即可获取赛题 我们做的是B题，同心鼓那个。起初我觉得这题很没劲，但做起来以后才觉得有意思。数学建模心路历程和经历已经写过了，可以看我博客的这套文章：2019数学建模国赛总结 Part 1 解决第一问： Q1. 在理想状态下，每个人都可以精确控制用力方向、时机和力度，试讨论这种情形下团队的最佳协作策略，并给出该策略下的颠球高度。 Solve by simulation我们拿到问题，首先考虑：我们要输出什么样子的结果？我们要输出“最佳协作策略”。既然要建模，一定要定量，那么这个策略也是定量的。想：协作需要确定什么量？以人为单位，每个人需要通过绳子提供拉力，拉力包含夹角和方向，同时也包含随时间变化的信息（毕竟游戏是一个过程）。 怎么刻画这些量？$\varphi_k - t \quad curve \quad and \quad \textbf{F}_k - t \quad curve$ $\varphi_k$是每个人施力夹角，$\textbf{F}_k$是每个人施力的大小 有了这个思路，我们就要确定这个有关每个人施力随时间变化的函数了。这个函数的目的：使得游戏得分最高。即：将函数输入游戏，游戏输出得分结果，我们要这么样的函数使得结果最优——优化问题。那么我们又要开始想：游戏如何输入这个函数？我们要构建游戏的真实过程！ 到此为止，我们其实已经进入正题上了——构建游戏的过程。说白了：物理仿真系统！ 仿真系统的构建，我们使用MATLAB，后面也一直在用MATLAB。我首先写了一个最简单的仿真系统——小球与地碰撞反弹的系统。 我们能参考的是什么？运动学方程，牛顿运动定律，碰撞定理。 以最简单的反弹系统为例，我们有这么几个微分方程：$\frac{dy(t)}{dt} = v(t)$$\frac{dv(t)}{dt} = g$$v(t+\Delta t)=-ev(t) \quad when \quad y(t)&lt;\epsilon$, e为碰撞系数 我们要做的是将其以时间步长$\Delta t$离散化以适应MATLAB仿真： $y(i+1)-y(i) = v(t) \Delta t$$v(i+1)-v(i) = g \Delta t$$v(i+1)=-ev(i) \quad when \quad y(i)&lt;\epsilon$ 好的，接下来就要代码实现了。其实，上面已经给出了迭代公式。离散化其实是对实际世界的近似。实际世界一切物理现象的发生可以看成是同时的，不会出现“上帝先确定某物体的加速度，再确定它的速度，最后确定它的位置”这种情况（至少用微分方程求解是这样的）。但我们迭代时候，就产生了这样的问题：我们有很多迭代公式，先迭代哪个？ 如果我们要求解差分方程组，也不存在这个问题，毕竟求解差分方程和求解微分方程是一个套路，最后得出的解与我们先解出哪个方程无关。可是迭代的话，我们先算谁后算谁，有一个因果性。我们一般按照高阶(eg:加速度，速度)到低阶(eg:速度，位置)的顺序来迭代。实例代码如下： 12345678for i=1:max_step t=t_step*i; v(i+1)=-g*t_step+v(i); y(i+1)=v(i-1)*t_step+y(i); if(y(i)&lt;eps) v(i+1)=-v(i)*0.998; endend 运行这个代码，可以得到这样的过程（横轴是时间）： Creat 1D Physical Simulation System我们其实完成了历史性的一步：构建了一个物理仿真系统。接下来，就是将其应用到题目中了。 题目第一问对实际施力情况可以做理想化处理。理想到什么程度？我们的把握是：让每个均匀站立的人可以做到对称施力——竖直分力相等，水平分力也相等使得八个人水平合力为0。那么，我们只需获取竖直方向合力，结合任何时刻人的高度和鼓的高度，即可得到每个人的分力以及力的夹角了。现在，我们的任务就是：设定每个时刻的竖直合力。 我们要考虑两个物体的运动。模型较为简单，迭代求解甚至可以考虑空气阻力、浮力等因素（不用获取解析形式，是数值方法的一大优点，这样可以使得我们考虑很多通过解析方法很难刻画的因素）。在我们直接列离散化之后的方程如下(为节省篇幅，一些明显易求得的物理量[例如截面积和体积]在这里不加以计算，仅做符号说明)： 符号说明： 脚标”d”代表”drum”，即“鼓”；脚标”b”代表”ball”，即“球”； “C”是空气阻力系数，对于球而言$C_b = 0.5$，对于圆柱而言$C_d = 0.73$ “V”代表体积，”S”代表截面积，”$F_z$”代表队员的竖直合力，”$\rho$”代表空气密度，”m”表示质量 指标$i$代表迭代变量（即时间），$\Delta t$为迭代步长 $f_{d_{air}}(i) = 0.5 C_d \rho S_d v_d(i) |v_d(i)|$……(鼓的空气阻力)$f_{b_{air}}(i) = 0.5 C_b \rho S_b v_b(i) |v_b(i)|$……(球的空气阻力)$m_d v_d(i+1) = (-m_d g + \rho V_d g - f_{d_{air}}(i) + F_z(i))\Delta t + m_d v_d(i)$……(鼓的速度迭代公式)$m_b v_b(i+1) = (-m_b g + \rho V_b g - f_{b_{air}}(i))\Delta t + m_b v_b(i)$……(球的速度迭代公式)$z_d(i+1) = z_d(i) + v_d(i)\Delta t$……(鼓的位置迭代公式)$z_b(i+1) = z_b(i) + v_b(i)\Delta t$……(球的位置迭代公式)$if \quad z_b(i)-z_d(i)&lt;\epsilon \quad :$(碰撞迭代公式) 碰撞的迭代公式的推导由来如下图所示其中P代表冲量，这几个式子是列了冲量定理和碰撞系数e的定义式。 How to Set $F_z(i)$上面的物理模型想要运行，必须实时输入$F_z(i)$，所以我们需要构建一个确定$F_z(i)$的系统。想了半天，憋出来一个想法：使用状态机并引入反馈。 什么是状态机？就是设置决策者在不同情况下进入不同的状态，不同的状态下做出不同的决策。这里，我们设置了这么几种状态： 在不同状态下，设定不同的$F_z(i)$值，即：$F_z(i)\in \lbrace F_I,F_{II},F_{III},F_{IV} \rbrace$，每次迭代都根据目前状态设置$F_z(i)$即可。 我们探索了几个$F_z(i)$的值，比如$[F_I,F_{II},F_{III},F_{IV}]=[(g+6)m_d,0,gm_d,(g+3)m_d]$，设置鼓向下运动最大速度为0.6m/s，按照上述图片的状态转换规则进行迭代，会得到这样的结果(在这里我也加入了和地面碰撞反弹的判断)(红线是球的运动轨迹，蓝线是鼓面的运动轨迹，碰撞判断时没有考虑球的大小)： 会发现，要不就往下掉，要不就越飞越高，没有实现稳定地控制。怎么让它稳定？我们想到了引入负反馈！也就是，鼓太高的时候，就减少向上的力使鼓保持在下面。 反馈很简单，设置一个分段函数$T_{lim}(y)$如下： 每次迭代按照状态设置完参考竖直力之后，按照这个函数限制一下$F_z(i)$(这个函数是力的上限，如果设定值大于$T_{lim}(z(i))$，就将$F_z(i)$设置为$T_{lim}(z(i))$)。于是得到了令人满意的结果： 鼓的初始高度对稳定结果没有影响，实验结果如下： 接下来，我们要获取最优值，“最优”是什么？当然是游戏得分多！所以，我们首先要实现：接球数目最多。接下来，我们能接球的时间越长越好。当然，还要考虑一下成员的操作难易度。这个操作难易度定义为$\xi = |\frac{1}{Iter}\sum_{i=1}^{Iter}y_d(i)-y_{conft}|$，其中$y_{conft}$为舒适高度，我们取的0.8，$Iter$为游戏能进行的最大迭代次数，$t_{step}$是迭代步长。 搜索反馈函数的两个lim值和stageI的参考值，求解这个目标规划：进一步考虑了球的大小，得到最优结果如下： 基本符合预期！在迭代时间极限40秒内，击打了80个球！ 接下来，将分力分解给每个队员： 设每个队员都有一个舒适施力的值服从正态分布，记为$h_{pull}-N(1.1,0.1^2)$，设N个队员之间夹角为$\gamma=2\pi /N$，人到鼓边缘的距离$d_{max}=\frac{0.3}{sin(\gamma/2)}-0.2$ (其中0.3=0.6/2,即两人之间距离的一半，0.2为鼓的半径)。那么，任意时刻，每个人施力夹角为：$\varphi_k(i)=arctan( \frac{h_{pull_k} - y_d(i)}{d_{max}} )$，每个人施力大小为：$T_k(i)=T_z(i)/(Nsin \varphi_k(i) )$ 于是，我们就有了分解之后队员的力与时间的指导规律： 开始向第二问进发！ Second Process: Move to the 3D World Q2. 在现实情形中，队员发力时机和力度不可能做到精确控制，存在一定误差，于是鼓面可能出现倾斜。试建立模型描述队员的发力时机和力度与某一特定时刻的鼓面倾斜角度的关系。设队员人数为8，绳长为1.7m，鼓面初始时刻是水平静止的，初始位置较绳子水平时下降11 cm，表1中给出了队员们的不同发力时机和力度，求0.1 s时鼓面的倾斜角度。抱歉，不小心把答案也贴上来了（不要紧不要紧） Solution - How to rotate?第二问其实就是让我们专注建立物理模型了！力都给我们给好了，也没有排球什么事儿，就让鼓运动就是了！关键是：怎么让鼓转起来？ 当然是：转动定理！（不知道的小伙伴们快快复习大物！） 如上图所示：鼓的一些向量属性。$\pmb{R_k}(k=1,2…8)$代表[鼓的中心点]到[鼓面边缘&amp;每个人所在竖直平面交点]的向量，它的长度是$D/2$，D是鼓的直径，方向记为$\pmb{R^o_k}(k=1,2…8)$；$\pmb{T_k}(k=1,2…8)$代表每个人拉力的大小和方向；$\pmb{n}$为鼓的法向量。 另外，我们令：$\pmb{M_k}(k=1,2…8)$为每个人的拉力对鼓造成的力矩，$\pmb{\omega}$为鼓的角速度，$\Delta\theta$为鼓的法向量偏离竖直方向$\pmb{e_z}$的角度，$\pmb{\Delta n}为法向量的瞬时移动量$，$J$为鼓的转动惯量。 转动惯量的计算：将鼓看作圆管状模型，忽略牛皮的质量。制作鼓常用的木材为梧桐木，梧桐木的密度$\rho_{wood}=0.8\times 10^3 kg/m^3$，根据题中给的鼓的尺寸和质量可以求得梧桐木厚度为w=0.0163m；根据转动惯量的定义，鼓相对侧面中轴的转动惯量为：\begin{equation}J=\int_{D/2-w}^{D/2} \int_{-h/2}^{h/2} 2 \pi \rho_{wood} (r^2+z^2)rdrdz=0.1517kg·m^2\end{equation} 那么，我们依然直接列出离散化之后的动力学迭代公式： 转动迭代$\pmb{M_k}(i)=\frac{D}{2}\pmb{R^o_k}(i)\times\pmb{T_k}(i)(k=1,2…8)$……(每个人造成的分力矩)$\pmb{M}(i)=\sum^{8}_{k=1}\pmb{M_k}(i)$……(拉力对鼓的合力矩)$\pmb{\omega}(i+1)=\pmb{\omega}(i)+\pmb{M}(i)\Delta t/J$……(转动定理)$\pmb{\Delta n}(i)=\pmb{n(i)}\times\pmb{\omega}(i)\Delta t$……(角速度造成的法向量瞬时变化量)$\pmb{n}(i+1)=\pmb{n}(i)+\pmb{\Delta n}(i)$……(更新法向量)$\pmb{n}(i+1)=\pmb{n}(i+1)/ | \pmb{n}(i+1) |$……(归一化消除长度误差)$\pmb{R_k^o}(i+1)=\pmb{R}(i+1)\pmb{R_k^o}(1)$……(按照新的法向量更新鼓的径向,R为旋转矩阵) 平动迭代$\pmb{f_{d_{air}}}(i) = 0.5 C_d \rho S_d \pmb{v_d}(i) |\pmb{v_d}(i)|$……(鼓的空气阻力)$m_d \pmb{v_d}(i+1) = (-m_d g\pmb{e_z} + \rho V_d g\pmb{e_z} - \pmb{f_{d_{air}}}(i) + \pmb{\sum T}(i))\Delta t + m_d \pmb{v_d(i)}$……(鼓的速度迭代公式)$\pmb{r_d}(i+1) = \pmb{r_d}(i) + \pmb{v_d}(i)\Delta t$……(鼓的位置迭代公式) 上面有待确定的量旋转矩阵$\pmb{R}$的求解: 见博文三维旋转矩阵和刚体旋转(Object Rotation in 3D Space)$\pmb{\sum T}(i)$: 按照每一时刻表格中给出的力合成为矢量。$\pmb{T}(i)=\sum^{8}_{k=1}\pmb{T_k}(i)$$\pmb{T_k}(i)$: 方向为$\pmb{T_k^o}(i)=\frac{[cos(2\pi (k-1)/N),sin(2\pi (k-1)/N),tan\varphi_k(i)]}{|[cos(2\pi (k-1)/N),sin(2\pi (k-1)/N),tan\varphi_k(i)]|}$,其中$\varphi_k(i)$的求法在第一问中已给出 至此，我们第二问完全可以求解了。将题目表格中的数据代入，就可以轻松求出任何时刻鼓的位置和角度了！结果如下： Bonus Experiment我们还没讨论绳长是否可变和绳子松弛的情况。其实无论如何，影响的都是力$\pmb{T_k}(i)$的$\varphi_k(i)$。假设中，拉力在鼓面的投影一定平行于径向（这样就避免了鼓的水平自转），如果绳长可变，那么可以保证在人的水平位置不变的条件下，拉绳子的手可以竖直运动；如果绳长不可变，即假设人拉绳子总是牵着末端，那么人必定在水平移动。 无论哪种假设，都只是对$\varphi_k(i)$的计算公式产生影响。我们看看$\varphi_k(i)$对倾斜角度（第二问表格里的结果）有什么影响？因为施力时间比较短，所以假设施力过程中$\varphi_k=\varphi$不变。我们得到了这样的结果： 对情形2（施力不均匀）拟合了一下，是一个正弦函数——$\theta = 4^o sin\varphi$可以发现对结果影响比较小，0.2秒时间内最多影响4° 2019数学建模系列传送门 2019数学建模国赛总结 Part 1 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 2 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 3 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 4 (2019 CUMCM Summary) 2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></content>
      <tags>
        <tag>math</tag>
        <tag>math_moding</tag>
        <tag>MATLAB</tag>
        <tag>physics</tag>
        <tag>simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KNN实现手写数字识别(KNN handwriting recognization)]]></title>
    <url>%2F2019%2F09%2F25%2FKNN-handwriting-recognization%2F</url>
    <content type="text"><![CDATA[by lucainiaoge Intro（这其实是我第一次真正意义上的手写数字识别，使用KNN方法） 之前在某宝上买来了一个小姐姐给的吴恩达作业（小姐姐讲解声音真的好听！），但是学完了（包括手写数字识别的手打BP神经网络实现），发现自己弄还是不会！现在，上了模式识别课程，一切重新开始！ 学校开的模式识别选修课使我重新捡起python了。模式识别其实挺有意思的，就是需要有些代码工程开发经验才可以更好地学习。这篇博客记载了我完成这门课的第一个作业的过程。 What is KNNKNN是什么？其实很简单： 看这里 kNN算法:K最近邻(kNN,k-NearestNeighbor)分类算法 - JYRoy - 博客园 机器学习(二):k近邻法(kNN) - E博客 - CSDN博客 没必要看完，知道概念就行。 Purpose就是用手写数字图片训练你的机器，让你的机器可以识别其他的手写数字。 课程群里有人问有没有baseline，无知的我竟然不知道这个词是啥意思！果断百度！Benchmark和Baseline是什么：CSDN-【简单易懂】Benchmark和baseline的区别！ 助教回答：木有baseline。开心！ How to Read Dataset Using Python拿到作业的时候，我的电脑Python是炸的，数据集怎么读不知道，甚至连能不能做出来都不知道…… 课上头铁看书不听课，结果书的前面讲的贝叶斯分类，老师却直接讲KNN，并且留了KNN实现手写数字识别的作业，整的我挺慌。 那么，就这样开始了~ 老舍友lmh大神花了一天时间，搞出来了并且把代码挂到了github上（看这里www.github.com/bugstop/handwritten-digit-recognition-knn），附带转发知乎回答：室友想抄我的代码，给还是不给…… 首先，拿到大佬的开源代码，又是logging又是threading的（python里面我不明白的操作），我也是一脸懵逼的。不管了，立个小目标：读取数据集。 Information about the dataset format 怎么读这来自远古的文件？不好好看消息记录的我竟然不知道助教大大发了数据集官网！ 来自几位大佬的手写数字数据集官网：http://yann.lecun.com/exdb/mnist/ 没接触过的小伙伴，请认真阅读官网！你想知道的有关这个数据集的信息都在上面！ 不要用txt打开这样的文件，否则你的下场是这样： Deal with two Python versions解决python无法使用的问题，自闭了半天后发现原因是我的电脑有两个版本的python：2.7和3.7 这可咋办？我的sublime text编译不了啊！在网上搜索相关博文，这篇博客讲得不错：Windows下多版本Python的pip安装与使用的吐血总结 Python coding to read dataset开始着手写代码了。阅读lmh大佬的代码后，少走了不少弯路！再次贡上大佬的代码：www.github.com – (站内搜索)bugstop/handwriting-number-recognization-knn 给大佬点了个小星星！ 基本的文件读取直接拿来大佬的代码： 12345678910111213training_image_file = './source/train-images.idx3-ubyte'training_label_file = './source/train-labels.idx1-ubyte'test_image_file = './source/t10k-images.idx3-ubyte'test_label_file = './source/t10k-labels.idx1-ubyte'with open(training_image_file, 'rb') as f: training_images = f.read()[16:]with open(training_label_file, 'rb') as f: training_labels = f.read()[8:]with open(test_image_file, 'rb') as f: test_images = f.read()[16:]with open(test_label_file, 'rb') as f: test_labels = f.read()[8:]print(test_labels) Python读取文件还是蛮方便的。在前面加上这段代码就更好了！（lmh不知从哪学来的神操作） 123456789101112131415#Lmh dalao's basic operationclass Log(object): def __init__(self): logging.basicConfig(level=logging.INFO, filemode='w', filename='knn.log', format='%(message)s') console = logging.StreamHandler() console.setLevel(logging.INFO) formatter = logging.Formatter('%(message)s') console.setFormatter(formatter) logging.getLogger('').addHandler(console) @staticmethod def info(msg): logging.info(f'&#123;datetime.now().strftime("%H:%M:%S.%f")&#125;： &#123;msg&#125;')log = Log()log.info('Read Files') 为方便后续使用，定义全局变量，并了解一下数据集的结构： 12345global num_of_training_img, num_of_test_img, dim_of_imgnum_of_training_img = 60000num_of_test_img = 10000dim_of_img = 28print('Type of images: ',type(training_images)) 看傻了的小伙伴，请认真阅读官网！你想知道的有关这个数据集的信息都在上面！ print显示读取得到的数据集，发现长成这样： b’\x07\x02\x01\x00\ … 类型是Byte，这个类型是啥？其实，就是16进制拼成的数组，两个16进制数字拼成一个byte作为一个成员，顺序访问即可。所以，后续这么读（示例）： 1234567891011def read_training_img(): current_pos=0 imgs=[] for i in range(num_of_training_img): current_pos=i*dim_of_img*dim_of_img current_img=[None]*(dim_of_img*dim_of_img) for index in range(dim_of_img*dim_of_img): #不能通过切片逐行读取，否则无法存储每个像素点 current_img[index]=training_images[current_pos+index] imgs.append(current_img) return imgs 读取训练标签集和测试集大家可以作为练手代码。调用函数如下： 1234567891011log.info('Read Files: training imgs')training_imgs_vec=read_training_img()training_imgs_vec=np.array(training_imgs_vec)#不要用乱用np.matrix()方法training_imgs_mat=np.empty((num_of_training_img,dim_of_img,dim_of_img))#一定要遵循(address,floor,room)原则log.info('Reshaping: training imgs')training_imgs_mat=training_imgs_vec.reshape(num_of_training_img,dim_of_img,dim_of_img)print('shape of training image vectors:',training_imgs_vec.shape)print('shape of training image matrices:',training_imgs_mat.shape)log.info('Read Files: training labels')training_labels_vec=read_training_label()print('training labels:',training_labels_vec[0:4]) 得到这样的结果： shape of training image vectors: (60000, 784)shape of training image matrices: (60000, 28, 28)18:57:31/281304: Read Files: training labelstraining labels: [5, 0, 4, 1] 具体把图片保存成图片还是向量，自己看着办咯。为了确认读出来了，我还画了个图： 注意：plt.show()之后，程序不会继续运行，需要把图片窗口关掉，才可以继续. 接下来，就正式开始KNN了！ KNN algorthim using Python1234567891011121314####正式开始KNN算法 ####算法思路：#将test图片和训练集图片比对计算距离#距离计算方法：#选择1，逐点计算欧式距离，利用compare_ssim(img1, img2)#选择2，计算余弦距离#可选处理，将图片下采样#判断方法：#选择1，直接选前k个，按照多的取胜#可选处理1，随机选取一定数量训练集图片#可选处理2，统计被比较的训练集每类数量作为权重，最后真正的数量需要除以每类的数量 其实很简单。直接贴出函数： 1234567891011121314151617181920212223242526272829303132def identify_img(img_check,imgs_refer,labels_refer_list,k_rank):#img_check为矩阵(dim,dim),#imgs_refer为与img_check同维度的矩阵组(length,dim,dim)#labels_refer_list为imgs_refer的标签(length,-)#k_rank为KNN的前k名门限值 train_length=len(labels_refer_list) #train_length=labels_refer.size #labels_refer_list=labels_refer.tolist() if(k_rank&gt;train_length): k_rank=train_length//2 cnt_per_id=&#123;&#125; for i in set(labels_refer_list): cnt_per_id[i] = labels_refer_list.count(i) #计算距离 dist=[None]*train_length for index in range(train_length): dist[index]=compare_ssim(img_check,imgs_refer[index],multichannel=False) #距离排序获取索引 rank=np.argsort(-np.array(dist)) #按照索引访问标签 top_labels_list=[None]*k_rank for index in range(k_rank): top_labels_list[index]=labels_refer_list[rank[index]] #获取前k_rank名的标签数量并计算最终得分 top_cnt_per_id=&#123;&#125; score=[None]*10 for i in set(labels_refer_list): top_cnt_per_id[i] = top_labels_list.count(i) score[i]=top_cnt_per_id[i]#/cnt_per_id[i] #获取第一名 first_place=np.where(score==np.max(score))[0][0] return first_place 对test样本反复调用这个函数，就ok了。期间，计算两个图片之间的“距离”，我没有用基本的欧氏距离，而是用了专门计算图片相似度的函数，SSIM，后来排序的时候降序排列。 继续地，我是用了欧几里得距离进行计算。求距离就不用开方了！保持单调性就行！ 根据欧几里得距离排序的方法这里就不贴出来了。 接下来，调用函数！ 12345678910111213141516171819202122starttime = datetime.now()#euclidian method with randomnum_of_test=1000result=[None]*num_of_test_imgcheck_vec=[None]*num_of_test_imgnum_of_refer=5000candidate_label=[None]*num_of_refercandidate_img=np.empty((num_of_training_img,dim_of_img*dim_of_img))for index in range(num_of_test): sample_list = random.sample(range(0,num_of_training_img),num_of_refer) for cad in range(num_of_refer): candidate_label[cad]=training_labels_vec[sample_list[cad]] candidate_img[cad]=training_imgs_vec[sample_list[cad]] log.info('Identifying: test '+str(index)) result[index]=identify_img_euclidian(test_imgs_vec[index],candidate_img,candidate_label,3) check_vec[index]=(test_labels_vec[index]==result[index]) #print('test_label='+str(test_labels_vec[index]),'result='+str(result[index]))endtime = datetime.now()num_of_correct=check_vec.count(True)num_of_incorrect=check_vec.count(False)print('time elapsed: ',endtime-starttime)log.info(f'Accuracy: &#123;num_of_correct / (num_of_correct+num_of_incorrect) * 100&#125;%') 细心的同学会发现，我引入了随机！没错，在挑选样本的时候，我没把全训练集都用于KNN比较。既然不需要训练，那索性就随机取训练集样本。当然，不随机版本的更好些，拿来练练手。 作者：CandyBullet链接：https://www.imooc.com/article/23400来源：慕课网路菜鸟KNN手写数字（结果汇总）训练集样本个数|是否随机选取样本|选取排名|测试样本数|计算相似度方法|是否除以了各类数|准确率|用时(精确到秒)-|-|-|-|-|-|-|-2000|FALSE|200|100|compare_ssim|TRUE|79%|-10000|FALSE|200|100|compare_ssim|TRUE|92%|5:351000|FALSE|200|100|compare_ssim|TRUE|72%|0:371000|FALSE|50|100|compare_ssim|TRUE|78%|0:341000|FALSE|10|100|compare_ssim|TRUE|86%|0:325000|FALSE|10|100|compare_ssim|TRUE|93%|2:565000|FALSE|10|100|compare_ssim|FALSE|94%|-5000|FALSE|3|100|compare_ssim|FALSE|93%|2:4030000|FALSE|3|100|compare_ssim|FALSE|99%|16:1830000|FALSE|1|100|compare_ssim|FALSE|98%|16:531000|FALSE|10|100|euclidian|FALSE|84%|0:011000|FALSE|10|100|euclidian|TRUE|85%|0:001000|FALSE|10|10000|euclidian|TRUE|84.76%|1:5260000|FALSE|10|100|euclidian|TRUE|98%|0:545000|FALSE|10|100|euclidian|TRUE|94%|0:045000|FALSE|3|100|euclidian|TRUE|94%|0:045000|FALSE|3|100|euclidian|FALSE|94%|0:0430000|FALSE|3|100|euclidian|FALSE|97%|0:2530000|TRUE|3|100|euclidian|FALSE|99%|0:445000|TRUE|3|100|euclidian|FALSE|93%|0:075000|TRUE|3|100|euclidian|TRUE|94%|0:0730000|TRUE|3|1000|euclidian|TRUE|95%|7:355000|TRUE|3|1000|euclidian|TRUE|91.10%|1:16 Several attentions on python grammar•python踩到的几个坑： 不能用list访问list，但可以用list访问numpy的array numpy的array没有count()方法，不过可以通过tolist()方法转化成list再进行操作 不能对list取负号，但可以对numpy的array取负号 想初始化list就用[None]*数组大小，想初始化numpy的array就用np.empty(维度1，维度2，维度3……) 想转numpy的array就用np.array()函数，但不要用np.matrix()，后者兼容性不好 在用matplotlib画subplot的时候，不要直接对分图使用方法title()，要用set_title()，否则报错]]></content>
      <tags>
        <tag>machine_learning</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开始搭建自己博客(How to start my blog)]]></title>
    <url>%2F2019%2F09%2F23%2Fhow_to_build_a_blog%2F</url>
    <content type="text"><![CDATA[by lucainiaoge Intro这篇文章其实是一个引导性的文章，汇总网友们的博客链接。博客的搭建对于小白来说还是要了解那么一阵子的。我同样也是，开学后半个月才摸出了点门道。我就先把我走过的路再给大家呈现一遍，然后介绍一点自己的经验。 How to Post Your Website怎样发布你的网页，让全世界人都看到？ 你需要一个服务器。服务器存储你的资源，并将它们共享给全世界。这也是互联网的的运作方式。那么怎么申请一个服务器？ 参考百度经验：申请阿里云服务器 我是怎么干的？我没有自己搞服务器，而是借用了Github的服务器，因为Github有专门的推出的gitpage。创建github账户之后，就可以申请挂一个网址，自定义空间还是挺大的。这一步步怎么走 参考韩朔的简书：Gitpage教程 参考Gitpage官网介绍和教程 这位大佬做出来以后就是这个样子：韩朔的Gitpage(里面也有教程哦) 也挺建议看这个，这个博客直接把后面讲的一块讲了：使用Hexo和Github Pages搭建属于自己的博客 - Hello World篇 Fill in Your Website当你按部就班地去做了以后，就会得到一个空网页（一般要申请仓库并部署之后再等上一会儿，我是中午吃了顿饭，回来搞了搞就好了） Code it yourself有一个办法(如果想速出结果，并不推荐，毕竟我现在也不大会)：学习html语言或者javascript(js)，学着自己写网页，自力动手丰衣足食。这里供上链接： 参考js友好的教程(W3school) 参考js参考手册(W3school) 参考js学习指南(简书) Hexo如果不想自己打代码去设计网页，就要借助强大外力了！ 这里，大家很多使用一个叫Hexo的程序帮助写博客。Hexo是什么？是一个不需要网页编程就可以实现博客搭建的框架：只需要在命令行中给它操作，它就可以按你的需要创建文件。你需要按照它给定的格式进行修改和配置，比如修改主题。 hexo主题是什么？顾名思义，是你的网页的色调啊、风格啊什么的。但是为什么要把它单独拿出来考虑？因为不同主题栏目设置啊、文档中配置啊没有统一的规定（虽然大同小异），所以，选定一个主题就尽量不要更换，要不之前的配置很可能作废。 话不多说，贡上连接： 前面提到的教程：使用Hexo和Github Pages搭建属于自己的博客 - Hello World篇 如果你选择了配置服务器：云服务器+域名方式搭建hexo博客 hexo的使用入门教程(含有安装步骤)：博客园:GitHub+Hexo搭建个人网站详细教程 先看这些吧，够用了（出现了奇奇特特的bug除外）。如果发的链接太多，反而会乱了方寸。 Hexo Tips如何修改Hexo的内容改动_config文件，一般是操作Hexo的方式。操作Hexo还有很多其它方式，牛逼的大佬会直接写js脚本（我现在不会哦），Hexo内部也有很多脚本可以修改以添加各种骚功能。这些东西我还没捋清楚，不过不用担心，每次你想实现一个功能就上网搜一下“Hexo 做XXX”，基本都会有答案的。 其实很多东西是在Hexo下面特定的主题里面修改的，这些请关注下一部分。 对付重装哦对，你会发现，按照教程做以后，你的网站源其实是在自己的电脑上，甚至在C盘，这是很危险的。我的想法是，时长做备份。但毕竟太麻烦。电脑不小心重装怎么办？ 参考简书：系统重装后如何找回Github+Hexo博客 hexo d -g命令失效还有，有时你会发现按照hexo d -g命令不行，可能是因为这个原因： 简书: Hexo d -g踩坑指北 简书: 关于部署HEXO博客到github时，提示typeError 是否: hexo d -g 部署没有成功怎么办 部署的Hexo去哪里了还有一个问题，就是Github部署Hexo以后，应该会覆盖掉你以前在Github仓库文件夹里的文件，这一点注意一下。当然，前提是你按照之前的教程改了_config文件而且完成了配套的配置，每次部署都会在github仓库文件夹生成一个发布版的博客。 每次开机如何打开Hexo用命令行(cmd)将你的路径定位到博客根目录下，然后操作hexo命令就好了！ 忽然无法用cmd执行hexo的命令错误提示如下：C:\Users\Administrator\blog&gt;hexo ssocket: (10106) 无法加载或初始化请求的服务提供程序。解决方案：管理员运行cmd，执行netsh winsock reset，然后重启电脑 其他问题其他问题待添加。写这篇总结的时候，我已经离开始摸索半个多月了，之前遇到的很多小bug基本忘光了，遗憾当初没有记下来，这里给大家道个歉，没法一一总结了。 Design Your Blog好了，到了这里你就可以开始着手写自己的博客了。 我觉得写博客包括： 设计主页：设计头像，设计背景，设计标签分类系统，设计友链和转发…… 功能添加：增加评论区，增加有趣的gif，增加“阅读更多”功能，增加目录，增加多媒体功能，增加文件分享和下载功能…… 写文章：就是写文章咯 Hexo Theme: yilia设置Hexo主题，可是牌面问题！ 主题的添加和配置，在上一部分Hexo的搭建中，已经有大神提到过了。这里主要是讲主题怎么使用。我了解的是yilia主题，所以喜欢其他主题的朋友只好找其他教程咯！ 还是放一个安装教程吧：来自Litten的yilia主题安装教程和初步使用 Litten的博客里有很多有趣的教程，这里直接贴上Litten的博客 我记得安装主题以后，会将主题存放在某个奇怪的地方而不是你的blog目录里，我的起初存在了这里： C:\Users\Administrator\Desktop\themes 反正，把这玩意直接复制到blog目录下的themes文件夹里，就ok了。 完了以后，你就可以操作themes\yilia文件夹里的_config.yml 哦对，如果幸运的话，里面还会有个readme.md，里面有一个yilia主题配置的入门教程，挺详细的。 如果不幸运的话，可以看这个： 博客园: github+hexo搭建自己的博客网站（二）更换主题yilia .md是什么？是markdown，一种方便的文本编辑格式。具体看这个了解一下就好了： CSDN: Markdown语法图文全面详解(10分钟学会) 在这里讲一讲yilia主题下配置博客md源文件的事项：有些语法的解析和标准的markdown不太一致，尤其是如果你用MarkdownPad来编辑时候，右面的预览和实际网页根本就不像的。细节很多，比如一般的markdown一个#就可以表示一个标题，但在这个主题下解析需要两个#包住你要的文字。很多细节这里不赘述了，毕竟我也刚开始嘛！ Write Your Blog很好，要开始写文章了！ 打开根目录下source\_post文件夹，里面放.md文件，就可以解析了。你只需要：尽情发挥你的md本领和你的文笔，写完以后hexo d -g一下，打开你的gitpage，就可以看到了。你还可以将你的gitpage分享给其他人，这样你的知识就得到了共享！ 当然，hexo s可以用来在hexo d部署之前预览你的博客！ Various Functions其实嘛，做一个博客还是蛮不容易的，有很多小功能要考虑，每样小功能甚至可能花很长时间来配置。这里贡上一些我觉得挺有必要的功能以及它们的教程。 添加“阅读更多”功能不上链接了，直接说：你的md文件里，在你想截断的地方另起一行，填上这句话就ok了： &lt;!–more–&gt; 如何在博客中添加图片 (别人家的博客): 福球大王的博客：hexo上传图片 一个上传图片并自动生成md代码的网址：https://sm.ms/ 更详细的图片设置: 知乎专栏 https://zhuanlan.zhihu.com/p/61226148 为自己的博客设置头像 CSDN: yilia头像/图标设置 github: 为什么博客里面头像不显示了 Gitalk: 让你的博客拥有评论区 CSDN: Hexo的yilia主题增加Gitalk插件 CSDN: 另一个Hexo的yilia主题增加Gitalk插件的教程 让自己的网页被搜索引擎搜到 CSDN: 想让你的博客被更多的人在搜索引擎中搜到吗？ 为文章添加目录 CSDN: Hexo博客yilia主题文章添加目录 简书: 另一个Hexo博客yilia主题文章添加目录的教程 为文章添加标签不上链接了，直接说：在你的md文件开始，添加这样的格式就ok了： --- toc: true title: XXXXXXX date: 2019.9.23 tags: [XXX1, XXX2] --- 网友的一些个性化设置总结 CSDN: Yilia个性设置 放个大招：一些hexo+yilia的集锦 简书: Hexo + yilia 搭建博客可能会遇到的所有疑问 上面链接包括:添加文章目录，随笔，标签，展开全文按钮，增加阅读量，“畅言”评论（不推荐），放音乐，放视频，放相册，写文章 SF: Hexo+yilia主题网站进阶教程 上面链接包括:展开全文按钮，添加赞赏和二维码，添加字数统计，统计网站访问量，添加版权说明，增加gitalk评论区，添加RSS，提交百度搜索引擎 简书: Hexo 基于yilia主题及其它插件优化 上面链接包括:yilia默认设置，修改代码块样式，添加不蒜子统计，单篇文章点击量查看，网站统计备案信息，版权声明，博文置顶，gitalk和valine评论系统，在线聊天，SEO让URL持久化，私密文件加密 先说这么多吧！待添加！]]></content>
      <tags>
        <tag>blog_start</tag>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019数学建模国赛总结 Part 1 (2019 CUMCM Summary)]]></title>
    <url>%2F2019%2F09%2F15%2Fmath-modling-2019CUMCM-summary-1%2F</url>
    <content type="text"><![CDATA[by lucainiaoge Intro总的来说，算是顺利。提交完文档恋恋不舍，仔仔细细又看了两遍，感觉漏洞百出，算了吧，不再提交MD码了，省得出车祸。具体过程呢，记不太清了，简要回忆一下吧。 Just before the start第一天12号，下午5:30上完电磁场与电磁波，和翔哥赶紧跑回2301B打开电脑焦急地等待。翔哥表示手在颤抖。能干啥呢？在闷热的活动室准备好了插排、台灯，烧了水，上了厕所，甚至还和hhn理论理论开空调的事情，不过被hhn拒绝了，他表示自己感冒了并且需要通风。看在他经常搞卫生的份上，我就预期接下来几天要坐桑拿房了。 6:00，子烨也来了，看题。我们分工看题，我先看的A题，给翔哥看B题，子烨看C题。不巧，子烨得紧急去开学生会，那就让他去吧。我就在钻研A，钻研半天总觉得不对劲，在bilibili和google还有油管上找了关于2019A题油管的视频（没错，我在油管找油管），但是我在油管死活找不到我要的油管：我要知道油管里是否一直有油，油管压强该怎么算，我要学流体力学，我要……妈呀想着都害怕。不过我本着不想出来这题咋回事不放弃做这道题的想法，继续逼着自己想啊想。油管里是否有空气？题目的流程到底是啥（这个想明白了）？数据咋获取？代码将会成什么样子？……我先换换思路吧，呼叫翔哥，问问他B题咋想。 扫了一眼B，发现，我天这也太小儿科了吧，弄个游戏整过来让我们建模，有意思么？？我于是老是在把这个想法表示出来，还好这只是牢骚而已，当真的话我可能就不会在现在兴奋地打字了。翔哥没有对这题表达太多的看法，难就是了，似乎要确定很多量；我当然，满脑子都在说“这题太无聊了”。恰好，子烨回来了。 我问他学生会其他的去搞建模的咋想？他说，他认识的几个都还啥都没想呢。再问：C题有想法么？他说找到数据了。仔细看C题，我天，一个数字都没有，就让我们建模。想到这里，我脑海里不禁想起通信导论的课设。我还挣扎一下，呼叫他俩看看A题，我啥都没讲出来。好吧，我们讨论了一番，决定B题了。具体怎么做出的集体决定，我也忘了，总之，觉得B题相对“好欺负”就是了。 first night - 2019.9.12好的，下面是干活阶段。拿到这玩意咋整？先不想乱七八糟的，第一问做不出来，其他的咋办？我们怎么分工？我第一天晚上会不会有思路？我还能不能睡着觉？我现在要不要上厕所？我还剩多长时间就比完赛了？我作业写了么？…… 总之嘛，要想的问题还是很多的（呵呵） 我的大脑告诉我，首先要决定的问题就是怎么分工。传说中的 “数学-代码-写作文”的分工只是说说而已，拿到这玩意谁会套用这个“数码写”（原谅我这么简称它）的模型？我摸了一遍四个问题，发现第一问是一维的，第二问是填空题，第三问是三维的，第四问是第三问。所以决定先想第一问好了，让子烨先去干问题重述、外加查资料，反正也分析过一遍题了。翔哥呢？没让他去打代码，刚开始我记得是让他去研究一维碰撞了。分工就大致明了了：我搞代码（这也是被他俩认可的，之前两周备赛，重要代码基本都是我弄出来的），翔哥研究物理问题，子烨查资料+写作文（我还教了他几招用word打公式的骚方法，输入速度秒杀LateX） 我隐约记得，第一天晚上蹦出了很多想法，比如考虑空气阻力，然后查找空气阻力的公式，子烨找了很多论文，什么斯托克斯公式啥的，都是作为参考了的，然后我们还估算各种值，比如浮力和重力的比例啊、碰撞系数啊啥的，完全是在做一个物理题嘛。我甚至回宿舍去找大物书，发现只剩下下册了，上册捐给学弟（学妹？不存在的）了，翔哥也去我宿舍拿了我的线代书，我拿了我的大物笔记，还好笔记记得全。 我想，第一问还有点难，不过大概要输出什么东西是明白了：游戏策略嘛，不过控制力道和方向，画个F-t图和φ-t图不就得了，当然还可以是电影；这图咋来？MATLAB来。MATLAB咋来？一定要离散化。离散化咋来？转微分表达式为差分表达式。Bingo！开始干活！脑袋一热，就写了一个小球在地上颠来颠去的程序（此处感谢HYX大一时候发来的VB碰撞仿真程序带来的灵感！）还真别说，给颠起来了，很兴奋了。于是我开始找怎么做电影，还真别说，给做出来了，但是MATLAB做电影是真的真的慢慢慢慢！算了，以后再说吧，这事儿算是告一段落。 与此同时，翔哥的一维碰撞也给似乎搞出来了。我们纠结了半天碰撞该怎么刻画，我坚持让翔哥找出恢复系数e，这样我们就不需要讨论能量守恒了；还没有开始写程序的翔哥表示，一个式子已经够便宜了，这个e哪找得到？哪都找不到！炒了半天，子烨表示，自己查到了足球和地面碰撞的e=0.68，大部分e值是试验测定的。好吧，不吵了，我在碰撞中将e设为0.8得了，翔哥不必在考虑这个问题了，今后对e进行灵敏度分析就好了。于是，我解锁新技能：挂起任务！这一点真的很关键，切中了马克思主义中的“把握主要矛盾”的哲学观念。 于是，我开始着手写第一问的代码，具体那四个状态我是怎么构建出来的，我记不太清了，因为那时我在集中思考，别人在干啥我都没在意，当然，我还记得在此期间hhn热心肠拿来了一个电风扇供我消遣（感动！）还真别说，我给写出来了（辣鸡MATLAB可真好用）。就看那小球越颠越高，要不就一颠就掉地上，总之，不符合常理。我就想，我不让你（指的小球）飞不就得了，你飞高了，我下次就给少一点劲儿，你飞低了，我下一次就给大一点劲儿，完美！反馈！！！负反馈！！！着手写负反馈函数，施力大小和高度成负相关。在夜里12点之前，我忘了翔哥在自闭啥，也忘了子烨写了哪部分论文，总之我很兴奋地宣布：第一问我快做出来了！到了这里，本来自闭的翔哥想开了，本来面瘫的子烨动容了，本来鸭梨山大的路菜鸟鸭梨小了。 于是，第一个物理引擎诞生了，它长在一维世界里。最后合力的分解才到二维。毕竟第一问是十分理想的。 睡觉吧。子烨先去睡了，我挑头表示我也去睡了，翔哥就也睡了。后来，隔壁Ldt表示，我们搞数学建模睡得竟然比他还早！谁叫他老喝快乐水吃炸鸡（而且肌肉发达不长胖还喜欢熬夜）呢！ 2019数学建模系列传送门 2019数学建模国赛总结 Part 1 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 2 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 3 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 4 (2019 CUMCM Summary) 2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></content>
      <tags>
        <tag>math_moding</tag>
        <tag>experience</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019数学建模国赛总结 Part 2 (2019 CUMCM Summary)]]></title>
    <url>%2F2019%2F09%2F15%2Fmath-modling-2019CUMCM-summary-2%2F</url>
    <content type="text"><![CDATA[by lucainiaoge First Battle - 2019.9.13第二天，我帅先到达战场（毕竟我就睡在隔壁战壕里）。自己又调了一会小模型，画了几个图给了子烨。就继续了。第一天晚上，我宣布完阶段性胜利后，便给翔哥分配任务了：“赶紧研究第二问，它和第一问没啥重合的，但是关系到后面”。翔哥于是就又开始自闭了。 我呢？继续搞第一问，弄了个优化程序。美其名曰：目标规划；其实呢：暴力搜索。好，选了几个比较要命的参数，弄了个三重搜索，就算“圆满完成第一问”了（后面持续真香）。 子烨终于有事干了：抄公式，画流程图，写目标规划表达式，定义符号，接收我画的图。我觉得他有很多事干，他却总是用些“ssbb（我内心os）”的问题烦我，真的烦（半开玩笑的严肃状）！一跟我说“我就觉得你的模型不符合题意”，我问他怎么不符合，他却说不出来。我讲了n遍我的想法，试图去说服（毕竟此时分心真的不值真的不值），我于是忽然语速惊人，话音变大，变得强势（解锁新技能：开BKB!），试图把他怼回去。不过子烨一副不肯善罢甘休的样子。好吧，我不管你，你先写文章吧。于是我对他说“你还是赶紧写论文吧，这是最要紧的！而且你说的这些，都是可以挂起的事情，我现在做的东西时最要命的！这个逻辑是为什么我也给你说了很多遍了，而且你也没能有效地反驳这些逻辑。你的想法可以保留，写到你的挂起任务纸上！”于是我的技能升级了：任务挂起到纸上！ 还是稍微回顾一下吧：子烨强烈要求我做的，包括：不用F-t图和φ-t图表示答案，给出切实有效的指导意见，你的连续施力不符合题意，至少也应该是忽然发力的那种。我就不明白了，我已经建好的模型这些东西（除了不画这个示意图），都可以包括这些。子烨表示，“我知道你的意思，可是我是这个意思……”我嘛，始终没领会这个意思。 此时此刻，就是考验魄力的时刻了。我真的就严肃起来了，我表示自己很伤心，给你讲了那么多你都不听，你也没听懂我的，我从你的复述上可以看出来。我现在不想被屁事分心，我要赶紧去做掉第二问（此时翔哥已经自闭几近崩溃，因为他之前推的一堆式子被我一票否决（我靠初始条件就错了），他写的程序也被我一票否决（这程序思路快到了，但是效果，emmmmmm））。这时候，我表示鸭梨又开始山大了，如果第二问搞不出来咋办？我决定：自己动手开始写第二问！自己动手开始侵略翔哥的任务！子烨此时也大概默许了我，开始加紧写论文了（我一直没有监督他，我觉得这没必要，只是不时地灌一罐心灵鸡汤（一股玩笑味儿）来激励一下（美其名曰：“特殊的时期就要采取特殊的办法”））！ 当我和翔哥多次交流以后，我逼着自己去进入他的推导（这可真难，我必须得时刻紧跟步伐，并通过表达我对这一步的理解以保证我没有跟丢！这很关键！解锁新技能：跟上别人思路！）于是，我打破了他的三维转一维建模的模式，直接上三维坐标系！将一切向量刻画为三维矢量，将一切空间上的运算转化为矢量运算！ 此时，干了那么久却没个结果，还被我否决了的翔哥是内心mmp的。但又能怎么样呢？为了前进，就要丢弃一些东西。为了合作，就要牺牲一些东西。为了承认新的事物，就很可能要摒弃旧的事物，不是吗？于是，想办法平覆翔哥的躁动以后，我们合力推公式，我把一些公式推导中的苦力活给了他（没想到，百感交集的翔哥进入了人生最差状态，推公式老是犯迷糊）。总之，我们构建出了一个用力拉鼓（圆柱体的侧面），使他转动，并且可以合成所有的力矩得到鼓的最终角速度，并且可以放到我原来的那个模型的世界里跑的思路。（其实，我在第一天晚上搞出来的时候，就义正辞严地告诫翔哥：你明天推出来的公式，要尽量适用于我的模型，大概是这一点让他更加害怕了吧）。顺着这个思路，翔哥大肆翻修了原来的程序，又写了个新的（魄力了得！）（我想起了当初在RM的谢老板，魄力更加了得，甚至会为了修复一个bug重写一辆车的代码）。不过，他写的程序总是跑飞，要不是不能走，要不就一转转个70多度，用翔哥的话说，“我人都晕了！” 我让他发来他的程序。我使用了顺着别人思路走的技能，找出了很多bug，那种成就感，几乎等同于翔哥的绝望感。不过，技能升级：读懂别人代码！ bug被一一扫清，具体是啥bug我也忘了，说出来也是牢骚了。此时也大概晚上了。我的大脑介于：完善第二问、突破第三问、睡觉、洗澡、打蚊子、背单词这些东西之间，啥都想不明白，最后我选择了睡觉。 P.S. 这天下午我忽然想，写了这么多东西，哪能白写？我要让它名垂（我的资源管理器）青史，于是便解锁新技能：求救于rar！，打包文件如下：（很多是后面的）（这些东西帮我了大忙！感谢自己！） 对了，说一说饭是怎么解决的吧：中午，我给翔哥带饭，子烨表示想出去散散心，自己去吃了；晚饭，我照例不吃，翔哥表示自己想要一个饼。hyx（托hhn？？）带来好吃的蛋糕放到了桌上，我象征性地问了问翔哥和子烨要不要吃，实际内心已经想直接吞掉它了（事实上嘛。。。是这样的！） 夜晚游戏难度提升：WXh会来捣乱，made。。。。。。劳资烦着呢！（wxh满脸堆笑，像是有什么非分之想（hhhhhhhh））于是，百感交集的我对wxh说道“回去打你的csgo去！”（我其实还计划着比完赛打两盘dota嘿嘿） 这一天，睡得晚了一个小时，不过夜晚已经基本找到思路了，并且有了一点起色（第二问跑出来结果终于能看了，但好像还不太正常的亚子），也就是凌晨1：00上床的。上床前，猛然发现单词还没背；鸭梨山大的我愤怒地把单词上限从200减到70.哈哈哈，我一秒钟背完130个词！ 2019数学建模系列传送门 2019数学建模国赛总结 Part 1 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 2 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 3 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 4 (2019 CUMCM Summary) 2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></content>
      <tags>
        <tag>math_moding</tag>
        <tag>experience</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019数学建模国赛总结 Part 3 (2019 CUMCM Summary)]]></title>
    <url>%2F2019%2F09%2F15%2Fmath-modling-2019CUMCM-summary-3%2F</url>
    <content type="text"><![CDATA[by lucainiaoge Great progress - 2019.9.14第三天一早（其实严格来说是第二天），我double帅先到达战场 开始干第二问，抱着一腔“宜将剩勇追穷寇，不可沽名学霸王”的热忱。 翔哥也来了，带着一把鼻涕和一脸阴沉，看出来感冒了。昨晚上弄出了成果，我就让他想办法验证。于是一早，他又来自闭推公式了。我首先问他，你这个碰撞公式是否是对的？为什么某个速度向量表达式不含碰撞系数e？他表示不理解加不爽，看得出他及其暴躁，语气及其敷衍，用诸如“怎么了？不行么？我没错啊？你说是就是吧？”之类的话来怼。我表示不理解，并明确表达了出来，换来一句“你说是就是吧”。此时我也很不爽，甚至想发火。要知道，我在大学还是没发过几次火的。（顶多称得上愠怒，不能说发火，我真没情绪失控过，就算之前被坑、被隐瞒真相、觉得十分不公、觉得自己走错了人生道路……都没发过火，这一点不能说引以为啥，因为这一点或许是懦弱的一面。我一直觉得自己是很懦弱的。） 一旁子烨似乎不受影响。不知他内心在怎么想。反正，我只顾着和翔哥理论呢。因为，我压住了火气，从源头推了一遍碰撞的结论，找出问题所在了。 我向来是对事不对人的 (其实还是对人的)，你说是不是？你说不是就不是吧，反正我觉得是就是了。 接下来，就是见证奇迹的时刻：一次第二问的胜利，让翔哥重获新生，似乎是凤凰涅槃、超新星爆发一般，从废墟中焕发出了连感冒都夺不走的活力（哈哈哈哈哈）。 没错，我又出来了。这一次是真的出来了，以至于我高兴地一上午都在研究怎么画图把这难得的结果欣赏出来。（当然，顺手把平动的模型也弄出来了） 这时，我也可以开开心心地给翔哥解释他的错误所在了。并且表达了我对推公式（数学推证、数理逻辑思考）这件事情的看法：不时地定义新事物，可能会使复杂的事情变得简单；不时地想自己的问题能否抽象为已知的事物（例如，一大坨数其实就是一个常量），可能会使复杂的问题变得简单；主动地去尝试各个可能性，才是数学中发现新宇宙的精髓（这一点我没跟翔哥说，嘘。。。。。毕竟他已经自闭了）。我甚至还骄傲地以自己为例子：我，考试的时候大部分公式是自己推出来的（所以我很讨厌那种记公式记文字的考试，纯粹只是为了检验你有没有为了考试而学习） 顺便提一句，这天上午子烨依然不肯善罢甘休，甚至更加积极了：他总认为我的模型方向有误。我又开始演讲了。不过很明显，我们互相听不进去。我于是也开始急躁了：心理就在暗暗地跺脚，我靠这不就纯粹地浪费时间么？我还是压住火气，拿出我总结的系统雏形（没错，我一直在酝酿这个雏形，尽管它现在还没有实体化，但我只能尽力去说服了。还是那句话，特殊时刻，只能用特殊的办法。） 不知是不是我突然“主动地”变得耐心的缘故，让他和我一起头脑风暴，达成几点共识：这道题想要表达的，就是那三个基础量（合力大小，方向，随时间变化），并用F-t图这样程序；总之子烨似乎进入了我的世界明白了我的想法，或许是完全是服从于团队而不得不妥协，总之，随着几声“奥”，他也便耐心去打论文了。真的，压力和时间的流逝使我暴躁，但我的习惯又不想让我暴躁，这大概也是反馈吧。 来不及再高兴了，看看剩多大点儿时间？只有一天半了！赶紧继续分工，把公式扔给子烨（我的天他还是老看不懂公式，老不理解系统，这可咋整？我说：你还不理解是吧？你再不理解我就……再给你讲一遍，你不理解10遍我给你讲10遍直到你理解），让翔哥：继续想验证第二问的事情，想合力分解成分力的事情，并对后者给予了厚望和大权值。 为什么想这个事情？因为我在做第二问的时候，心里就在想给第三问打铺垫了：既然考虑了三维，转动，那一定就是基于这个重新设计系统，使它适应三维世界。怎么适应三维世界？有一维已经ok了，就是竖直维度；其余两个维度，不过就是xOy平面。在这个平面上，我们要接到球，并且合适地反弹它使得它竖直上抛。问题就转化为怎么去接球，怎么设置速度了。不过，我们要控制8个人，怎么弄？很简单，先想想鼓会自己动！也就是：只考虑合力！ 哈哈！有思路了（好难啊）！分解成这么几个数学问题：怎么控制它，使其想到哪到哪（莫名想到PID哦）；怎么控制它，使其速度想怎样怎样（莫名想到PID哦）；怎么设置碰撞时的速度和朝向；怎么保证鼓在移动时不翻转；怎么保证鼓在其他时刻不乱翻转？ 乍一看，我了个c。。。 但再一想，诶呦不错哦！PID！RM战队牛逼！谢老板带领的南工骁鹰真是给我上了美好的一课，在这里竟然用上了！用！上！了！不过，这是一个多维度的PID，不！太！一！样！ 于是第一时间，我想到了。。。百度（哈哈，第二时间我才想到了谢老板）。可悲啊，百度太辣鸡，连个二阶pid都查不到，更别说多维度了，都是水不拉几的学位论文，都给我消失！还是谢老板厉害！于是开始勾搭谢老板。没想到秒回！不过嘛。。。谢老板工程经验丰富，但是对于我这么具体的理论问题就表示无奈了，毕竟这太具体了。谢老板表示我应该着重推公式，好吧，我去推公式了，自闭时间到。（最后我表示推一下系统函数，然后，真香） 自闭了一下午，我推了三张纸，越推越爽，一直推公式一直爽！那是真的爽！不骗你！当你发现复杂问题被自己简化成一个小迭代公式还不用涉及什么矩阵求逆啊之类的复杂运算之后，整个人都升华了——觉得自己成为了数学家！问翔哥，是否想听我讲一讲；翔哥用表情表示：算了，我还在自闭，我想不开。问子烨，继续扔给他公式，要他写我们的第三问，子烨这回善罢甘休了，不过能力也是有限啊。于是我就花了很长时间，召集子烨和翔哥，讲了半天我的PID推导，讲了半天我的系统，收获了翔哥的赞叹和子烨的默许，好的，这是一个好的开始了。 后来子烨果然闷闷闷儿去用公式编辑器打公式了，不过还需要我不时地给予指导。 这方面，我没能控制住自己的傲气，这点不好。我本应理解他不能立刻理解这些数学原理的，因为毕竟用他的话来说，“这些不是计科的东西啊，我哪能直接就会”。当然，这些也不是电信的东西，但毕竟，我搞过RM，我搞过控制，我搞过MATLAB，我对物理和数学都感兴趣！我有责任讲清楚！ 不过一切都开始进入正轨了。 现在距离考试结束还有86400秒钟！（包含睡觉！） 夜晚，我们大家确实都开始恋恋不舍这个战场了。我终于有那个魄力向三维的世界进发，向我所担心、害怕的世界进发！我担心，我花了一下午，6个小时，360分钟，21600秒钟推导出来的公式会是一坨辣鸡；我担心，我头铁坚持了那么久，强硬了那么多，怼了那么多次队友而坚持的道路会戛然而止；我担心，我自己会因为思绪混乱而放弃…… 不过，一晚上的尝试，让我再一次表示：我靠，还真给出来了！ 这得感谢我在电赛中学到的经验：把大的任务分解成小的任务。检验一个电路，先从电源模块入手，再检查每个管脚、每个功能，逐个排查；写一个大的代码，不要奢求“一把嗦（这么写的么？请教尤大佬！！）”（翻译：一步到位），而应该一步一步实现基础功能，一步一个脚印地组合到最终的功能。想一下不久前电赛备赛的时候，尤大佬是如何“一把嗦”的？他在实验室做了好久，先实现了MSP430的ADC，进而实现MSP430计数、墨水屏幕显示，然后才是MSP430频率计；然后实现FPGA流水灯、FPGA数码转换和数码管显示、FPGA的基本操作、FPGA的读取外设、FPGA的IO口使用，到这时候，尤大佬对这些东西已经有一个基本概念了，所以又可以通过FPGA实现频率计了；学会了GPIO，又可以手写SPI协议和MSP通信；SPI协议又要从最基础的发射开始，不要奢求一开始就可以读取，先用示波器尝试……啊好多啊，想想就复杂，但就被尤大佬一点点积累，等到最后出成果那一刻，美其名曰“一把嗦”！后来，我借鉴这个经验，才一点点搭建出我的FPGA第一个流水灯、读取IO、DDS生成器、AM调制、AM解调、FM调制、FM解调、TDMA信号的调制、TDMA信号的解调仿真、TDMA实际信号的解调的。 翔哥觉得自己不够聪明，实际上，就像我说的，“脑子清醒不清醒是一回事；聪明不聪明，是另一回事”。回想大一时候，翔哥可是线性代数很厉害的，我真的也赞叹。于是我就努力地学好线性代数。人在不同阶段总是要有榜样的，这种榜样，可以是自己所崇拜的那种，可以是身边的技胜一筹的，甚至可以是自己所不屑却嫉妒的那种人。 哇，扯了这么多。说说我怎么解决的最难问题吧！ Step1：在三维模型中，不引入水平反馈，看看竖直方向是否还和之前一样。if一样，转Step2，else改程序； Step2：尝试平动的PID公式，将球的初始水平位置设置到别处，看看能不能过去。if可以，转Step3，else找问题，确定是step1还是step2出了问题； Step3：尝试加入接球击打的PID公式，if能打上去，转Step4，else，顺着Step123排查问题； Step4是啥？我还差一个PID公式没有调，也就是修正转动的PID，但我并不急着弄！Step4是什么？是思考人生！思考的结果是：干到了这里，一定得有点结果！于是，我是用了求助rar技能！然后，向他俩汇报阶段性成果，接下来什么截图保存啊啥的都得有。好，转Step5 Step5：加入转动的PID公式，if能转正，then诶嘿嘿嘿，else，我了个c 嗯没错，我了个c 这该咋办？它转不正？！我先试一拨玄学调参，调整PID的比例值大小，没用；调整公式一些参数，没用；用动画来呈现这一切，看看到底咋转，写了一晚上电影，得到结论，它给我乱转！不过走到这一步，基本确定问题了：它乱转，转到最后转没了！我怀疑是不是我画图边界太窄了，我加大好几倍，它还是没。我于是追根溯源，直接看变量值，得到NaN！我的妈耶！加了PID不收敛了！但是又发现，法向量的y分量的结果很喜人，只是x分量炸了，导致z分量一起炸，那是不是….?我还没开窍，于是傻不愣登地把参数都设为0；好，回到了step3，一切正常。也就是我的迭代公式本身没有使我的系统爆炸，而是我的PID参数。也就是，我还有没考虑到的地方。我差点就要对PID参数进行搜索了，不过上帝给了我的大脑一盏LED灯，告诉我：你的Kp应该设成负的，要不就把迭代公式设成负的，至少也要试一试！于是我就试了试，成了！哇，到了晚上两点！宣布完毕这个阶段性成果，我就去洗澡了（哇头皮是真的痒痒，如果大于两天不洗头的话）。他俩还是恋恋不舍。 这晚上我有点睡不着，不过还是莫名其妙睡着了…… （因为中午出去买饭遇上了下雨，电梯里人也很多，所以我今天把！单！词！背！完！了！） （无wxh之乱耳，无wxh之劳形，今晚成果颇丰，过得很爽！（wxh：我耳朵长铁轨上了）） 2019数学建模系列传送门 2019数学建模国赛总结 Part 1 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 2 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 3 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 4 (2019 CUMCM Summary) 2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></content>
      <tags>
        <tag>math_moding</tag>
        <tag>experience</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019数学建模国赛总结 Part 4 (2019 CUMCM Summary)]]></title>
    <url>%2F2019%2F09%2F15%2Fmath-modling-2019CUMCM-summary-4%2F</url>
    <content type="text"><![CDATA[by lucainiaoge Final battle - 2019.9.159.15，今天， 我要完成最后一关， 打大Boss！ 毕竟我第四问还没碰呢。。。 一早6:30起床，于是早上我又被自己的帅帅帅先而帅到了！ 一坐下，赶快进入状态（虽然还有点迷迷糊糊）。十分担心，打开客户端检查有没有被退赛，嗯好，没有，好的，打开MATLAB看看我的程序还在不在，嗯好，没有，哦不，还在，嗯好。 …… 嗯，大概在7：00左右，我的大脑开始全速运转了。我把图存的差不多，直接就上手搜索PID的比例系数了。搜索量还是挺大的，而且老跑飞。保存图像时候，MATLAB还老给我崩掉（大概是因为，我画的点太多了）。不过硬着头皮，我搞出来很多组数。 这里一个教训：跑出来结果别老顾着高兴。要提前准备好一个有条理的总结方式：每次跑出结果，要记录完那些东西才能跑下一次，并且不要忘了这一次跑的条件是什么，用哪个版本的程序跑出来的……我就是，写了很多个版本，不下三个，每个版本的迭代初始设置都不一样（初始设置本来就纷繁复杂，我试图在MATLAB代码中汇总，但是还是没有根除这个问题），我最后就迷失在了这里。我真的不知道某个数据是用哪个程序跑出来的了。 我安慰自己，没事儿了，都到这个份儿上了，随便找个程序，再跑一遍，得到好看的结果直接记下来，把这一个所有信息都记下来，写到论文上就好了！ 嗯，这个思想起到了效果，不过没有起到长期效果…… 随着我的任务推进，我的大脑逐渐僵硬，我渐渐意识到自己快成为翔哥之前推公式的样子了。对了，上午我给翔哥的任务很简单而明确，就是：给翔哥输入合力（原来还想再给合力矩，但是很明显翔哥吃不消了，要是我我可能也无从下手了，转而自闭），翔哥输入N个人的分力的大小和方向（自由度很高，需要想办法设置分工规则）。翔哥思路很不错：让球来的方向那几个人着重使劲，其他人抵消其他影响。说的简单做起来难，翔哥也写了一上午。期间，我就用我那杂七杂八程序跑出来的玄学合力喂给他，他也用他的玄学程序输出玄学分力喂给我。于是我们就陷入了玄学循环…… 眼见着就要被这点屁事带倒了。我决定，先吃顿午饭（其实是外卖，因为之前跟楚笑说我们仨今天不下去了，帮我们点外卖吧），是鸡公煲，我一般不吃鸡公煲，但今天吃的还挺有味道的。吃完回来继续重新跑我问题4的优化程序，就崩了！崩了！再也！跑！不！出！来！了！ 我按捺住自己忐忑的心情，重新修改程序，修改了有半个小时，还！是！不行！！这个时刻，出这种事，谁顶得住？！ 还好！还好！还记得这个嘛？ 求救rar技能 现在的自闭时间是下午2:00，我在这时候之前做了一件让我自己十分受益的事情： 仰天长笑哈哈 我复活了！ 我按照这个里面打好包的程序又跑了一遍，（虽然这个程序时漏洞百出的，毕竟两个小时内，我也改了一些漏洞的，但是也可能制造了一些漏洞呢）成了！赶紧记下来，直接作为最后结果了！先不给它搜索，直接记下来，反正是按照最后一问条件跑出来的！（这个条件的刻画，我在清早分给翔哥了！）无所谓！结果可喜就是了！ 好，圆满完成？ 哦不！屁事多多！ e的灵敏度分析；问题4优化算法求解；论文很多公式还没打下来；论文问题4还没写结果；论文还没有写改进意见和附录；我们的支撑材料还没有完整化；翔哥还在推模型的验证系统该怎么弄（这涉及到另一套代码）……继续分工，我和翔哥搞模型验证系统（又开始互传变量了！），子烨继续打公式写论文；只不过我多了一个工序：让子烨不断发论文现稿给我订正。但毕竟我精力有限，很多错误直到比赛结束也没能排查出来，哎…… 最后，连扯带拽连滚带爬，极限操作搞定了以上我提到的收尾问题。虽然这些事情离我现在敲下这个字（2019.9.16 0:18，哦今天要上课了妈呀）是最近的，但是我真的不太记得这些事情是怎么解决的了。大概就是：ddl提供最大生产力吧！ 总之，在最后千钧一发之际，我们搞定了。打开客户端，确认存活，上传MD5码： 啊哦又发现文件命名不太对，重新提交（我忽然玩性大发，想要在8:00之前好好玩一玩提交MD5码，被队友制止了）： 告一段落！再会！收拾桌子！准备哈皮（？？？） P.S. 数学建模一大感受：我老是疑神疑鬼，总是觉得数学建模客户端就要关闭了。殊不知，我应该做的是再好好阅读一遍赛区规则说明。 P.P.S. 最后学校群里有一个可怜虫在图书馆电脑上提交MD5码以后把文件给删了，觉得提交了MD5码就ok了，在删了以后才意识到： 这个可怜虫内心大概已经MD了5000万次了 若有来者，有幸看到这篇文章的最后，并且打算参加下一次建模比赛，请谨记这个教训：MD5码只是一个路径的加密码而已，并不是文件本身的信息。这个码是为了保证你的文件是你的，不是别人作弊搞的。所以提交MD5码以后，不能修改信息！ 所以，最后才会有一个上传文件阶段（此阶段，你的文件路径已经锁住了！锁住了！你再动，再修改，就是作弊！）所以，在我正在写总结的时候，看到上面这位可怜虫的教训的时候，我在反省自己，是否也犯了类似地错误（毕竟我提交MD5以后还欣赏了两边我的论文甚至把它发送给了我爹一起欣赏（当然最后发现我靠漏洞百出（再当然，我不敢再修改了））） 所以，我终于意识到，我还没有提交文件！ hzy也参加数模，我赶快提醒他 谢天谢地。我的心路历程： “我靠怎么这么多漏洞” 到：庆幸自己终于交上去了， 论文写成啥样，怎么审核，随他去吧！随他去吧！我提交上去了！ 谢天谢地！啊啊啊啊啊！ 嗯，没错，我在今天的0:53分依然十分担心自己没传上去作品，又忐忐忑忑地传了一次，截图为凭！爽了，睡觉！ 2019数学建模系列传送门 2019数学建模国赛总结 Part 1 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 2 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 3 (2019 CUMCM Summary) 2019数学建模国赛总结 Part 4 (2019 CUMCM Summary) 2019数模国赛B题解析Part1(Analysis of 2019CUMCM Question-B Part1) 2019数模国赛B题解析Part2(Analysis of 2019CUMCM Question-B Part2)]]></content>
      <tags>
        <tag>math_moding</tag>
        <tag>experience</tag>
      </tags>
  </entry>
</search>
