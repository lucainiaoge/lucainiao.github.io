{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing How create_graph=True Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 20.])\n",
      "tensor([30., 10.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0,1.0], requires_grad=True)\n",
    "y = torch.tensor([1.0,2.0], requires_grad=True)\n",
    "z = torch.sum(x*y)\n",
    "w = z**2\n",
    "w.backward() #get dw/dx, dw/dy\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x,y\\in {\\rm R}^2, z\\in {\\rm R}$\n",
    "\n",
    "$w=z^2, \\quad z=x^T y, \\quad w(x,y)=(x^T y)^2$\n",
    "\n",
    "$\\nabla_x w=2y^T x y, \\quad \\nabla_y w=2x^T y x$\n",
    "\n",
    "$\\frac{\\partial^2 w}{\\partial x_1^2}=2y_1^2, \\quad$\n",
    "$\\frac{\\partial^2 w}{\\partial x_1x_2}=2y_1y_2, \\quad$\n",
    "$\\frac{\\partial^2 w}{\\partial x_1y_1}=4x_1y_1+2x_2y_2, \\quad$\n",
    "$\\frac{\\partial^2 w}{\\partial x_1y_2}=2x_2y_1, \\quad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 20.], grad_fn=<CloneBackward>)\n",
      "tensor([30., 10.], grad_fn=<CloneBackward>)\n",
      "tensor(10., grad_fn=<SelectBackward>)\n",
      "tensor([0., 0.], grad_fn=<CloneBackward>)\n",
      "tensor([0., 0.], grad_fn=<CloneBackward>)\n",
      "tensor([2., 4.], grad_fn=<CloneBackward>)\n",
      "tensor([16.,  2.], grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0,1.0], requires_grad=True)\n",
    "y = torch.tensor([1.0,2.0], requires_grad=True)\n",
    "z = torch.sum(x*y)\n",
    "w = z**2\n",
    "'''get dw/dx, dw/dy, and get the formula (computation graph) for dw/dx1, dw/dx2, dw/dy1, dw/dy2'''\n",
    "w.backward(create_graph=True)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "partial_x_1 = x.grad[0]\n",
    "print(partial_x_1)\n",
    "x.grad.data.zero_() #release grad logging\n",
    "y.grad.data.zero_() #release grad logging\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "\n",
    "partial_x_1.backward() #get d(dw/dx1)/dx, d(dw/dx1)/dy\n",
    "print(x.grad) #[d2w/dx1dx1,d2w/dx1dx2]\n",
    "print(y.grad) #[d2w/dx1dy1,d2w/dx1dy2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 20.], grad_fn=<CloneBackward>)\n",
      "tensor([30., 10.], grad_fn=<CloneBackward>)\n",
      "tensor(10., grad_fn=<SelectBackward>)\n",
      "tensor([12., 24.], grad_fn=<CloneBackward>)\n",
      "tensor([46., 12.], grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0,1.0], requires_grad=True)\n",
    "y = torch.tensor([1.0,2.0], requires_grad=True)\n",
    "z = torch.sum(x*y)\n",
    "w = z**2\n",
    "w.backward(create_graph=True) #get dw/dx, dw/dy, and get the formula (computation graph) for dw/dx1, dw/dx2, dw/dy1, dw/dy2\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "partial_x_1 = x.grad[0]\n",
    "print(partial_x_1)\n",
    "#without releasing grad logging\n",
    "\n",
    "partial_x_1.backward() #get d(dw/dx1)/dx, d(dw/dx1)/dy\n",
    "print(x.grad) #[d2w/dx1dx1,d2w/dx1dx2]+[dw/dx1,dw/dx2]\n",
    "print(y.grad) #[d2w/dx1dy1,d2w/dx1dy2]+[dw/dy1,dw/dy2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real example: MAML (2nd order)\n",
    "https://colab.research.google.com/drive/1MFJwRdOTefd6UOYRsNjdc7BWuB7Qe3lY\n",
    "\n",
    "The following code cannot run; just for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, init_weight=None):\n",
    "        super(net, self).__init__()\n",
    "        if type(init_weight) != type(None):\n",
    "            for name, module in init_weight.named_modules():\n",
    "                if name != '':\n",
    "                    setattr(self, name, MetaLinear(module))\n",
    "        else:\n",
    "            self.hidden1 = nn.Linear(1, 40)\n",
    "            self.hidden2 = nn.Linear(40, 40)\n",
    "            self.out = nn.Linear(40, 1)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        layers = self.__dict__['_modules']\n",
    "        for layer in layers.keys():\n",
    "            layers[layer].zero_grad()\n",
    "    def update(self, parent, lr = 1):\n",
    "        layers = self.__dict__['_modules']\n",
    "        parent_layers = parent.__dict__['_modules']\n",
    "        for param in layers.keys():\n",
    "            layers[param].weight = layers[param].weight - lr*parent_layers[param].weight.grad\n",
    "            layers[param].bias = layers[param].bias - lr*parent_layers[param].bias.grad\n",
    "        # gradient will flow back due to clone backward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta_learning_model():\n",
    "    def __init__(self, init_weight = None):\n",
    "        super(Meta_learning_model, self).__init__()\n",
    "        self.model = net().to(device)\n",
    "        if type(init_weight) != type(None):\n",
    "            self.model.load_state_dict(init_weight)\n",
    "        self.grad_buffer = 0\n",
    "    def gen_models(self, num, check = True):\n",
    "        models = [net(init_weight=self.model).to(device) for i in range(num)]\n",
    "        return models\n",
    "    def clear_buffer(self):\n",
    "        print(\"Before grad\", self.grad_buffer)\n",
    "        self.grad_buffer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "train_x, train_y, train_label = meta_task_data(task_num=50000*10) \n",
    "train_x = torch.Tensor(train_x).unsqueeze(-1) # add one dim\n",
    "train_y = torch.Tensor(train_y).unsqueeze(-1)\n",
    "train_dataset = data.TensorDataset(train_x, train_y)\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=bsz, shuffle=False)\n",
    "\n",
    "meta_model = Meta_learning_model()\n",
    "meta_optimizer = torch.optim.Adam(meta_model.model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "for e in range(epoch):\n",
    "    meta_model.model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        sub_models = meta_model.gen_models(bsz)\n",
    "\n",
    "        meta_l = 0\n",
    "        for model_num in range(len(sub_models)):\n",
    "                \n",
    "            sample = list(range(10))\n",
    "            np.random.shuffle(sample)\n",
    "            \n",
    "            # meta learning\n",
    "            \n",
    "            y_tilde = sub_models[model_num](x[model_num][sample[:5],:])\n",
    "            little_l = F.mse_loss(y_tilde, y[model_num][sample[:5],:])\n",
    "            #compute gradient ∇_ϕ, obtain its computation graph for high-order gradient\n",
    "            little_l.backward(create_graph = True)\n",
    "            sub_models[model_num].update(lr = 1e-2, parent = meta_model.model)\n",
    "            #clear gradient in optimizer (avoid from gradient cumulation)\n",
    "            meta_optimizer.zero_grad()\n",
    "            #compute 2nd-order gradient\n",
    "            #in detail: the update() method in sub_model is defined as such:\n",
    "            #layers[par].weight = layers[par].weight-lr*parent_layers[par].weight.grad\n",
    "            #parent_layers[par].weight.grad has computation graph because of \n",
    "            #create_graph=True\n",
    "            #therefore, when again using sub_models for forwarding, we actually applying computation graph of grad. Therefore, the meta-update will consider the computation graph of grad.\n",
    "            y_tilde = sub_models[model_num](x[model_num][sample[5:],:])\n",
    "            meta_l =  meta_l + F.mse_loss(y_tilde, y[model_num][sample[5:],:])\n",
    "\n",
    "        meta_l = meta_l / bsz\n",
    "        meta_l.backward()\n",
    "        meta_optimizer.step()\n",
    "        meta_optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
